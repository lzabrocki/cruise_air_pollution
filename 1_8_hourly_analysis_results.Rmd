---
title: "Analyzing Results - Entering Cruise Experiment"
description: |
  Comparing hours with entering cruise traffic to hours without. Adjusting for calendar and weather indicators.
author:
  - name: Marie-Abèle Bind 
    url: https://scholar.harvard.edu/marie-abele
    affiliation: Biostatistics Center, Massachusetts General Hospital
    affiliation_url: https://biostatistics.massgeneral.org/faculty/marie-abele-bind-phd/
  - name: Marion Leroutier 
    url: https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/
    affiliation: Misum, Stockholm School of Economics
    affiliation_url: https://www.hhs.se/en/persons/l/leroutier-marion/
  - name: Léo Zabrocki 
    url: https://lzabrocki.github.io/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/fr/zabrocki-leo/
date: "`r Sys.Date()`"
output: 
    distill::distill_article:
      toc: true
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
# code chunk option
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  dev = "CairoPNG",
  dpi = 300
)
```

In this document, we take great care providing all steps and R codes required to analyze the effects of entering cruise traffic on air pollutants. We compare hours where:

* treated units are hours with positive entering cruise traffic in t.
* control units are hours without entering cruise traffic in t.

We adjust for calendar calendar indicator and weather confouding factors.

**Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.**

# Required Packages

To reproduce exactly the `6_script_analyzing_results.html` document, we first need to have installed:

* the [R](https://www.r-project.org/) programming language 
* [RStudio](https://rstudio.com/), an integrated development environment for R, which will allow you to knit the `6_script_analyzing_results.Rmd` file and interact with the R code chunks
* the [R Markdown](https://rmarkdown.rstudio.com/) package
* and the [Distill](https://rstudio.github.io/distill/) package which provides the template for this document. 

Once everything is set up, we have to load the following packages:

```{r}
# load required packages
library(knitr) # for creating the R Markdown document
library(here) # for files paths organization
library(tidyverse) # for data manipulation and visualization
library(retrodesign) # for computing power, type M and S errors
library(Cairo) # for printing custom police of graphs
library(kableExtra) # for table formatting
```

We finally load our custom `ggplot2` theme for graphs:

```{r}
# load ggplot custom theme
source(here::here(
  "inputs",
  "2.functions",
  "script_theme_tufte.R"
))
# define nice colors
my_blue <- "#0081a7"
my_orange <- "#fb8500"
```


# Preparing the Data

We load the matching and matched data and bind them together:

```{r}
# load matched data
data_matched <-
  readRDS(
    here::here(
      "inputs",
      "1.data",
      "1.hourly_data",
      "2.data_for_analysis",
      "1.matched_data",
      "1.experiments_cruise",
      "1.experiment_entry_cruise",
      "matched_data_entry_cruise.rds"
    )
  ) 
```

# Distribution of the Pair Differences in Concentration between Treated and Control units for each Pollutant

## Computing Pairs Differences in Pollutant Concentrations

We first compute the differences in a pollutant's concentration for each pair over time:

```{r}
data_matched_wide <- data_matched %>%
  mutate(is_treated = ifelse(is_treated == TRUE, "treated", "control")) %>%
  select(
    is_treated,
    pair_number,
    contains("no2_l"),
    contains("no2_sl"),
    contains("o3"),
    contains("pm10_l"),
    contains("pm10_sl"),
    contains("pm25"),
    contains("so2")
  ) %>%
  pivot_longer(
    cols = -c(pair_number, is_treated),
    names_to = "variable",
    values_to = "concentration"
  ) %>%
  mutate(
    pollutant = NA %>%
      ifelse(str_detect(variable, "no2_l"), "NO2 Longchamp", .) %>%
      ifelse(str_detect(variable, "no2_sl"), "NO2 Saint-Louis", .) %>%
      ifelse(str_detect(variable, "o3"), "O3 Longchamp", .) %>%
      ifelse(str_detect(variable, "pm10_l"), "PM10 Longchamp", .) %>%
      ifelse(str_detect(variable, "pm10_sl"), "PM10 Saint-Louis", .) %>%
      ifelse(str_detect(variable, "pm25"), "PM2.5 Longchamp", .) %>%
      ifelse(str_detect(variable, "so2"), "SO2 Lonchamp", .)
  ) %>%
  mutate(
    time = 0 %>%
      ifelse(str_detect(variable, "lag_1"),-1, .) %>%
      ifelse(str_detect(variable, "lag_2"),-2, .) %>%
      ifelse(str_detect(variable, "lag_3"),-3, .) %>%
      ifelse(str_detect(variable, "lead_1"), 1, .) %>%
      ifelse(str_detect(variable, "lead_2"), 2, .) %>%
      ifelse(str_detect(variable, "lead_3"), 3, .)
  ) %>%
  select(-variable) %>%
  select(pair_number, is_treated, pollutant, time, concentration) %>%
  pivot_wider(names_from = is_treated, values_from = concentration)

data_pair_difference_pollutant <- data_matched_wide %>%
  mutate(difference = treated - control) %>%
  select(-c(treated, control))
```

## Pairs Differences in NO2 Concentrations

Boxplots for NO2:

```{r, fig.width=10, fig.height=5, code_folding="Please show me the code!"}
# create the graph for no2
graph_boxplot_difference_pollutant_no2 <-
  data_pair_difference_pollutant %>%
  filter(str_detect(pollutant, "NO2")) %>%
  ggplot(., aes(x = as.factor(time), y = difference)) +
  geom_boxplot(colour = my_blue) +
  facet_wrap(~ pollutant) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  ggtitle("Distribution of Pair Difference in Concentration") +
  ylab("Pair Difference in \nConcentration (µg/m3)") + xlab("Hour") +
  theme_tufte()

# display the graph
graph_boxplot_difference_pollutant_no2

# save the graph
graph_boxplot_difference_pollutant_no2 <-
  graph_boxplot_difference_pollutant_no2 +
  theme(plot.title = element_blank())

ggsave(
  graph_boxplot_difference_pollutant_no2,
  filename = here::here(
      "inputs",
      "3.outputs",
      "1.hourly_analysis",
      "2.experiments_cruise",
      "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_boxplot_difference_pollutant_no2.pdf"
  ),
  width = 40,
  height = 18,
  units = "cm",
  device = cairo_pdf
)
```

## Pairs Differences in O3 Concentrations

Boxplots for O3:

```{r, fig.width=10, fig.height=5, code_folding="Please show me the code!"}
# create the graph for o3
graph_boxplot_difference_pollutant_o3 <-
  data_pair_difference_pollutant %>%
  filter(str_detect(pollutant, "O3")) %>%
  ggplot(., aes(x = as.factor(time), y = difference)) +
  geom_boxplot(colour = my_blue) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  ggtitle("Distribution of Pair Difference in Concentration") +
  ylab("Pair Difference in \nConcentration (µg/m3)") + xlab("Hour") +
  theme_tufte()

# display the graph
graph_boxplot_difference_pollutant_o3

# save the graph
graph_boxplot_difference_pollutant_o3 <-
  graph_boxplot_difference_pollutant_o3 +
  theme(plot.title = element_blank())

ggsave(
  graph_boxplot_difference_pollutant_o3,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_boxplot_difference_pollutant_o3.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```

## Pairs Differences in PM10 Concentrations

Boxplots for PM10:

```{r, fig.width=10, fig.height=5, code_folding="Please show me the code!"}
# create the graph for pm10
graph_boxplot_difference_pollutant_pm10 <-
  data_pair_difference_pollutant %>%
  filter(str_detect(pollutant, "PM10")) %>%
  ggplot(., aes(x = as.factor(time), y = difference)) +
  geom_boxplot(colour = my_blue) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  facet_wrap( ~ pollutant) +
  ggtitle("Distribution of Pair Difference in Concentration") +
  ylab("Pair Difference in \nConcentration (µg/m3)") + xlab("Hour") +
  theme_tufte()

# display the graph
graph_boxplot_difference_pollutant_pm10

# save the graph
graph_boxplot_difference_pollutant_pm10 <-
  graph_boxplot_difference_pollutant_pm10 +
  theme(plot.title = element_blank())

ggsave(
  graph_boxplot_difference_pollutant_pm10,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_boxplot_difference_pollutant_pm10.pdf"
  ),
  width = 40,
  height = 18,
  units = "cm",
  device = cairo_pdf
)
```

## Pairs Differences in PM2.5 Concentrations

Boxplots for PM2.5:

```{r, fig.width=10, fig.height=5, code_folding="Please show me the code!"}
# create the graph for pm2.5
graph_boxplot_difference_pollutant_pm25 <-
  data_pair_difference_pollutant %>%
  filter(str_detect(pollutant, "PM2.5")) %>%
  ggplot(., aes(x = as.factor(time), y = difference)) +
  geom_boxplot(colour = my_blue) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  ggtitle("Distribution of Pair Difference in Concentration") +
  ylab("Pair Difference in \nConcentration (µg/m3)") + xlab("Hour") +
  theme_tufte()

# display the graph
graph_boxplot_difference_pollutant_pm25

# save the graph
graph_boxplot_difference_pollutant_pm25 <-
  graph_boxplot_difference_pollutant_pm25 +
  theme(plot.title = element_blank())

ggsave(
  graph_boxplot_difference_pollutant_pm25,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_boxplot_difference_pollutant_pm25.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```

## Pairs Differences in SO2 Concentrations

Boxplots for SO2:

```{r, fig.width=10, fig.height=5, code_folding="Please show me the code!"}
# create the graph for so2
graph_boxplot_difference_pollutant_so2 <-
  data_pair_difference_pollutant %>%
  filter(str_detect(pollutant, "SO2")) %>%
  ggplot(., aes(x = as.factor(time), y = difference)) +
  geom_boxplot(colour = my_blue) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  ggtitle("Distribution of Pair Difference in Concentration") +
  ylab("Pair Difference in \nConcentration (µg/m3)") + xlab("Hour") +
  theme_tufte()

# display the graph
graph_boxplot_difference_pollutant_so2

# save the graph
graph_boxplot_difference_pollutant_so2 <-
  graph_boxplot_difference_pollutant_so2 +
  theme(plot.title = element_blank())

ggsave(
  graph_boxplot_difference_pollutant_so2,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_boxplot_difference_pollutant_so2.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```

# Testing the Sharp Null Hypothesis

We test the sharp null hypothesis of no effect for any units. We first create a dataset where we nest the pair differences by pollutant and time. We also compute the observed test statistic which is the observed average of pair differences:

```{r}
# nest the data by pollutant and time
ri_data_sharp_null <- data_pair_difference_pollutant   %>%
  select(pollutant, time, difference) %>%
  group_by(pollutant, time) %>%
  mutate(observed_mean_difference = mean(difference)) %>%
  group_by(pollutant, time, observed_mean_difference) %>%
  summarise(data_difference = list(difference))
```

We then create a function to compute the randomization distribution of the test statistic:

```{r}
# randomization distribution function
# this function takes the vector of pair differences
# and then compute the average pair difference according
# to the permuted treatment assignment
function_randomization_distribution <- function(data_difference) {
  randomization_distribution = NULL
  n_columns = dim(permutations_matrix)[2]
  for (i in 1:n_columns) {
    randomization_distribution[i] =  sum(data_difference * permutations_matrix[, i]) / number_pairs
  }
  return(randomization_distribution)
}
```

We store the number of pairs and the number of simulations we want to run:

```{r}
# define number of pairs in the experiment
number_pairs <- nrow(data_matched)/2

# define number of simulations
number_simulations <- 100000
```

We compute the permutations matrix:

```{r}
# set the seed
set.seed(42)

# compute the permutations matrix
permutations_matrix <- matrix(rbinom(number_pairs*number_simulations, 1,.5)*2-1, nrow = number_pairs, ncol = number_simulations)
```

For each pollutant and time, we compute the randomization distribution of the test statistic using 100,000 iterations. It took us 46 seconds to run this code chunck on our basic local computer:

```{r}
# compute the test statistic distribution
ri_data_sharp_null <- ri_data_sharp_null %>%
  mutate(randomization_distribution = map(data_difference, ~ function_randomization_distribution(.)))
```

Using the observed value of the test statistic and its randomization distribution, we compute the two-sided *p*-values:

```{r}
# function to compute the upper one-sided p-value
function_fisher_upper_p_value <- function(observed_mean_difference, randomization_distribution){
  sum(randomization_distribution >= observed_mean_difference)/number_simulations
}

# function compute the lower one-sided p-value
function_fisher_lower_p_value <- function(observed_mean_difference, randomization_distribution){
  sum(randomization_distribution <= observed_mean_difference)/number_simulations
}

# compute the lower and upper one-sided p-values
ri_data_sharp_null <- ri_data_sharp_null %>%
  mutate(p_value_upper = map2_dbl(observed_mean_difference, randomization_distribution, ~ function_fisher_upper_p_value(.x, .y)),
         p_value_lower = map2_dbl(observed_mean_difference, randomization_distribution, ~ function_fisher_lower_p_value(.x, .y)))

# compute the two-sided p-value using rosenbaum (2010) procedure
ri_data_sharp_null <- ri_data_sharp_null %>%
  rowwise() %>%
  mutate(two_sided_p_value = min(c(p_value_upper, p_value_lower))*2) %>%
  mutate(two_sided_p_value = min(two_sided_p_value, 1)) %>%
  select(pollutant, time, observed_mean_difference, two_sided_p_value) %>%
  ungroup()
```


We plot below the two-sided p-values for the sharp null hypothesis for each pollutant:

```{r, layout="l-body-outset", fig.width=30, fig.height=12, code_folding="Please show me the code!"}
# make the graph
graph_p_values <- ri_data_sharp_null %>%
  ggplot(., aes(x = as.factor(time), y = two_sided_p_value)) +
  geom_segment(aes(
    x = as.factor(time),
    xend = as.factor(time),
    y = 0,
    yend = two_sided_p_value
  )) +
  geom_point(
    shape = 21,
    size = 4,
    colour = "black",
    fill = my_blue
  ) +
  facet_wrap( ~ pollutant, ncol = 4) +
  xlab("Hour") + ylab("Two-Sided P-Value") +
  theme_tufte()

# display the graph
graph_p_values

# save the graph
ggsave(
  graph_p_values,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_p_values.pdf"
  ),
  width = 60,
  height = 25,
  units = "cm",
  device = cairo_pdf
)
```

We display below the table of Fisher p-values:

```{r, code_folding="Please show me the code!"}
ri_data_sharp_null %>%
  select(pollutant, time, observed_mean_difference, two_sided_p_value) %>%
  mutate(observed_mean_difference = round(observed_mean_difference, 1)) %>%
  rename(
    "Pollutant" = pollutant,
    "Time" = time,
    "Observed Value of the Test Statistic" = observed_mean_difference,
    "Two-Sided P-Values" = two_sided_p_value
  ) %>%
  rmarkdown::paged_table(.)
```

# Computing Fisherian intervals

To quickly compute 95% Fisherian intervals, we run the procedure on an Amazon Web Services virtual computer (EC2 t3.2xlarge). It took about 38 minutes for the code to run. It can be found in the `script_aws_fisherian_intervals.R` file. We explain below how we proceed. We first create a nested dataset with the pair differences for each pollutant and hour. We also add the set of hypothetical constant effects.

```{r, eval = FALSE}
# create a nested dataframe with 
# the set of constant treatment effect sizes
# and the vector of observed pair differences
ri_data_fi <- data_pair_difference_pollutant %>%
  select(pollutant, time, difference) %>%
  group_by(pollutant, time) %>%
  summarise(data_difference = list(difference)) %>%
  group_by(pollutant, time, data_difference) %>%
  expand(effect = seq(from = -10, to = 10, by = 0.1)) %>%
  ungroup()
```
 
We then substract for each pair difference the hypothetical constant effect:

```{r, eval = FALSE}
# function to get the observed statistic
adjusted_pair_difference_function <- function(pair_differences, effect){
  adjusted_pair_difference <- pair_differences-effect
  return(adjusted_pair_difference)
} 

# compute the adjusted pair differences
ri_data_fi <- ri_data_fi %>%
  mutate(data_adjusted_pair_difference = map2(data_difference, effect, ~ adjusted_pair_difference_function(.x, .y)))
```

We compute the observed mean of adjusted pair differences:

```{R, eval = FALSE}
# compute the observed mean of adjusted pair differences
ri_data_fi <- ri_data_fi %>%
  mutate(observed_mean_difference = map(data_adjusted_pair_difference, ~ mean(.))) %>%
  unnest(cols = c(observed_mean_difference)) %>%
  select(-data_difference) %>%
  ungroup()
``` 

We use the same `function_randomization_distribution` to compute the randomization distribution of the test statistic and run 100,000 iterations for each pollutant-hour observation:

```{r, eval = FALSE}
# define number of pairs in the experiment
number_pairs <- nrow(data_matched)/2

# define number of simulations
number_simulations <- 100000

# compute the permutations matrix
permutations_matrix <- matrix(rbinom(number_pairs*number_simulations, 1,.5)*2-1, nrow = number_pairs, ncol = number_simulations)

# randomization distribution function
# this function takes the vector of pair differences
# and then compute the average pair difference according 
# to the permuted treatment assignment
function_randomization_distribution <- function(data_difference) {
  randomization_distribution = NULL
  n_columns = dim(permutations_matrix)[2]
  for (i in 1:n_columns) {
    randomization_distribution[i] =  sum(data_difference * permutations_matrix[, i]) / number_pairs
  }
  return(randomization_distribution)
}
```  

We run the function:

```{r, eval = FALSE}
# compute the test statistic distribution
ri_data_fi <- ri_data_fi %>%
  mutate(randomization_distribution = map(data_adjusted_pair_difference, ~ function_randomization_distribution(.)))
``` 

We compute the lower and upper *p*-values functions. From these functions, we retrieve the lower and upper bound of the 95% Fisherian intervals:

```{r, eval = FALSE}
# define the p-values functions
function_fisher_upper_p_value <- function(observed_mean_difference, randomization_distribution){
  sum(randomization_distribution >= observed_mean_difference)/number_simulations
}

function_fisher_lower_p_value <- function(observed_mean_difference, randomization_distribution){
  sum(randomization_distribution <= observed_mean_difference)/number_simulations
}

# compute the lower and upper one-sided p-values
ri_data_fi <- ri_data_fi %>%
  mutate(p_value_upper = map2_dbl(observed_mean_difference, randomization_distribution, ~ function_fisher_upper_p_value(.x, .y)),
         p_value_lower = map2_dbl(observed_mean_difference, randomization_distribution, ~ function_fisher_lower_p_value(.x, .y)))

# retrieve the constant effects with the p-values equal or the closest to 0.025
ri_data_fi <- ri_data_fi %>%
  mutate(p_value_upper = abs(p_value_upper - 0.025),
         p_value_lower = abs(p_value_lower - 0.025)) %>%
  group_by(pollutant, time) %>%
  filter(p_value_upper == min(p_value_upper) | p_value_lower == min(p_value_lower)) %>%
# in case two effect sizes have a p-value equal to 0.025, we take the effect size
# that make the Fisherian interval wider to be conservative
  summarise(lower_fi = min(effect),
            upper_fi = max(effect))
```  

We finally compute the point estimates of the Fisherian intervals which we define as the observed average of pair differences:

```{r, eval = FALSE}
# compute observed average of pair differences
ri_data_fi_point_estimate <- data_pair_difference_pollutant   %>%
  select(pollutant, time, difference) %>%
  group_by(pollutant, time) %>%
  mutate(observed_mean_difference = mean(difference)) %>%
  ungroup()
``` 

We merge the point estimates data with the Fisherian intervals data:

```{r, eval = FALSE}
# merge ri_data_fi_point_estimate with ri_data_fi
ri_data_fi_final <- left_join(ri_data_fi, ri_data_fi_point_estimate, by = c("pollutant", "time"))
```  

We plot below the 95% Fisherian intervals:

```{r, layout="l-body-outset", fig.width=20, fig.height=10, code_folding="Please show me the code!"}
# open the data on fisherian intervals
ri_data_fi_final <-
  readRDS(
    here::here(
     "inputs",
      "1.data",
      "1.hourly_data",
      "2.data_for_analysis",
      "1.matched_data",
      "1.experiments_cruise",
      "1.experiment_entry_cruise",
      "ri_data_fisherian_intervals.rds"
    )
  )

# create an indicator to alternate shading of confidence intervals
ri_data_fi_final <- ri_data_fi_final %>%
  arrange(pollutant, time) %>%
  mutate(stripe = ifelse((time %% 2) == 0, "Grey", "White")) %>%
  ungroup()

# make the graph
graph_fisherian_intervals <-
  ggplot(ri_data_fi_final,
         aes(x = as.factor(time), y = observed_mean_difference)) +
  geom_rect(
    aes(fill = stripe),
    xmin = as.numeric(as.factor(ri_data_fi_final$time)) - 0.42,
    xmax = as.numeric(as.factor(ri_data_fi_final$time)) + 0.42,
    ymin = -Inf,
    ymax = Inf,
    color = NA,
    alpha = 0.1
  ) +
  geom_hline(yintercept = 0, color = "black") +
  geom_pointrange(
    aes(
      x = as.factor(time),
      y = observed_mean_difference,
      ymin = lower_fi ,
      ymax = upper_fi
    ),
    colour = my_blue,
    lwd = 0.8
  ) +
  facet_wrap( ~ pollutant, scales = "free_y", ncol = 4) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  scale_fill_manual(values = c("grey90", NA)) +
  guides(fill = FALSE) +
  ylab("Constant-Additive Increase \nin Concentrations (µg/m³)") + xlab("Hour") +
  theme_tufte()

# print the graph
graph_fisherian_intervals

# save the graph
ggsave(
  graph_fisherian_intervals,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_fisherian_intervals.pdf"
  ),
  width = 40,
  height = 18,
  units = "cm",
  device = cairo_pdf
)
```

We display below the table with the 95% Fisherian intervals and the point estimates:

```{r, code_folding="Please show me the code!"}
ri_data_fi_final %>%
  select(pollutant, time, observed_mean_difference, lower_fi, upper_fi) %>%
  mutate(observed_mean_difference = round(observed_mean_difference, 1)) %>%
  rename(
    "Pollutant" = pollutant,
    "Time" = time,
    "Point Estimate" = observed_mean_difference,
    "Lower Bound of the 95% Fisherian Interval" = lower_fi,
    "Upper Bound of the 95% Fisherian Interval" = upper_fi
  ) %>%
  rmarkdown::paged_table(.)
```

# Checking the Sensivity of Results

In this section, we carry out four investigations:

* We evaluate to which extent our results could suffer from hidde bias.
* We check how our results are sensitive to outliers by computing 95% Fisherian intervals based on the Wilcoxon's signed rank test statistic.
* As we imputed the missing pollutant concentrations, we also want to see how our results might for the non-missing outcomes. We compute 95% Fisherian intervals based on the Wilcoxon's signed rank test statistic.
* We compute confidence intervals for the average treatment effect using Neyman's approach.
* We explore how our study could suffer from type S and M errors.


### Sensitivity to Hidden Bias

We implement the studentized sensitivity analysis for the ATE developed by Colin B. Fogarty (2019). We first load the relevant functions:

```{r}
# load fogarty's studentized Sensitivity Analysis functions
# retrieved from http://www.mit.edu/~cfogarty/StudentizedSensitivity.R

#' StudentizedSensitivity
#'Function to perform a Studentized Sensitivity analysis on the sample average treatment
#'effect in a paired observational study
#'
#' @param PairedDiff: Vector of treated-minus-control paired differences.
#' @param null: Value of the sample average treatment effect under the null.
#' @param alpha: Desired Type I error rate.
#' @param alternative: Can be "less", "greater", or "two.sided".
#' @param Gamma: Vector of values for Gamma at which to perform the sensitivity
#'  analysis.
#' @param nperm: Number of permutations to perform permutation test.
#' @param Changepoint: If true, function returns the maximal Gamma for which the
#' test rejects at level alpha.
#' @param SensitivityInterval: If true, function returns (100-alpha) sensitivity
#' intervals. They will be one-sided if the alternative is less than or greater than,
#' and two-sided if the alternative is two-sided.
#'
#' @return Gamma: Vector of Gammas for which the sensitivity analysis was performed.
#' @return pval: P-values for each value of Gamma.
#' @return GammaPval: Matrix combining Gamma and pval.
#' @return Changepoint: Maximal Gamma for which the test rejected at level alpha.
#' @return SensitivityInterval: Upper and lower bounds for 100(1-alpha) sensitivity
#' intervals for each value of Gamma.
#' @export


StudentizedSensitivity = function(PairedDiff, null = 0, alpha = 0.05, alternative = "greater", Gamma = 1, nperm = 50000, Changepoint = T, SensitivityInterval = T)
{
   if(any(Gamma < 1))
   {
     stop("Values for Gamma must be >= 1")
   }
   if(alternative!="less" & alternative!= "greater" & alternative != "two.sided")
   {
     stop("Values for alternative are `less', `greater', or `two.sided'")
   }
   if(length(null) > 1)
   {
     stop("Value under the null must be a scalar")
   }
      if(alpha < 0 | alpha > 0.5)
   {
     stop("alpha must be between 0 and 0.5")
      }

  PairedDifftrue <- PairedDiff
  alphatrue <- alpha
  I <- length(PairedDiff)
  Adjust <- PairedDiff - null

  if(alternative == "less")
  {
    Adjust <- -Adjust
  }
  if(alternative == "two.sided")
  {
    alpha <- alphatrue/2

    if(mean(Adjust) < 0)
    {
      Adjust <- -Adjust
    }
  }

  pval <- rep(0, length(Gamma))

  for(i in 1:length(Gamma))

  {
  D <- (Adjust) - (Gamma[i]-1)/(1+Gamma[i])*abs(Adjust)
  obs <- mean(D)/(sd(D)/sqrt(I))
  Adjmat <- matrix(abs(Adjust), I, nperm)
  Zmat <- matrix(runif(I*nperm) < Gamma[i]/(1+Gamma[i]), I, nperm)
  Dmat <- (2*Zmat-1)*(Adjmat) - (Gamma[i]-1)/(1+Gamma[i])*Adjmat
  perm <- colMeans(Dmat)/(sqrt(colVars(Dmat)/I))
  pval[i] <- (1+sum(perm>=obs))/(nperm + 1)
  }
  pvalret = pval
  if(alternative == "two.sided")
  {
    pvalret = 2*pval
  }
  Pmatrix <- cbind(Gamma, pvalret)
  colnames(Pmatrix) <- c("Gamma", "P-value")

  if(Changepoint == T)
  {
    proceed <- StudentizedSensitivity(PairedDifftrue, null, alphatrue, alternative, Gamma=1, nperm,
                                      Changepoint = F, SensitivityInterval = F)$pval <= alphatrue

    change <- 1

    if(proceed)
    {
      change <- uniroot(StudentizedChangepoint, interval = c(1, 30), PairedDiff = PairedDifftrue, null = null,
                        alpha = alphatrue, alternative = alternative, nperm = nperm,
                        extendInt = "upX")$root
    }
  }

  if(SensitivityInterval == T)
    {
      lb = rep(-Inf, length(Gamma))
      ub = rep(Inf, length(Gamma))
      for(i in 1:length(Gamma))
      {
        # Warm Starts
      UB = uniroot(BoundFinder, PairedDifftrue, Gamma[i],
                   interval = c(mean(PairedDifftrue), mean(PairedDifftrue)+4*sd(PairedDifftrue)/sqrt(I)), extendInt = "yes")$root
      LB = -uniroot(BoundFinder, -PairedDifftrue, Gamma[i],
                    interval = c(-mean(PairedDifftrue)-4*sd(PairedDifftrue)/sqrt(I), -mean(PairedDifftrue)), extendInt = "yes")$root

      SUB = Inf
      SLB = -Inf

      if(alternative == "greater")
      {
        SLB = uniroot(StudentizedSI, interval = c(UB-4*sd(PairedDifftrue)/sqrt(I), UB), extendInt = "yes",
                      Gamma = Gamma[i], PairedDiff=PairedDifftrue, alternative = "greater", alpha = alpha, nperm = nperm)$root
      }

      if(alternative == "less")
      {
        SUB = uniroot(StudentizedSI, interval = c(LB, LB + 4*sd(PairedDifftrue)/sqrt(I)), extendInt = "yes",
                      Gamma = Gamma[i], PairedDiff=PairedDifftrue, alternative = "less", alpha = alpha, nperm = nperm)$root
      }

      if(alternative == "two.sided")
      {
       SLB = uniroot(StudentizedSI, interval = c(UB-4*sd(PairedDifftrue)/sqrt(I), UB), extendInt = "yes",
                     Gamma = Gamma[i], PairedDiff=PairedDifftrue, alternative = "greater", alpha = alpha, nperm = nperm)$root
       SUB = uniroot(StudentizedSI, interval = c(LB, LB+4*sd(PairedDifftrue)/sqrt(I)), extendInt = "yes",
                     Gamma = Gamma[i], PairedDiff=PairedDifftrue, alternative = "less", alpha = alpha, nperm = nperm)$root
      }

      lb[i] = SLB
      ub[i] = SUB
      }

    SImat = cbind(Gamma, lb, ub)
    colnames(SImat) = c("Gamma", "Lower Bound", "Upper Bound")
    }
    if(Changepoint == F & SensitivityInterval == F)
    {
      return(list(Gamma=Gamma, pval = pvalret, GammaPval = Pmatrix))
    }
    if(Changepoint == F & SensitivityInterval == T)
    {
      return(list(Gamma = Gamma, pval = pvalret, GammaPval = Pmatrix, SensitivityInterval = SImat))
    }
    if(Changepoint == T & SensitivityInterval == F)
    {
      return(list(Gamma = Gamma, pval = pvalret, GammaPval = Pmatrix, Changepoint = change))
    }
    if(Changepoint == T & SensitivityInterval == T)
    {
      return(list(Gamma = Gamma, pval = pvalret, GammaPval = Pmatrix, Changepoint = change,
                  SensitivityInterval = SImat))
    }

}

####These are auxiliary functions used for root finding and for calculating columnwise variances in StudentizedSensitivity
StudentizedChangepoint = function(Gamma, PairedDiff, null, alternative, alpha, nperm)
{
  alphachange = alpha
  StudentizedSensitivity(PairedDiff, null, alpha, alternative, Gamma, nperm, Changepoint = F, SensitivityInterval = F)$pval - alphachange
}

StudentizedSI = function(null,  Gamma, PairedDiff,  alternative, alpha, nperm)
{
  StudentizedSensitivity(PairedDiff, null, alpha, alternative, Gamma, nperm, Changepoint = F, SensitivityInterval = F)$pval - alpha
}

BoundFinder = function(null,  PairedDiff, Gamma)
{
  mean(PairedDiff - null - (Gamma-1)/(1+Gamma)*abs(PairedDiff-null))
}

colVars <- function(x) {
  N = nrow(x)
  (colSums(x^2) - colSums(x)^2/N) / (N-1)
}
```

We select the pair differences for NO$_{2}$ in Longchamp and PM$_{10}$ in Saint-Louis concentrations in t and run the function for $\Gamma=2$:

```{r}
# we select the relevant pair differences
data_hidden_bias <- data_pair_difference_pollutant %>%
  filter(pollutant %in% c("NO2 Longchamp", "PM10 Saint-Louis") &
           time == 0) %>%
  select(pollutant, difference) %>%
  group_by(pollutant) %>%
  nest() %>%
  mutate(data = map(data, ~ unlist(.)))

data_hidden_bias %>%
  mutate(
    sensitivity_interval = map(
      data,
      ~ StudentizedSensitivity(
        .,
        null = 0,
        alpha = 0.05,
        alternative = "two.sided",
        Gamma = 1.5,
        nperm = 10000,
        Changepoint = T,
        SensitivityInterval = T
      )$SensitivityInterval %>% as_tibble(.)
    )
  ) %>%
  select(-data) %>%
  unnest(cols = c(sensitivity_interval)) %>%
  mutate_at(vars(-pollutant), ~ round(., 2)) %>%
  kable(., align = c("l", rep("c", 3))) %>%
  kable_styling(position = "center")
```


### Outliers

To gauge how sensitive our results are to outliers, we use a Wilcoxon signed rank test statistic and compute 95% Fisherian intervals using the `wilcox.test()` function.

```{r, layout="l-body-outset", fig.width=10, fig.height=5, code_folding="Please show me the code!"}
# carry out the wilcox.test
data_rank_ci <- data_pair_difference_pollutant %>%
  select(-pair_number) %>%
  group_by(pollutant, time) %>%
  nest() %>%
  mutate(
    effect = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$estimate),
    lower_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[1]),
    upper_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[2])
  ) %>%
  unnest(cols = c(effect, lower_ci, upper_ci)) %>%
  mutate(data = "Wilcoxon Rank Test Statistic")

# bind ri_data_fi_final with data_rank_ci
data_ci <- ri_data_fi_final %>%
  rename(effect = observed_mean_difference,
         lower_ci = lower_fi,
         upper_ci = upper_fi) %>%
  mutate(data = "Average Pair Difference Test Statistic") %>%
  bind_rows(., data_rank_ci)

# create an indicator to alternate shading of confidence intervals
data_ci <- data_ci %>%
  arrange(pollutant, time) %>%
  mutate(stripe = ifelse((time %% 2) == 0, "Grey", "White")) %>%
  ungroup()

# make the graph
graph_ri_ci_wilcoxon <-
  ggplot(
    data_ci,
    aes(
      x = as.factor(time),
      y = effect,
      ymin = lower_ci,
      ymax = upper_ci,
      colour = data,
      shape = data
    )
  ) +
  geom_rect(
    aes(fill = stripe),
    xmin = as.numeric(as.factor(data_ci$time)) - 0.42,
    xmax = as.numeric(as.factor(data_ci$time)) + 0.42,
    ymin = -Inf,
    ymax = Inf,
    color = NA,
    alpha = 0.1
  ) +
  geom_hline(yintercept = 0, color = "black") +
  geom_pointrange(position = position_dodge(width = 1),
                  size = 0.8,
                  fatten = 2) +
  scale_shape_manual(name = "Test Statistic:", values = c(16, 17)) +
  scale_color_manual(name = "Test Statistic:", values = c(my_orange, my_blue)) +
  facet_wrap( ~ pollutant, scales = "free_y", ncol = 4) +
  scale_fill_manual(values = c('grey80', NA)) +
  guides(fill = FALSE) +
  ylab("Constant-Additive Increase \nin Concentrations (µg/m³)") + xlab("Hour") +
  theme_tufte()

# print the graph
graph_ri_ci_wilcoxon

# save the graph
ggsave(
  graph_ri_ci_wilcoxon,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_ri_ci_wilcoxon.pdf"
  ),
  width = 40,
  height = 20,
  units = "cm",
  device = cairo_pdf
)
```

### Missing Outcomes

We load non-imputed air pollution data and compute for each pollutant the 0-6 daily lags and leads:

```{r}
# load marseille raw air pollution data
data_marseille_raw_pollutants <- readRDS(
  here::here(
    "inputs",
    "1.data",
    "1.hourly_data",
    "1.raw_data",
    "2.pollution_data",
    "marseille",
    "raw_marseille_hourly_pollutants_2008_2018_data.rds"
  )
) %>%
  rename_at(vars(-date), function(x)
    paste0("raw_", x)) %>%
  as_data_frame()

# we first define data_marseille_raw_pollutants_leads and data_marseille_raw_pollutants_lags
# to store leads and lags

data_marseille_raw_pollutants_leads <- data_marseille_raw_pollutants
data_marseille_raw_pollutants_lags <- data_marseille_raw_pollutants

#
# create leads
#

# create a list to store dataframe of leads
leads_list <- vector(mode = "list", length = 3)
names(leads_list) <- c(1:3)

# create the leads
for (i in 1:3) {
  leads_list[[i]] <- data_marseille_raw_pollutants_leads %>%
    mutate_at(vars(-date), ~  lead(., n = i, order_by = date)) %>%
    rename_at(vars(-date), function(x)
      paste0(x, "_lead_", i))
}

# merge the dataframes of leads
data_leads <- leads_list %>%
  reduce(left_join, by = "date")

# merge the leads with the data_marseille_raw_pollutants_leads
data_marseille_raw_pollutants_leads <-
  left_join(data_marseille_raw_pollutants_leads, data_leads, by = "date") %>%
  select(-c(raw_mean_no2_l:raw_mean_pm10_sl))

#
# create lags
#

# create a list to store dataframe of lags
lags_list <- vector(mode = "list", length = 3)
names(lags_list) <- c(1:3)

# create the lags
for (i in 1:3) {
  lags_list[[i]] <- data_marseille_raw_pollutants_lags %>%
    mutate_at(vars(-date), ~  lag(., n = i, order_by = date)) %>%
    rename_at(vars(-date), function(x)
      paste0(x, "_lag_", i))
}

# merge the dataframes of lags
data_lags <- lags_list %>%
  reduce(left_join, by = "date")

# merge the lags with the initial data_marseille_raw_pollutants_lags
data_marseille_raw_pollutants_lags <-
  left_join(data_marseille_raw_pollutants_lags, data_lags, by = "date")

#
# merge data_marseille_raw_pollutants_leads with data_marseille_raw_pollutants_lags
#

data_marseille_raw_pollutants <-
  left_join(data_marseille_raw_pollutants_lags,
            data_marseille_raw_pollutants_leads,
            by = "date")
```

We merge these data with the matched data and compute pair differences: 

```{r}
# merge with the matched data
data_matched_with_raw_pollutants <-
  merge(data_matched, data_marseille_raw_pollutants, by = "date")

# compute pair differences
data_matched_wide_raw_pollutants <-
  data_matched_with_raw_pollutants %>%
  mutate(is_treated = ifelse(is_treated == TRUE, "treated", "control")) %>%
  select(
    is_treated,
    pair_number,
    contains("raw_mean_no2_l"),
    contains("raw_mean_no2_sl"),
    contains("raw_mean_o3"),
    contains("raw_mean_pm10_l"),
    contains("raw_mean_pm10_sl"),
    contains("raw_mean_pm25"),
    contains("raw_mean_so2")
  ) %>%
  pivot_longer(
    cols = -c(pair_number, is_treated),
    names_to = "variable",
    values_to = "concentration"
  ) %>%
  mutate(
    pollutant = NA %>%
      ifelse(str_detect(variable, "no2_l"), "NO2 Longchamp", .) %>%
      ifelse(str_detect(variable, "no2_sl"), "NO2 Saint-Louis", .) %>%
      ifelse(str_detect(variable, "o3"), "O3 Longchamp", .) %>%
      ifelse(str_detect(variable, "pm10_l"), "PM10 Longchamp", .) %>%
      ifelse(str_detect(variable, "pm10_sl"), "PM10 Saint-Louis", .) %>%
      ifelse(str_detect(variable, "pm25"), "PM2.5 Longchamp", .) %>%
      ifelse(str_detect(variable, "so2"), "SO2 Lonchamp", .)
  ) %>%
  mutate(
    time = 0 %>%
      ifelse(str_detect(variable, "lag_1"),-1, .) %>%
      ifelse(str_detect(variable, "lag_2"),-2, .) %>%
      ifelse(str_detect(variable, "lag_3"),-3, .) %>%
      ifelse(str_detect(variable, "lead_1"), 1, .) %>%
      ifelse(str_detect(variable, "lead_2"), 2, .) %>%
      ifelse(str_detect(variable, "lead_3"), 3, .)
  ) %>%
  select(-variable) %>%
  select(pair_number, is_treated, pollutant, time, concentration) %>%
  pivot_wider(names_from = is_treated, values_from = concentration)

data_raw_pair_difference_pollutant <-
  data_matched_wide_raw_pollutants %>%
  mutate(difference = treated - control) %>%
  select(-c(treated, control)) 
```

We display below the number of missing differences by pollutant and day:

```{r, layout="l-body-outset", fig.width=30, fig.height=12, code_folding="Please show me the code!"}
# make the graph
graph_missing_pollutants <- data_raw_pair_difference_pollutant %>%
  group_by(pollutant, time) %>%
  summarise(n_missing = sum(is.na(difference))) %>%
  ggplot(., aes(x = as.factor(time), y = n_missing)) +
  geom_segment(aes(
    x = as.factor(time),
    xend = as.factor(time),
    y = 0,
    yend = n_missing
  )) +
  geom_point(
    shape = 21,
    size = 4,
    colour = "black",
    fill = my_blue
  ) +
  facet_wrap( ~ pollutant) +
  xlab("Day") + ylab("Number of Pairs with Missing Concentrations") +
  theme_tufte()

# display the graph
graph_missing_pollutants

# save the graph
ggsave(
  graph_missing_pollutants,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_missing_pollutants.pdf"
  ),
  width = 40,
  height = 20,
  units = "cm",
  device = cairo_pdf
)
```

As we have `r nrow(data_matched)/2` pairs, up to 25% of the pairs can have missing pollutant concentrations. We compute below the 95% Fisherian intervals for pairs without missing concentrations and compare the results to those found with the imputed dataset:

```{r, layout="l-body-outset", fig.width=30, fig.height=12, code_folding="Please show me the code!"}
# carry out the wilcox.test
data_raw_rank_ci <- data_raw_pair_difference_pollutant %>%
  drop_na() %>%
  select(-pair_number) %>%
  group_by(pollutant, time) %>%
  nest() %>%
  mutate(
    effect = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$estimate),
    lower_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[1]),
    upper_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[2])
  ) %>%
  unnest(cols = c(effect, lower_ci, upper_ci)) %>%
  mutate(data = "Pairs without Missing Concentrations")

# bind data_rank_ci with data_raw_rank_ci
data_ci <- data_rank_ci %>%
  mutate(data = "Pairs with Imputed Pollutant Concentrations") %>%
  bind_rows(., data_raw_rank_ci)

# create an indicator to alternate shading of confidence intervals
data_ci <- data_ci %>%
  arrange(pollutant, time) %>%
  mutate(stripe = ifelse((time %% 2) == 0, "Grey", "White")) %>%
  ungroup()

# make the graph
graph_ri_ci_missing_concentration <-
  ggplot(
    data_ci,
    aes(
      x = as.factor(time),
      y = effect,
      ymin = lower_ci,
      ymax = upper_ci,
      colour = data,
      shape = data
    )
  ) +
  geom_rect(
    aes(fill = stripe),
    xmin = as.numeric(as.factor(data_ci$time)) - 0.42,
    xmax = as.numeric(as.factor(data_ci$time)) + 0.42,
    ymin = -Inf,
    ymax = Inf,
    color = NA,
    alpha = 0.1
  ) +
  geom_hline(yintercept = 0, color = "black") +
  geom_pointrange(position = position_dodge(width = 1),
                  size = 0.8,
                  fatten = 2) +
  scale_shape_manual(name = "Dataset:", values = c(16, 17)) +
  scale_color_manual(name = "Dataset:", values = c(my_orange, my_blue)) +
  facet_wrap( ~ pollutant, scales = "free_y", ncol = 4) +
  scale_fill_manual(values = c('grey90', NA)) +
  guides(fill = FALSE) +
  ylab("Constant-Additive Increase \nin Concentrations (µg/m³)") + xlab("Hour") +
  theme_tufte()

# print the graph
graph_ri_ci_missing_concentration

# save the graph
ggsave(
  graph_ri_ci_missing_concentration,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_ri_ci_missing_concentration.pdf"
  ),
  width = 70,
  height = 30,
  units = "cm",
  device = cairo_pdf
)
``` 

### Neyman's Approach: Computing Confidence Intervals for the Average Treatment Effects

We compute confidence intervals for the average treatement using Neyman's approach. We use the formula for the standard error of pair randomized experiment found in Imbens and Rubin (2015).

```{r}
# we first compute the average treatment effects for each pollutant and hour
data_pair_mean_difference <- data_pair_difference_pollutant %>%
  group_by(pollutant, time) %>%
  summarise(mean_difference = mean(difference)) %>%
  ungroup()

# we store the number of pairs
n_pair <- nrow(data_matched) / 2

# compute the standard error
data_se_neyman_pair <-
  left_join(
    data_pair_difference_pollutant,
    data_pair_mean_difference,
    by = c("pollutant", "time")
  ) %>%
  mutate(squared_difference = (difference - mean_difference) ^ 2) %>%
  group_by(pollutant, time) %>%
  summarise(standard_error = sqrt(1 / (n_pair * (n_pair - 1)) * sum(squared_difference))) %>%
  select(pollutant, time, standard_error) %>%
  ungroup()

# merge the average treatment effect data witht the standard error data
data_neyman <-
  left_join(data_pair_mean_difference,
            data_se_neyman_pair,
            by = c("pollutant", "time")) %>%
  # compute the 95% confidence intervals
  mutate(
    ci_lower_95 = mean_difference - 1.96 * standard_error,
    ci_upper_95 = mean_difference + 1.96 * standard_error
  )


saveRDS(
  data_neyman,
  here::here(
    "inputs",
    "1.data",
    "1.hourly_data",
    "2.data_for_analysis",
    "1.matched_data",
    "1.experiments_cruise",
    "1.experiment_entry_cruise",
    "data_neyman.rds"
  )
)

``` 

We plot the the point estimates for the average treatment effects and their associated 95% confidence intervals:

```{r, layout="l-body-outset", fig.width=30, fig.height=8, code_folding="Please show me the code!"}
# create an indicator to alternate shading of confidence intervals
data_neyman <- data_neyman %>%
  arrange(pollutant, time) %>%
  mutate(stripe = ifelse((time %% 2) == 0, "Grey", "White")) %>%
  ungroup()

# make the graph
graph_neyman_ci <-
  ggplot(data_neyman, aes(x = as.factor(time), y = mean_difference)) +
  geom_rect(
    aes(fill = stripe),
    xmin = as.numeric(as.factor(data_neyman$time)) - 0.42,
    xmax = as.numeric(as.factor(data_neyman$time)) + 0.42,
    ymin = -Inf,
    ymax = Inf,
    color = NA,
    alpha = 0.1
  ) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = "0", color = "black") +
  geom_pointrange(
    aes(
      x = as.factor(time),
      y = mean_difference,
      ymin = ci_lower_95 ,
      ymax = ci_upper_95
    ),
    colour = my_blue,
    lwd = 0.8
  ) +
  facet_wrap(~ pollutant, scales = "free_y", ncol = 4) +
  scale_fill_manual(values = c('grey90', NA)) +
  guides(fill = FALSE) +
  ylab("Average Pair Difference \nin Concentrations (µg/m³)") + xlab("Hour") +
  theme_tufte()

# print the graph
graph_neyman_ci

# save the graph
ggsave(
  graph_neyman_ci,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_ci_neyman.pdf"
  ),
  width = 70,
  height = 30,
  units = "cm",
  device = cairo_pdf
)
``` 


### Statistical Power Issues 

Our matching procedure resulted in few matched treated units: we might therefore have a low statistical power to detect the effect of cruise vessels on air pollutant concentrations. Even more worrying is our higher chance to suffer from type-S an type-M errors. While we do not know what the true effect of cruise on air pollutants is, we can explore our statistical power and our probability to make types S and M errors using a grid of plausible effect sizes. We proceed as follows:

* We take the standard error computed for the average treatment effect of cruise on NO2 concentration in *t* at Saint-Louis. 
* We set create a grid of plausible effect sizes.
* The `retrodesign` package allows us to compute the statistical power, the type-M and type-S errors associated with each effect size.

```{r}
# retrieve the standard error
standard_error <- data_neyman %>%
  filter(pollutant == "NO2 Saint-Louis" & time == 0) %>%
  pull(standard_error)

# create data on plausible effect sizes
data_type_m_s_errors <-
  tibble(plausible_effect = seq(from = 0.25, to = 6, by = 0.1))

# add the standard error to data_type_m_s_errors
data_type_m_s_errors <- data_type_m_s_errors %>%
  mutate(standard_error = standard_error)

# we compute power, type s and m errors
data_type_m_s_errors <- data_type_m_s_errors %>%
  mutate(
    power = map2(
      plausible_effect,
      standard_error,
      ~ retro_design(.x, .y)$power * 100
    ),
    type_m_error = map2(plausible_effect, standard_error, ~ retro_design(.x, .y)$typeM),
    type_s_error = map2(
      plausible_effect,
      standard_error,
      ~ retro_design(.x, .y)$typeS * 100
    )
  ) %>%
  unnest(c(power, type_m_error, type_s_error))
```

We plot and save the results:

```{r, layout="l-body-outset", fig.width=30, fig.height=8, code_folding="Please show me the code!"}
# make the graph
graph_type_m_s_errors <- data_type_m_s_errors %>%
  rename(
    "Power (%)" = power,
    "Type M Error (Bias)" = type_m_error,
    "Type S Error (%)" = type_s_error
  ) %>%
  pivot_longer(
    cols = -c(plausible_effect, standard_error),
    names_to = "variable",
    values_to = "value"
  ) %>%
  ggplot(., aes(x = plausible_effect, y = value)) +
  geom_line(size = 1.3, colour = my_blue) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  facet_wrap(~ variable, scales = "free") +
  xlab("Plausible Effect Sizes for an Average Increase\n in NO2 Concentrations (µg/m³)") + ylab("") +
  theme_tufte()


# print the graph
graph_type_m_s_errors

# save the graph
ggsave(
  graph_type_m_s_errors,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_type_m_s_errors.pdf"
  ),
  width = 40,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```


# Heterogeneity Analysis

```{r, fig.width=12, fig.height=6, layout="l-body-outset", code_folding="Please show me the code!"}
# prepare data on pair differences in concentration and wind direction
data_pair_difference_pollutant_0 <-
  data_pair_difference_pollutant %>%
  filter(time == 0)

data_pair_wd <- data_matched %>%
  select(pair_number, wind_direction_east_west) %>%
  distinct()

data_pair_difference_pollutant_0_wd <-
  left_join(data_pair_difference_pollutant_0, data_pair_wd, by = "pair_number") %>%
  select(-time, -pair_number)


# carry out the wilcox.test
data_rank_ci_wd <- data_pair_difference_pollutant_0_wd %>%
  group_by(pollutant, wind_direction_east_west) %>%
  nest() %>%
  mutate(
    effect = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$estimate),
    lower_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[1]),
    upper_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[2])
  ) %>%
  unnest(cols = c(effect, lower_ci, upper_ci)) %>%
  select(-data)


# make the graph
graph_wilcoxon_wd <-
  ggplot(
    data_rank_ci_wd,
    aes(
      x = wind_direction_east_west,
      y = effect,
      ymin = lower_ci,
      ymax = upper_ci,
      colour = wind_direction_east_west,
      shape = wind_direction_east_west
    )
  ) +
  geom_hline(yintercept = 0, color = "black") +
  geom_pointrange(position = position_dodge(width = 1), size = 1.2) +
  scale_shape_manual(name = "Wind Direction:", values = c(16, 17)) +
  scale_color_manual(name = "Wind Direction:", values = c(my_orange, my_blue)) +
  facet_wrap(~ pollutant, ncol = 4) +
  guides(fill = FALSE) +
  ylab("Constant-Additive Increase \nin Concentrations (µg/m³)") + xlab("Day") +
  theme_tufte()

# print the graph
graph_wilcoxon_wd

# save the graph
ggsave(
  graph_wilcoxon_wd,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_wilcoxon_wd.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```


```{r, fig.width=10, fig.height=5, layout="l-body-outset", code_folding="Please show me the code!"}
# joint concentration differences with tonnage differences
data_pair_difference_pollutant <- data_pair_difference_pollutant %>%
  rename(difference_concentration = difference)

data_matched_wide_tonnage <- data_matched %>%
  mutate(is_treated = ifelse(is_treated == TRUE, "treated", "control")) %>%
  select(is_treated,
         pair_number,
         contains("total_gross_tonnage_entry_cruise")) %>%
  pivot_longer(
    cols = -c(pair_number, is_treated),
    names_to = "variable",
    values_to = "tonnage"
  ) %>%
  mutate(
    time = 0 %>%
      ifelse(str_detect(variable, "lag_3"),-3, .) %>%
      ifelse(str_detect(variable, "lag_2"),-2, .) %>%
      ifelse(str_detect(variable, "lag_1"),-1, .) %>%
      ifelse(str_detect(variable, "lead_1"), 1, .) %>%
      ifelse(str_detect(variable, "lead_2"), 2, .) %>% 
      ifelse(str_detect(variable, "lead_3"), 3, .)
  ) %>%
  mutate(variable = "total_gross_tonnage_entry_cruise") %>%
  select(pair_number, is_treated, variable, time, tonnage) %>%
  pivot_wider(names_from = is_treated, values_from = tonnage)

data_pair_difference_tonnage <- data_matched_wide_tonnage %>%
  mutate(difference_tonnage = treated - control) %>%
  select(-c(treated, control))

data_concentration_tonnage_pair <- left_join(
  data_pair_difference_pollutant,
  data_pair_difference_tonnage,
  by = c("pair_number", "time")
) %>%
  mutate(time = ifelse(time == 1, "+1", time),
         time = paste("Hour", time, sep = " "))

# make the graph
graph_concentration_tonnage_pair <-
  data_concentration_tonnage_pair %>%
  ggplot(., aes(x =  difference_tonnage, y = difference_concentration)) +
  geom_point(shape = 16,
             colour = my_blue,
             alpha = 0.3) +
  geom_smooth(method = "lm",
              se = FALSE,
              colour = my_orange) +
  scale_x_continuous(
    breaks = scales::pretty_breaks(n = 8),
    labels = function(x)
      format(x, big.mark = " ", scientific = FALSE)
  ) +
  facet_wrap( ~ time, scales = "free_x") +
  xlab("Pair Differences in Cruise Tonnage (unitless)") + ylab("Pair Differences \nin Concentrations (µg/m³)") +
  theme_tufte()

# save the graph
ggsave(
  graph_concentration_tonnage_pair,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_concentration_tonnage_pair.pdf"
  ),
  width = 35,
  height = 12,
  units = "cm",
  device = cairo_pdf
)
``` 



# Alternative Matching

We implement below a propensity score matching procedure where:

1. Each day with an heat wave is matched to the most similar day without heat wave. This is a 1:1 nearest neighbor matching without replacement.
2. The distance metric used for the matching is the propensity score which is predicted using a logistic model where we regress the treatment indicator on a set of vessel traffic, weather and calendar variables. We choose a caliper equals to 0.01 of the standard deviation of the propensity scores.
3. Once treated and control units are matched, we assess whether covariates balance has improved. 
4. We finally estimate the treatment effect.


First, we load the matching dataset:

```{r, eval = FALSE}
# load matching data
data_matching <-
  readRDS(
    here::here(
      "inputs",
      "1.data",
      "1.hourly_data",
      "2.data_for_analysis",
      "1.matched_data",
      "1.experiments_cruise",
      "1.experiment_entry_cruise",
      "matching_data.rds"
    )
  )


# select relevant covariates
data_matching <- data_matching %>%
  select(
    contains("mean_no2_l"),
    contains("mean_no2_sl"),
    contains("mean_pm10_l"),
    contains("mean_pm10_sl"),
    contains("mean_pm25_l"),
    contains("mean_so2_l"),
    contains("mean_o3_l"),
    is_treated,
    total_gross_tonnage_entry_ferry,
    total_gross_tonnage_entry_ferry_lag_1,
    total_gross_tonnage_entry_other_vessels,
    total_gross_tonnage_entry_other_vessels_lag_1,
    total_gross_tonnage_entry_cruise_lag_1,
    total_gross_tonnage_exit_cruise,
    total_gross_tonnage_exit_ferry,
    total_gross_tonnage_exit_ferry_lag_1,
    total_gross_tonnage_exit_other_vessels,
    total_gross_tonnage_exit_other_vessels_lag_1,
    total_gross_tonnage_exit_cruise_lag_1,
    temperature_average,
    temperature_average_lag_1,
    humidity_average,
    humidity_average_lag_1,
    rainfall_height_dummy,
    rainfall_height_dummy_lag_1,
    wind_direction_east_west,
    wind_direction_east_west_lag_1,
    wind_speed,
    wind_speed_lag_1,
    hour,
    weekday,
    holidays_dummy,
    bank_day_dummy,
    month,
    year
  ) %>%
  drop_na() %>%
  mutate_at(vars(hour, weekday, month, year), ~ as.factor(.))
``` 

We first match each treated unit to its closest control unit using the `matchit()` function and 0.01 caliper:

```{r, eval = FALSE}
# match without caliper
matching_ps  <-
  matchit(
    is_treated ~
      total_gross_tonnage_entry_ferry +
      total_gross_tonnage_entry_ferry_lag_1 +
      total_gross_tonnage_entry_other_vessels +
      total_gross_tonnage_entry_other_vessels_lag_1 +
      total_gross_tonnage_entry_cruise_lag_1 +
      total_gross_tonnage_exit_cruise +
      total_gross_tonnage_exit_ferry +
      total_gross_tonnage_exit_ferry_lag_1 +
      total_gross_tonnage_exit_other_vessels +
      total_gross_tonnage_exit_other_vessels_lag_1 +
      total_gross_tonnage_exit_cruise_lag_1 +
      temperature_average + temperature_average_lag_1 +
      humidity_average + humidity_average_lag_1 +
      rainfall_height_dummy + rainfall_height_dummy_lag_1 +
      wind_direction_east_west + wind_direction_east_west_lag_1 +
      wind_speed + wind_speed_lag_1 +
      hour + weekday + holidays_dummy + bank_day_dummy + month * year,
    caliper = 0.01,
    data = data_matching
  )

# display summary of the procedure
matching_ps
```

The output of the matching procedure indicates us the method (1:1 nearest neighbor matching without replacement) and the distance (propensity score) we used. It also tells that out of the  units,  were matched to similar controls. We assess how covariates balance has improved by comparing the distribution of propensity scores before and after matching:


```{r, eval = FALSE, fig.width = 10, fig.height = 6, code_folding="Please show me the code!"}
# distribution of propensity scores
graph_propensity_score <- bal.plot(
  matching_ps,
  var.name = "distance",
  which = "both",
  sample.names = c("Initial Data", "Matched Data"),
  type = "density") +
  xlab("Propensity Scores") +
  scale_fill_manual(
    name = "Group:",
    values = c(my_blue, my_orange),
    labels = c("Days without Cruise Vessels", "Days with Cruise Vessels")
  ) +
  theme_tufte()

# display the graph
graph_propensity_score

# save the graph
ggsave(
  graph_propensity_score,
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_propensity_score.pdf"
  ),
  width = 16,
  height = 10,
  units = "cm",
  device = cairo_pdf
)
```

We see on this graph that propensity scores distribution for the two groups perfectly overlap after matching. We can also evaluate the covariates balance using the `love.plot()` function from the cobalt package and the absolute mean difference as the summary statistic. For binary variables, the absolute difference in proportion is computed. For continuous covariates, denoted with a star, the absolute standardized mean difference is computed (the difference is divided by the standard deviation of the variable for treated units before matching).

```{r, eval = FALSE, fig.width = 8, fig.height = 8, code_folding="Please show me the code!"}
# make the love plot
graph_love_plot_ps <- love.plot(
  is_treated ~
      total_gross_tonnage_entry_ferry +
      total_gross_tonnage_entry_ferry_lag_1 +
      total_gross_tonnage_entry_other_vessels +
      total_gross_tonnage_entry_other_vessels_lag_1 +
      total_gross_tonnage_entry_cruise_lag_1 +
      total_gross_tonnage_exit_cruise +
      total_gross_tonnage_exit_ferry +
      total_gross_tonnage_exit_ferry_lag_1 +
      total_gross_tonnage_exit_other_vessels +
      total_gross_tonnage_exit_other_vessels_lag_1 +
      total_gross_tonnage_exit_cruise_lag_1 +
    mean_no2_l_lag_1 + mean_no2_sl_lag_1 + mean_pm10_l_lag_1 + mean_pm10_sl_lag_1 +
    mean_pm25_l_lag_1 + mean_so2_l_lag_1 + mean_o3_l_lag_1 +
    temperature_average + temperature_average_lag_1 +
    humidity_average + humidity_average_lag_1 +
    rainfall_height_dummy + rainfall_height_dummy_lag_1 +
    wind_direction_east_west + wind_direction_east_west_lag_1 +
    wind_speed + wind_speed_lag_1 +
    hour + weekday + holidays_dummy + bank_day_dummy + month + year,
  data = data_matching,
  weights = matching_ps,
  drop.distance = TRUE,
  abs = TRUE,
  var.order = "unadjusted",
  binary = "raw",
  s.d.denom = "treated",
  thresholds = c(m = .1),
  #var.names = cov_labels,
  sample.names = c("Initial Data", "Matched Data"),
  shapes = c("circle", "triangle"),
  colors = c(my_orange, my_blue),
  #stars = "std"
) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Absolute Mean Differences") +
  theme_tufte()

# display the graph
graph_love_plot_ps

# save the graph
ggsave(
  graph_love_plot_ps + labs(title = NULL),
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_love_plot_ps.pdf"
  ),
  width = 20,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```

We display below the evolution of the average of standardized mean differences for continuous covariates:

```{r, eval = FALSE, code_folding="Please show me the code!"}
graph_love_plot_ps[["data"]] %>%
  filter(type == "Contin.") %>%
  group_by(Sample) %>%
  summarise("Average of Standardized Mean Differences" = round(mean(stat), 2),
            "Std. Deviation" = round(sd(stat), 2)) %>%
  kable(align = c("l", "c")) %>%
  kable_styling(position = "center")
```

We also display below the evolution of the difference in proportions for binary covariates:

```{r, eval = FALSE, code_folding="Please show me the code!"}
graph_love_plot_ps[["data"]] %>%
  filter(type == "Binary") %>%
  group_by(Sample) %>%
  summarise("Average of Proportion Differences" = round(mean(stat), 2),
            "Std. Deviation" = round(sd(stat), 2)) %>%
  kable(align = c("l", "c")) %>%
  kable_styling(position = "center")
```

Overall, for both types of covariates, the balance has clearly improved after matching.


We now move to the analysis of the matched datasets using a simple regression model where we first regress the YLL on the treatment indicator. We start with the matched data resulting from the propensity score without a caliper. We first retrieve the matched dataset:

```{r, eval = FALSE}
# we retrieve the matched data
data_ps <- match.data(matching_ps)
```

Balance check:

```{r, eval = FALSE}
# we select covariates
data_covs <- data_ps %>%
  select(
    is_treated:year,
    subclass,
    contains("lag_1")
  )

# recode some variables
data_covs <- data_covs %>%
  mutate(is_treated = ifelse(is_treated == TRUE, 1, 0)) %>%
  mutate_at(
    vars(wind_direction_east_west , wind_direction_east_west_lag_1),
    ~ ifelse(. == "West", 1, 0)
  ) %>%
  fastDummies::dummy_cols(., select_columns = c("year", "month", "weekday", "hour")) %>%
  select(-c("year", "month", "weekday", "hour"))

# format data for asIfRandPlot() function
subclass <- as.numeric(data_covs$subclass)
is_treated <- data_covs$is_treated

data_covs <- data_covs %>%
  select(-subclass,-is_treated) %>%
  select(-year_2018, - month_December, -weekday_Sunday, -hour_0) 

# run balance check
asIfRandPlot(
  X.matched = data_covs,
  indicator.matched = is_treated,
  assignment = c("complete", "blocked"),
  subclass = subclass,
  perms = 100
)
```


```{r, eval = FALSE, echo = FALSE, out.width = "800%", fig.align = "center"}
knitr::include_graphics(
    here::here(
          "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "ri_balance_check_ps.png"))
```

We then estimate the treatment effect of heat waves with a simple linear regression model:

```{r, eval = FALSE}
function_ps_analysis <- function(data) {
  
  # we fit the regression model
  model_ps <- lm(concentration ~ is_treated,
                 data = data,
                 weights = weights)
  
  # retrieve the estimate and 95% ci
  results_ps <- tidy(coeftest(model_ps,
                              vcov. = vcovCL,
                              cluster = ~ subclass),
                     conf.int = TRUE) %>%
    filter(term == "is_treatedTRUE") %>%
    select(term, estimate, conf.low, conf.high)
  
  # return output
  return(results_ps)
}


# reshape in long according to pollutants
data_ps_analysis <- data_ps %>%
  pivot_longer(cols = c(contains("no2"), contains("o3"), contains("pm10"), contains("pm25"), contains("so2")), names_to = "variable", values_to = "concentration") %>%
  mutate(pollutant = NA %>%
           ifelse(str_detect(variable, "no2_l"), "NO2 Longchamp",.) %>%
           ifelse(str_detect(variable, "no2_sl"), "NO2 Saint-Louis",.) %>%
           ifelse(str_detect(variable, "o3"), "O3 Longchamp",.) %>%
           ifelse(str_detect(variable, "pm10_l"), "PM10 Longchamp",.) %>%
           ifelse(str_detect(variable, "pm10_sl"), "PM10 Saint-Louis",.) %>%
           ifelse(str_detect(variable, "pm25"), "PM2.5 Longchamp",.) %>%
           ifelse(str_detect(variable, "so2"), "SO2 Lonchamp",.)) %>%
  mutate(    time = 0 %>%
      ifelse(str_detect(variable, "lag_3"),-3, .) %>%
      ifelse(str_detect(variable, "lag_2"),-2, .) %>%
      ifelse(str_detect(variable, "lag_1"),-1, .) %>%
      ifelse(str_detect(variable, "lead_1"), 1, .) %>%
      ifelse(str_detect(variable, "lead_2"), 2, .) %>% 
      ifelse(str_detect(variable, "lead_3"), 3, .)) %>%
  select(-variable)

# we nest the data by pollutant and time
data_ps_analysis <- data_ps_analysis %>% 
  group_by(pollutant, time) %>%
  nest()

data_ps_analysis <- data_ps_analysis %>%
  mutate(result = map(data, ~ function_ps_analysis(.))) %>%
  select(-data) %>%
  unnest() %>%
  select(-term) %>%
  rename("mean_difference" = "estimate", "ci_lower_95" = "conf.low", "ci_upper_95" = "conf.high") %>%
  mutate(analysis = "Propensity Score Matching")

data_neyman <- data_neyman %>%
  mutate(analysis = "Constrained Pair Matching")

data_neyman_ps <- bind_rows(data_neyman, data_ps_analysis)

# make the graph

# create an indicator to alternate shading of confidence intervals
data_neyman_ps <- data_neyman_ps %>%
  arrange(pollutant, time) %>%
  mutate(stripe = ifelse((time %% 2) == 0, "Grey", "White")) %>%
  ungroup()

# make the graph
graph_ps_analysis <-
  ggplot(
    data_neyman_ps,
    aes(
      x = as.factor(time),
      y = mean_difference,
      ymin = ci_lower_95,
      ymax = ci_upper_95,
      colour = analysis,
      shape = analysis
    )
  ) +
  geom_rect(
    aes(fill = stripe),
    xmin = as.numeric(as.factor(data_neyman_ps$time)) - 0.42,
    xmax = as.numeric(as.factor(data_neyman_ps$time)) + 0.42,
    ymin = -Inf,
    ymax = Inf,
    color = NA,
    alpha = 0.4
  ) +
  geom_hline(yintercept = 0, color = "black") +
  geom_pointrange(position = position_dodge(width = 1), size = 1.2) +
  scale_shape_manual(name = "Analysis:", values = c(16, 17)) +
  scale_color_manual(name = "Analysis:", values = c(my_orange, my_blue)) +
  facet_wrap( ~ pollutant, ncol = 4) +
  scale_fill_manual(values = c('grey90', "white")) +
  guides(fill = FALSE) +
  ylab("Average Difference \nin Concentrations (µg/m³)") + xlab("Day") +
  theme_tufte()

# display the graph
graph_ps_analysis

# save the graph
ggsave(
  graph_ps_analysis + labs(title = NULL),
  filename = here::here(
    "inputs",
    "3.outputs",
    "1.hourly_analysis",
    "2.experiments_cruise",
    "1.experiment_entry_cruise",
    "2.matching_results",
    "graph_ps_analysis.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```










