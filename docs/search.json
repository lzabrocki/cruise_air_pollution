{
  "articles": [
    {
      "path": "2_3_daily_matching_procedure.html",
      "title": "Matching Procedure",
      "description": "Comparing days with cruise traffic to days without. Adjusting for calendar and weather indicators.\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nPreparing the Data for Matching\r\nSelecting and Creating Relevant Variables\r\nCreating Potential Experiments\r\n\r\nMatching Procedure\r\nDefining Thresholds for Matching Covariates\r\nRunning the Matching Procedure\r\n\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we provide all steps required to reproduce our matching procedure at the daily level. We compare days where:\r\ntreated units are days with cruise traffic in t.\r\ncontrol units are day without cruise traffic in t.\r\nWe adjust for calendar calendar indicator and weather confounding factors.\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce exactly the 2_daily_matching_procedure.html document, we first need to have installed:\r\nthe R programming language on your computer\r\nRStudio, an integrated development environment for R, which will allow you to knit the 2_daily_matching_procedure.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template of this document.\r\nOnce everything is set up, we have to load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(Rcpp) # for running the matching algorithm\r\nlibrary(optmatch) # for matching pairs\r\nlibrary(igraph) # for pair matching via bipartite maximal weighted matching\r\n\r\n\r\n\r\nWe also have to load the script_time_series_matching_function.R which provides the functions used for matching time series:\r\n\r\n\r\n# load matching functions\r\nsource(here::here(\r\n  \"inputs\",\r\n  \"2.functions\",\r\n  \"script_time_series_matching_function.R\"\r\n))\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\r\n  \"inputs\",\r\n  \"2.functions\",\r\n  \"script_theme_tufte.R\"\r\n))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nPreparing the Data for Matching\r\nSelecting and Creating Relevant Variables\r\nFirst, we load the data:\r\n\r\n\r\n# load data\r\ndata <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"0.main_data\",\r\n      \"data_for_analysis_daily.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nThen, we select relevant variables for matching and create new variables which we store in the processed_data:\r\n\r\n\r\n# select relevant variables\r\nrelevant_variables <- c(\r\n  \"date\",\r\n  \"mean_no2_l\",\r\n  \"mean_no2_sl\",\r\n  \"mean_pm10_l\",\r\n  \"mean_pm10_sl\",\r\n  \"mean_pm25_l\",\r\n  \"mean_so2_l\",\r\n  \"mean_o3_l\",\r\n  \"total_gross_tonnage\",\r\n  \"total_gross_tonnage_cruise\",\r\n  \"total_gross_tonnage_ferry\",\r\n  \"total_gross_tonnage_other_boat\",\r\n  \"temperature_average\",\r\n  \"rainfall_height_dummy\",\r\n  \"humidity_average\",\r\n  \"wind_speed\",\r\n  \"wind_direction_categories\",\r\n  \"wind_direction_east_west\",\r\n  \"road_traffic_flow_all\",\r\n  \"road_occupancy_rate\",\r\n  \"weekday\",\r\n  \"weekend\",\r\n  \"holidays_dummy\",\r\n  \"bank_day_dummy\",\r\n  \"month\",\r\n  \"season\",\r\n  \"year\"\r\n)\r\n\r\n# create processed_data with the relevant variables\r\nif (exists(\"relevant_variables\") && !is.null(relevant_variables)) {\r\n  # extract relevant variables (if specified)\r\n  processed_data = data[relevant_variables]\r\n} else {\r\n  processed_data = data\r\n}\r\n\r\n# create julian date and day of the year to define time windows\r\nprocessed_data <- processed_data %>%\r\n  mutate(julian_date = julian(date),\r\n         day_of_year = lubridate::yday(date))\r\n\r\n#\r\n# re-order columns\r\n#\r\n\r\nprocessed_data <- processed_data %>%\r\n  select(\r\n    # date variable\r\n    \"date\",\r\n    # pollutants\r\n    \"mean_no2_l\",\r\n    \"mean_no2_sl\",\r\n    \"mean_pm10_l\",\r\n    \"mean_pm10_sl\",\r\n    \"mean_pm25_l\",\r\n    \"mean_so2_l\",\r\n    \"mean_o3_l\",\r\n    # maritime traffic variables\r\n    \"total_gross_tonnage\",\r\n    \"total_gross_tonnage_cruise\",\r\n    \"total_gross_tonnage_ferry\",\r\n    \"total_gross_tonnage_other_boat\",\r\n    # weather parameters\r\n    \"temperature_average\",\r\n    \"rainfall_height_dummy\",\r\n    \"humidity_average\",\r\n    \"wind_speed\",\r\n    \"wind_direction_categories\",\r\n    \"wind_direction_east_west\",\r\n    # road traffic variables\r\n    \"road_traffic_flow_all\",\r\n    \"road_occupancy_rate\",\r\n    # calendar indicators\r\n    \"julian_date\",\r\n    \"day_of_year\",\r\n    \"weekday\",\r\n    \"weekend\",\r\n    \"holidays_dummy\",\r\n    \"bank_day_dummy\",\r\n    \"month\",\r\n    \"season\",\r\n    \"year\"\r\n  )\r\n\r\n\r\n\r\nFor each covariate, we create the first daily lags and leads:\r\n\r\n\r\n# we first define processed_data_leads and processed_data_lags\r\n# to store leads and lags\r\n\r\nprocessed_data_leads <- processed_data\r\nprocessed_data_lags <- processed_data\r\n\r\n#\r\n# create leads\r\n#\r\n\r\n# create a list to store dataframe of leads\r\nleads_list <- vector(mode = \"list\", length = 1)\r\nnames(leads_list) <- c(1)\r\n\r\n# create the leads\r\nfor (i in 1) {\r\n  leads_list[[i]] <- processed_data_leads %>%\r\n    mutate_at(vars(-date), ~  lead(., n = i, order_by = date)) %>%\r\n    rename_at(vars(-date), function(x)\r\n      paste0(x, \"_lead_\", i))\r\n}\r\n\r\n# merge the dataframes of leads\r\ndata_leads <- leads_list %>%\r\n  reduce(left_join, by = \"date\")\r\n\r\n# merge the leads with the processed_data_leads\r\nprocessed_data_leads <-\r\n  left_join(processed_data_leads, data_leads, by = \"date\") %>%\r\n  select(-c(mean_no2_l:year))\r\n\r\n#\r\n# create lags\r\n#\r\n\r\n# create a list to store dataframe of lags\r\nlags_list <- vector(mode = \"list\", length = 1)\r\nnames(lags_list) <- c(1)\r\n\r\n# create the lags\r\nfor (i in 1) {\r\n  lags_list[[i]] <- processed_data_lags %>%\r\n    mutate_at(vars(-date), ~  lag(., n = i, order_by = date)) %>%\r\n    rename_at(vars(-date), function(x)\r\n      paste0(x, \"_lag_\", i))\r\n}\r\n\r\n# merge the dataframes of lags\r\ndata_lags <- lags_list %>%\r\n  reduce(left_join, by = \"date\")\r\n\r\n# merge the lags with the initial processed_data_lags\r\nprocessed_data_lags <-\r\n  left_join(processed_data_lags, data_lags, by = \"date\")\r\n\r\n#\r\n# merge processed_data_leads with processed_data_lags\r\n#\r\n\r\nprocessed_data <-\r\n  left_join(processed_data_lags, processed_data_leads, by = \"date\")\r\n\r\n\r\n\r\nWe can now define the hypothetical experiment that we would like to investigate.\r\nCreating Potential Experiments\r\nWe defined our potential experiments such that:\r\ntreated units are days with cruise traffic in t.\r\ncontrol units are day without cruise traffic in t.\r\nBelow are the required steps to select the corresponding treated and control units whose observations are stored in the matching_data:\r\n\r\n\r\n# construct treatment assigment variable\r\nprocessed_data <- processed_data %>%\r\n  mutate(is_treated = NA) %>%\r\n  mutate(is_treated = ifelse(total_gross_tonnage_cruise > 0, TRUE, is_treated)) %>%\r\n  mutate(is_treated = ifelse(total_gross_tonnage_cruise == 0 , FALSE, is_treated))\r\n\r\n# remove the days for which assignment is undefined\r\nmatching_data = processed_data[!is.na(processed_data$is_treated), ]\r\n\r\n# susbet treated and control units\r\ntreated_units = subset(matching_data, is_treated)\r\ncontrol_units = subset(matching_data, !is_treated)\r\nN_treated = nrow(treated_units)\r\nN_control = nrow(control_units)\r\n\r\n\r\n\r\nThere are 2485 treated units and 1532 control units. We display the distribution of treated and control units through time:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make stripes graph\r\ngraph_stripes_daily_experiment <- matching_data %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"Treated\", \"Control\")) %>%\r\n  ggplot(., aes(x = date, y = 1, fill = is_treated)) +\r\n  geom_tile() +\r\n  scale_x_date(breaks = scales::pretty_breaks(n = 10)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  scale_fill_manual(name = \"Daily Observations:\", values = c(my_blue, my_orange)) +\r\n  xlab(\"Date\") +\r\n  theme_tufte() +\r\n  theme(\r\n    panel.grid.major.y = element_blank(),\r\n    axis.ticks.x = element_blank(),\r\n    axis.ticks.y = element_blank(),\r\n    axis.title.y = element_blank(),\r\n    axis.text.y = element_blank()\r\n  )\r\n\r\n# display the graph\r\ngraph_stripes_daily_experiment\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_stripes_daily_experiment,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_stripes_daily_experiment.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe save the matching_data :\r\n\r\n\r\n# save the matching data\r\nsaveRDS(\r\n  matching_data,\r\n  here::here(\r\n    \"inputs\",\r\n    \"1.data\",\r\n    \"2.daily_data\",\r\n    \"2.data_for_analysis\",\r\n    \"1.cruise_experiment\",\r\n    \"matching_data.RDS\"\r\n  )\r\n)\r\n\r\n\r\n\r\nMatching Procedure\r\nDefining Thresholds for Matching Covariates\r\nBelow is the code to define the relevant thresholds:\r\n\r\n\r\n# we create the scaling list as it is needed for running the algorithm\r\n# but we do not use it\r\n\r\nscaling =  rep(list(1), ncol(matching_data))\r\nnames(scaling) = colnames(matching_data)\r\n\r\n# instead, we manually defined the threshold for each covariate\r\nthresholds = rep(list(Inf), ncol(matching_data))\r\nnames(thresholds) = colnames(matching_data)\r\n\r\n# threshold for weekday\r\nthresholds$weekday = 0\r\n\r\n# threshold for holidays\r\nthresholds$holidays_dummy = 0\r\nthresholds$holidays_dummy_lag_1 = 0\r\n\r\n# threshold for bank days\r\nthresholds$bank_day_dummy = 0\r\nthresholds$bank_day_dummy_lag_1 = 0\r\n\r\n# threshold for distance in julian days\r\nthresholds$julian_date = 1095\r\n\r\n# threshold for month\r\nthresholds$month = 0\r\n\r\n# thresholds for average temperature\r\nthresholds$temperature_average = 4\r\nthresholds$temperature_average_lag_1 = 4\r\n\r\n# threshold for wind speed\r\nthresholds$wind_speed = 2\r\nthresholds$wind_speed_lag_1 = 2\r\n\r\n# threshold for east-west wind direction dummy\r\nthresholds$wind_direction_east_west = 0\r\nthresholds$wind_direction_east_west_lag_1 = 0\r\n\r\n# threshold for rainfall height dummy\r\nthresholds$rainfall_height_dummy = 0\r\nthresholds$rainfall_height_dummy_lag_1 = 0\r\n\r\n\r\n\r\nRunning the Matching Procedure\r\nWe compute discrepancy matrix and run the matching algorithm:\r\n\r\n\r\n# first we compute the discrepancy matrix\r\ndiscrepancies = discrepancyMatrix(treated_units, control_units, thresholds, scaling)\r\n\r\n# convert matching data to data.frame\r\nmatching_data <- as.data.frame(matching_data)\r\n\r\nrownames(discrepancies) = format(matching_data$date[which(matching_data$is_treated)], \"%Y-%m-%d\")\r\ncolnames(discrepancies) = format(matching_data$date[which(!matching_data$is_treated)], \"%Y-%m-%d\")\r\nrownames(matching_data) = matching_data$date\r\n\r\n# run the fullmatch algorithm\r\nmatched_groups = fullmatch(\r\n  discrepancies,\r\n  data = matching_data,\r\n  remove.unmatchables = TRUE,\r\n  max.controls = 1\r\n)\r\n\r\n# get list of matched  treated-control groups\r\ngroups_labels = unique(matched_groups[!is.na(matched_groups)])\r\ngroups_list = list()\r\nfor (i in 1:length(groups_labels)) {\r\n  IDs = names(matched_groups)[(matched_groups == groups_labels[i])]\r\n  groups_list[[i]] = as.Date(IDs[!is.na(IDs)])\r\n}\r\n\r\n\r\n\r\nFor somes cases, several controls units were matched to a treatment unit. We use the igraph package to force pair matching via bipartite maximal weighted matching. Below is the required code:\r\n\r\n\r\n# we build a bipartite graph with one layer of treated nodes, and another layer of control nodes.\r\n# the nodes are labeled by integers from 1 to (N_treated + N_control)\r\n# by convention, the first N_treated nodes correspond to the treated units, and the remaining N_control\r\n# nodes correspond to the control units.\r\n\r\n# build pseudo-adjacency matrix: edge if and only if match is admissible\r\n# NB: this matrix is rectangular so it is not per say the adjacendy matrix of the graph\r\n# (for this bipartite graph, the adjacency matrix had four blocks: the upper-left block of size\r\n# N_treated by N_treated filled with 0's, bottom-right block of size N_control by N_control filled with 0's,\r\n# top-right block of size N_treated by N_control corresponding to adj defined below, and bottom-left block\r\n# of size N_control by N_treated corresponding to the transpose of adj)\r\nadj = (discrepancies<Inf)\r\n\r\n# extract endpoints of edges\r\nedges_mat = which(adj,arr.ind = TRUE)\r\n\r\n# build weights, listed in the same order as the edges (we use a decreasing function x --> 1/(1+x) to\r\n# have weights inversely proportional to the discrepancies, since maximum.bipartite.matching\r\n# maximizes the total weight and we want to minimize the discrepancy)\r\nweights = 1/(1+sapply(1:nrow(edges_mat),function(i)discrepancies[edges_mat[i,1],edges_mat[i,2]]))\r\n\r\n# format list of edges (encoded as a vector resulting from concatenating the end points of each edge)\r\n# i.e c(edge1_endpoint1, edge1_endpoint2, edge2_endpoint1, edge2_endpoint1, edge3_endpoint1, etc...)\r\nedges_mat[,\"col\"] = edges_mat[,\"col\"] + N_treated\r\nedges_vector = c(t(edges_mat))\r\n\r\n# NB: by convention, the first N_treated nodes correspond to the treated units, and the remaining N_control\r\n# nodes correspond to the control units (hence the \"+ N_treated\" to shift the labels of the control nodes)\r\n\r\n# build the graph from the list of edges\r\nBG = make_bipartite_graph(c(rep(TRUE,N_treated),rep(FALSE,N_control)), edges = edges_vector)\r\n\r\n# find the maximal weighted matching\r\nMBM = maximum.bipartite.matching(BG,weights = weights)\r\n\r\n# list the dates of the matched pairs\r\npairs_list = list()\r\nN_matched = 0\r\nfor (i in 1:N_treated){\r\n  if (!is.na(MBM$matching[i])){\r\n    N_matched = N_matched + 1\r\n    pairs_list[[N_matched]] = c(treated_units$date[i],control_units$date[MBM$matching[i]-N_treated])\r\n  }\r\n}\r\n\r\n# transform the list of matched pairs to a dataframe\r\nmatched_pairs <- enframe(pairs_list) %>%\r\n  unnest(cols = \"value\") %>%\r\n  rename(pair_number = name,\r\n         date = value)\r\n\r\n\r\n\r\nThe hypothetical experiment we set up had 2485 treated units and 1532 control units. The matching procedure results in 189 matched treated units. We remove pairs separated by less than 7 days to avoid spillover effects:\r\n\r\n\r\n# define distance for days\r\nthreshold_day = 6\r\n\r\n# define distance for months\r\nthreshold_month_low = 3\r\nthreshold_month_high = 9\r\n\r\n# find pairs that should be removed\r\npairs_to_remove <- matched_pairs %>%\r\n  mutate(month = lubridate::month(date)) %>%\r\n  #  compute pair differences in days and months\r\n  group_by(pair_number) %>%\r\n  summarise(\r\n    difference_days = abs(date - dplyr::lag(date)),\r\n    difference_month = abs(month - dplyr::lag(month))\r\n  ) %>%\r\n  drop_na() %>%\r\n  # select pair according to the criteria\r\n  mutate(day_criteria = ifelse(difference_days < threshold_day, 1, 0)) %>%\r\n  mutate(\r\n    month_criteria = ifelse(\r\n      difference_month > threshold_month_low &\r\n        difference_month < threshold_month_high,\r\n      1,\r\n      0\r\n    )\r\n  ) %>%\r\n  filter(day_criteria == 0 & month_criteria == 0) %>%\r\n  pull(pair_number)\r\n\r\n\r\n# remove these pairs\r\nmatched_pairs <- matched_pairs %>%\r\n  filter(pair_number  %in% pairs_to_remove)\r\n\r\n\r\n\r\nOur final number of matched treated days is therefore 189. We finally merge the matched_pairs with the matching_matching_data to retrieve covariate values for the matched pairs and save the data:\r\n\r\n\r\n# select the matched data for the analysis\r\nfinal_data <- left_join(matched_pairs, matching_data, by = \"date\")\r\n\r\n# save the matched data\r\nsaveRDS(\r\n  final_data,\r\n  here::here(\r\n    \"inputs\",\r\n    \"1.data\",\r\n    \"2.daily_data\",\r\n    \"2.data_for_analysis\",\r\n    \"1.cruise_experiment\",\r\n    \"matched_data.RDS\"\r\n  )\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-21T13:16:31+02:00"
    },
    {
      "path": "2_5_daily_checking_intervention.html",
      "title": "Checking the Hypothetical Intervention",
      "description": "Comparing days with cruise traffic to days without. Adjusting for calendar and weather indicators.\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nPreparing the Data\r\nChecking the Hypothetical Intervention\r\nChecking Other Vessels’ Types Traffic Evolution\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we take great care providing all steps and R codes required to check the intervention we set up in our matching procedure. We compare days where:\r\ntreated units are days with cruise traffic in t.\r\ncontrol units are day without cruise traffic in t.\r\nWe adjust for calendar calendar indicators and weather confouding factors.\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce exactly the 2_5_checking_intervention.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the 2_5_checking_intervention.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we have to load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(ggridges) # for ridge density plots\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(patchwork) # combining plots\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\",\r\n                  \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nPreparing the Data\r\nWe load the matched data:\r\n\r\n\r\n# load matched data\r\ndata_matched <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"1.cruise_experiment\",\r\n      \"matched_data.rds\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nChecking the Hypothetical Intervention\r\nWe compute the difference in the daily cruise total gross tonnage for each pair:\r\n\r\n\r\n# compute the difference in tonnage by pair\r\npair_difference_tonnage_t <- data_matched %>%\r\n  select(total_gross_tonnage_cruise, is_treated, pair_number) %>%\r\n  arrange(pair_number, is_treated) %>%\r\n  select(-is_treated) %>%\r\n  group_by(pair_number) %>%\r\n  summarise(tonnage_difference = total_gross_tonnage_cruise[2] - total_gross_tonnage_cruise[1])\r\n\r\n\r\n\r\nWe find on average, a 1.85437^{5} difference in gross tonnage between treated and control units. Below is the distribution of the pair difference in hourly gross tonnage in t:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# plot the graph\r\ngraph_tonnage_difference_density <-\r\n  ggplot(pair_difference_tonnage_t, aes(x = tonnage_difference)) +\r\n  geom_density(\r\n    colour = my_blue,\r\n    size = 1.1,\r\n    alpha = 0.8\r\n  ) +\r\n  geom_vline(\r\n    xintercept = mean(pair_difference_tonnage_t$tonnage_difference),\r\n    size = 1.1,\r\n    color = my_orange\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = scales::pretty_breaks(n = 8),\r\n    labels = function(x)\r\n      format(x, big.mark = \" \", scientific = FALSE)\r\n  ) +\r\n  xlab(\"Pair Difference in Tonnage\") + ylab(\"Density\") +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_tonnage_difference_density\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_tonnage_difference_density,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_tonnage_difference_cruise_tonnage_density.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nTo check which hypothetical intervention we study, we plot below the average tonnage for each day and for treated and control groups :\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute mean tonnage for each day\r\ndata_mean_tonnage_day <- data_matched %>%\r\n  # select relevant variables\r\n  select(pair_number,\r\n         is_treated,\r\n         contains(\"total_gross_tonnage_cruise\")) %>%\r\n  # transform data in long format\r\n  pivot_longer(\r\n    cols = -c(pair_number, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"tonnage\"\r\n  ) %>%\r\n  # create the day variable\r\n  mutate(\r\n    time = 0 %>%\r\n      ifelse(str_detect(variable, \"lag_1\"),-1, .) %>%\r\n      ifelse(str_detect(variable, \"lead_1\"), 1, .)\r\n  ) %>%\r\n  # rename the labels of the is_treated dummy\r\n  mutate(is_treated = ifelse(is_treated == TRUE, \"Treated\", \"Control\")) %>%\r\n  # compute the mean tonnage for each day and pollutant\r\n  group_by(variable, is_treated, time) %>%\r\n  summarise(tonnage = mean(tonnage, na.rm = TRUE))\r\n\r\n# plot the graph\r\ngraph_mean_tonnage_day <-\r\n  ggplot(\r\n    data_mean_tonnage_day,\r\n    aes(\r\n      x = as.factor(time),\r\n      y = tonnage,\r\n      group = is_treated,\r\n      colour = is_treated,\r\n      fill = is_treated\r\n    )\r\n  )  +\r\n  geom_segment(\r\n    x = 2,\r\n    y = 0,\r\n    xend = 2,\r\n    yend = 184233,\r\n    lineend = \"round\",\r\n    # See available arrow types in example above\r\n    linejoin = \"round\",\r\n    size = 0.5,\r\n    colour = \"black\"\r\n  ) +\r\n  geom_line() +\r\n  geom_point(shape = 21,\r\n             size = 4,\r\n             colour = \"black\") +\r\n  scale_colour_manual(values = c(my_blue, my_orange)) +\r\n  scale_fill_manual(values = c(my_blue, my_orange)) +\r\n  scale_y_continuous(\r\n    breaks = scales::pretty_breaks(n = 8),\r\n    labels = function(x)\r\n      format(x, big.mark = \" \", scientific = FALSE)\r\n  ) +\r\n  labs(fill = \"Group:\") +\r\n  xlab(\"Day\") + ylab(\"Daily Cruise Gross Tonnage\") +\r\n  theme_tufte() +\r\n  guides(color = FALSE)\r\n\r\n# we print the graph\r\ngraph_mean_tonnage_day\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\ngraph_mean_tonnage_day <- graph_mean_tonnage_day +\r\n  theme(plot.title = element_blank())\r\n\r\nggsave(\r\n  graph_mean_tonnage_day,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_mean_cruise_tonnage_day.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nChecking Other Vessels’ Types Traffic Evolution\r\nWe also check how the difference in tonnage for other vessels’ types between treated and control units evolves:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we create a table with the tonnage for each pair,\r\n# for each vessel type,\r\n# and for -6 hours to + 6 hours\r\ndata_vessel_type_tonnage <- data_matched %>%\r\n  # relabel treatment indicator\r\n  mutate(is_treated = ifelse(is_treated == TRUE, \"treated\", \"control\")) %>%\r\n  # select relevant variables\r\n  select(\r\n    pair_number,\r\n    is_treated,\r\n    contains(\"total_gross_tonnage_cruise\"),\r\n    contains(\"total_gross_tonnage_ferry\"),\r\n    contains(\"total_gross_tonnage_other_boat\")\r\n  ) %>%\r\n  # transform data in long format\r\n  pivot_longer(\r\n    cols = -c(pair_number, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"tonnage\"\r\n  ) %>%\r\n  # create vessel type variable\r\n  mutate(\r\n    vessel_type = NA %>%\r\n      ifelse(str_detect(variable, \"cruise\"), \"Cruise\", .) %>%\r\n      ifelse(str_detect(variable, \"ferry\"), \"Ferry\", .) %>%\r\n      ifelse(str_detect(variable, \"other_boat\"), \"Other Types of Vessels\", .)\r\n  ) %>%\r\n  mutate(\r\n    time = 0 %>%\r\n      ifelse(str_detect(variable, \"lag_1\"),-1, .) %>%\r\n      ifelse(str_detect(variable, \"lead_1\"), 1, .)\r\n  ) %>%\r\n  select(pair_number, vessel_type, is_treated, time, tonnage) %>%\r\n  pivot_wider(names_from = is_treated, values_from = tonnage)\r\n\r\n# compute the average difference in traffic between treated and control units\r\ndata_mean_difference <- data_vessel_type_tonnage %>%\r\n  mutate(difference = treated - control) %>%\r\n  select(-c(treated, control)) %>%\r\n  group_by(vessel_type, time) %>%\r\n  summarise(mean_difference = mean(difference, na.rm = TRUE)) %>%\r\n  ungroup()\r\n\r\n# plot the evolution\r\ngraph_tonnage_difference_vessel_type <-\r\n  ggplot(data_mean_difference,\r\n         aes(x = as.factor(time), y = mean_difference, group = \"l\"))  +\r\n  geom_hline(yintercept = 0) +\r\n  geom_segment(aes(\r\n    x = as.factor(time),\r\n    xend = as.factor(time),\r\n    y = 0,\r\n    yend = mean_difference\r\n  )) +\r\n  geom_point(\r\n    shape = 21,\r\n    size = 4,\r\n    colour = \"black\",\r\n    fill = my_blue\r\n  ) +\r\n  scale_y_continuous(\r\n    breaks = scales::pretty_breaks(n = 5),\r\n    labels = function(x)\r\n      format(x, big.mark = \" \", scientific = FALSE)\r\n  ) +\r\n  facet_wrap( ~ vessel_type) +\r\n  xlab(\"Day\") + ylab(\"Average Difference in\\n Total Gross Tonnage\") +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_tonnage_difference_vessel_type\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_tonnage_difference_vessel_type,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_tonnage_difference_vessel_type.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-19T17:38:56+02:00"
    },
    {
      "path": "2_6_daily_checking_balance.html",
      "title": "Checking Covariates Balance",
      "description": "Comparing days with cruise traffic to days without. Adjusting for calendar and weather indicators.\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nPreparing the Data\r\nFigures for Covariates Distribution for Treated and Control Units\r\nWeather Covariates\r\nPollutants\r\nRoad Traffic\r\nCalendar Indicator\r\n\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we take great care providing all steps and R codes required to check whether our most matching procedure achieved balance. We compare days where:\r\ntreated units are days with cruise traffic in t.\r\ncontrol units are day without cruise traffic in t.\r\nWe adjust for calendar calendar indicator and weather confounding factors.\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce exactly the 2_6_daily_checking_balance.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the 2_6_daily_checking_balance.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we have to load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(ggridges) # for ridge density plots\r\nlibrary(Cairo) # for printing costumed police of graphs\r\nlibrary(patchwork) # combining plots\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\",\r\n                  \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nPreparing the Data\r\nWe load the matched data:\r\n\r\n\r\n# load matched data\r\ndata_matched <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"1.cruise_experiment\",\r\n      \"matched_data.rds\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nFigures for Covariates Distribution for Treated and Control Units\r\nWe check whether coviariates balance was achieved with the thresholds we defined for our matching procedure. We plot distributions of weather and calendar variables (Lags 0-1) and pollutants (Lag 1) for treated and control groups.\r\nWeather Covariates\r\nFor continuous weather covariates, we draw boxplots for treated and control groups:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we select control variables and store them in a long dataframe\r\ndata_weather_continuous_variables <- data_matched %>%\r\n  select(\r\n    temperature_average,\r\n    temperature_average_lag_1,\r\n    humidity_average,\r\n    humidity_average_lag_1,\r\n    wind_speed,\r\n    wind_speed_lag_1,\r\n    is_treated\r\n  ) %>%\r\n  pivot_longer(\r\n    cols = -c(is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  mutate(\r\n    new_variable = NA %>%\r\n      ifelse(\r\n        str_detect(variable, \"temperature_average\"),\r\n        \"Average Temperature (°C)\",\r\n        .\r\n      ) %>%\r\n      ifelse(\r\n        str_detect(variable, \"humidity_average\"),\r\n        \"Humidity Average (%)\",\r\n        .\r\n      ) %>%\r\n      ifelse(str_detect(variable, \"wind_speed\"), \"Wind Speed (m/s)\", .)\r\n  ) %>%\r\n  mutate(time = \"in t\" %>%\r\n           ifelse(str_detect(variable, \"lag_1\"), \"in t-1\", .)) %>%\r\n  mutate(variable = paste(new_variable, time, sep = \" \")) %>%\r\n  mutate(is_treated = if_else(is_treated == TRUE, \"Treated\", \"Control\"))\r\n\r\ngraph_boxplot_continuous_weather <-\r\n  ggplot(data_weather_continuous_variables,\r\n         aes(x = is_treated, y = values, colour = is_treated)) +\r\n  geom_violin(size = 1.2) +\r\n  geom_boxplot(width = 0.1, outlier.shape = NA) +\r\n  scale_color_manual(values = c(my_blue, my_orange)) +\r\n  ylab(\"Covariate Value\") +\r\n  xlab(\"\") +\r\n  labs(colour = \"Units Status:\") +\r\n  facet_wrap(~ variable, scale = \"free\", ncol = 2) +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_boxplot_continuous_weather\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_continuous_weather,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_boxplot_continuous_weather.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 30,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nFor the rainfall dummy and the wind direction categories, we plot the proportions:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we select the rainfall variables\r\ndata_weather_categorical <- data_matched %>%\r\n  select(\r\n    rainfall_height_dummy,\r\n    rainfall_height_dummy_lag_1,\r\n    wind_direction_categories,\r\n    wind_direction_categories_lag_1,\r\n    is_treated\r\n  ) %>%\r\n  mutate_at(\r\n    vars(rainfall_height_dummy:rainfall_height_dummy_lag_1),\r\n    ~ ifelse(. == 1, \"True\", \"False\")\r\n  ) %>%\r\n  mutate_all( ~ as.character(.)) %>%\r\n  pivot_longer(\r\n    cols = -c(is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  # group by is_treated, variable and values\r\n  group_by(is_treated, variable, values) %>%\r\n  # compute the number of observations\r\n  summarise(n = n()) %>%\r\n  # compute the proportion\r\n  mutate(freq = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup() %>%\r\n  filter(!(\r\n    variable %in% c(\"rainfall_height_dummy\", \"rainfall_height_dummy_lag_1\") &\r\n      values == \"False\"\r\n  )) %>%\r\n  mutate(\r\n    new_variable = NA %>%\r\n      ifelse(str_detect(variable, \"wind\"), \"Wind Direction\", .) %>%\r\n      ifelse(str_detect(variable, \"rainfall\"), \"Rainfall Dummy\", .)\r\n  ) %>%\r\n  mutate(time = \"\\nin t\" %>%\r\n           ifelse(str_detect(variable, \"lag_1\"), \"\\nin t-1\", .)) %>%\r\n  mutate(variable = paste(new_variable, time, sep = \" \")) %>%\r\n  mutate(is_treated = if_else(is_treated == TRUE, \"Treated\", \"Control\"))\r\n\r\n\r\n# build the graph for wind direction\r\ngraph_categorical_wd_weather <- data_weather_categorical %>%\r\n  filter(new_variable == \"Wind Direction\") %>%\r\n  ggplot(., aes(x = freq, y = values, fill = is_treated)) +\r\n  geom_point(shape = 21,\r\n             size = 6,\r\n             alpha = 0.8) +\r\n  scale_fill_manual(values = c(my_blue, my_orange)) +\r\n  facet_wrap( ~ variable, scales = \"free\") +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"\") +\r\n  labs(fill = \"Units Status:\") +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_categorical_wd_weather\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# build the graph for rainfall dummy\r\ngraph_categorical_rainfall_weather <- data_weather_categorical %>%\r\n  filter(new_variable == \"Rainfall Dummy\") %>%\r\n  ggplot(., aes(x = freq, y = variable, fill = is_treated)) +\r\n  geom_point(shape = 21,\r\n             size = 6,\r\n             alpha = 0.8) +\r\n  scale_fill_manual(values = c(my_blue, my_orange)) +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"\") +\r\n  labs(fill = \"Units Status:\") +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_categorical_rainfall_weather\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# combine plots\r\ngraph_categorical_weather <-\r\n  graph_categorical_wd_weather / graph_categorical_rainfall_weather +\r\n  plot_annotation(tag_levels = 'A') &\r\n  theme(plot.tag = element_text(size = 30, face = \"bold\"))\r\n\r\n# save the graph\r\nggsave(\r\n  graph_categorical_weather,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_categorical_weather.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 20,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPollutants\r\nFor pollutants lag 1, we draw boxplots for treated and control groups:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we select control variables and store them in a long dataframe\r\ndata_pollutant_variables <- data_matched %>%\r\n  select(mean_no2_l_lag_1:mean_o3_l_lag_1,\r\n         is_treated) %>%\r\n  # transform the data to long to compute the proportion of observations for each variable\r\n  pivot_longer(\r\n    cols = -c(is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"Treated\", \"Control\")) %>%\r\n  mutate(\r\n    pollutant = NA %>%\r\n      ifelse(str_detect(variable, \"no2_l\"), \"NO2 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"no2_sl\"), \"NO2 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"o3\"), \"O3 in Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_l\"), \"PM10 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_sl\"), \"PM10 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"pm25\"), \"PM2.5 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"so2\"), \"SO2 Longchamp\", .)\r\n  ) %>%\r\n  mutate(time = \"\\nin t-1\") %>%\r\n  mutate(variable = paste(pollutant, time, sep = \" \"))\r\n\r\n# make graph\r\ngraph_boxplot_pollutants <- data_pollutant_variables %>%\r\n  ggplot(., aes(x = is_treated, y = values, colour = is_treated)) +\r\n  geom_violin(size = 1.2) +\r\n  geom_boxplot(width = 0.1, outlier.shape = NA) +\r\n  scale_color_manual(values = c(my_blue, my_orange)) +\r\n  ylab(\"Concentration (µg/m³)\") +\r\n  xlab(\"\") +\r\n  labs(colour = \"Units Status:\") +\r\n  facet_wrap( ~ variable, scale = \"free\", ncol = 4) +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_boxplot_pollutants\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_pollutants,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_boxplot_pollutants.pdf\"\r\n  ),\r\n  width = 40,\r\n  height = 25,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nRoad Traffic\r\nFor road traffic flow and occupancy rate, we draw boxplots for treated and control groups over the 0-1 lags:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we select control variables and store them in a long dataframe\r\ndata_road_traffic_variables <- data_matched %>%\r\n  select(\r\n    road_traffic_flow_all,\r\n    road_traffic_flow_all_lag_1,\r\n    road_occupancy_rate,\r\n    road_occupancy_rate_lag_1,\r\n    is_treated\r\n  ) %>%\r\n  pivot_longer(\r\n    cols = -c(is_treated),\r\n    names_to = \"traffic_variable\",\r\n    values_to = \"value\"\r\n  ) %>%\r\n  mutate(\r\n    traffic_variable = case_when(\r\n      traffic_variable ==  \"road_traffic_flow_all\" ~ \"Road Traffic Flow (N. Vehicles) in t\",\r\n      traffic_variable ==  \"road_traffic_flow_all_lag_1\" ~ \"Road Traffic Flow (N. Vehicles) in t-1\",\r\n      traffic_variable == \"road_occupancy_rate\" ~ \"Road Occupancy Rate (%) in t\",\r\n      traffic_variable == \"road_occupancy_rate_lag_1\" ~ \"Road Occupancy Rate (%) in t-1\"\r\n    )\r\n  ) %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"Treated\", \"Control\"))\r\n\r\n# make graph\r\ngraph_boxplot_road_traffic <- data_road_traffic_variables %>%\r\n  ggplot(., aes(x = is_treated, y = value, colour = is_treated)) +\r\n  geom_violin(size = 1.2) +\r\n  geom_boxplot(width = 0.1, outlier.shape = NA) +\r\n  scale_color_manual(values = c(my_blue, my_orange)) +\r\n  ylab(\"Road Traffic Flow\") +\r\n  xlab(\"\") +\r\n  labs(colour = \"Units Status:\") +\r\n  facet_wrap(~ traffic_variable, scale = \"free\", ncol = 2) +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_boxplot_road_traffic\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_road_traffic,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_boxplot_road_traffic.pdf\"\r\n  ),\r\n  width = 25,\r\n  height = 20,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nCalendar Indicator\r\nFor calendar variables such as the day of the week, bank days and holidays we matched strictly. We plot the proportions of observations belonging to each month by treatment status:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute the proportions of observations belonging to each month by treatment status\r\ndata_month <- data_matched %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"Treated\", \"Control\")) %>%\r\n  select(month, is_treated) %>%\r\n  mutate(\r\n    month = recode(\r\n      month,\r\n      `1` = \"January\",\r\n      `2` = \"February\",\r\n      `3` = \"March\",\r\n      `4` = \"April\",\r\n      `5` = \"May\",\r\n      `6` = \"June\",\r\n      `7` = \"July\",\r\n      `8` = \"August\",\r\n      `9` = \"September\",\r\n      `10` = \"October\",\r\n      `11` = \"November\",\r\n      `12` = \"December\"\r\n    ) %>%\r\n      fct_relevel(\r\n        .,\r\n        \"January\",\r\n        \"February\",\r\n        \"March\",\r\n        \"April\",\r\n        \"May\",\r\n        \"June\",\r\n        \"July\",\r\n        \"August\",\r\n        \"September\",\r\n        \"October\",\r\n        \"November\",\r\n        \"December\"\r\n      )\r\n  ) %>%\r\n  pivot_longer(., -is_treated) %>%\r\n  group_by(name, is_treated, value) %>%\r\n  summarise(n = n()) %>%\r\n  mutate(proportion = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup()\r\n\r\n# we plot the data using cleveland dot plots\r\ngraph_month <-\r\n  ggplot(data_month,\r\n         aes(\r\n           x = as.factor(value),\r\n           y = proportion,\r\n           colour = is_treated,\r\n           group = is_treated\r\n         )) +\r\n  geom_line(size = 1.2) +\r\n  scale_colour_manual(values = c(my_blue, my_orange),\r\n                      guide = guide_legend(reverse = FALSE)) +\r\n  expand_limits(x = 0, y = 0) +\r\n  ggtitle(\"Month\") +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"\") +\r\n  labs(colour = \"Units Status:\") +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_month\r\n\r\n\r\n\r\n\r\nWe plot the proportions of observations belonging to each year by treatment status:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute the proportions of observations belonging to each year by treatment status\r\ndata_year <- data_matched %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"Treated\", \"Control\")) %>%\r\n  select(year, is_treated) %>%\r\n  pivot_longer(.,-is_treated) %>%\r\n  group_by(name, is_treated, value) %>%\r\n  summarise(n = n()) %>%\r\n  mutate(proportion = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup()\r\n\r\n# we plot the data using cleveland dot plots\r\ngraph_year <-\r\n  ggplot(data_year,\r\n         aes(\r\n           x = as.factor(value),\r\n           y = proportion,\r\n           colour = is_treated,\r\n           group = is_treated\r\n         )) +\r\n  geom_line(size = 1.2) +\r\n  scale_colour_manual(values = c(my_blue, my_orange),\r\n                      guide = guide_legend(reverse = FALSE)) +\r\n  expand_limits(x = 0, y = 0) +\r\n  ggtitle(\"Year\") +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"\") +\r\n  labs(colour = \"Units Status:\") +\r\n  theme_tufte()\r\n\r\n# we print the graph\r\ngraph_year\r\n\r\n\r\n\r\n\r\nWe combine and save the two previous plots:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# combine plots\r\ngraph_month_year <- graph_month / graph_year +\r\n  plot_annotation(tag_levels = 'A') &\r\n  theme(plot.tag = element_text(size = 30, face = \"bold\"))\r\n\r\n# save the plot\r\nggsave(\r\n  graph_month_year,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_month_year.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 25,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-20T00:08:33+02:00"
    },
    {
      "path": "2_7_daily_checking_balance_improvement.html",
      "title": "Checking Balance Improvement",
      "description": "Comparing days with cruise traffic to days without. Adjusting for calendar and weather indicators.\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nPreparing the Data\r\nLove Plots\r\nContinuous Weather Covariates\r\nCategorical Weather Covariates\r\nPollutants\r\nVessels Traffic\r\nRoad Traffic\r\nCalendar Indicators\r\n\r\nOverall Balance Improvement\r\nContinuous Covariates\r\nCategorical Covariates\r\n\r\nRandomization Check for Covariate Balance\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we take great care providing all steps and R codes required to check whether our matching procedure allowed to improve covariates balance. We compare days where:\r\ntreated units are days with positive cruise traffic in t.\r\ncontrol units are days without cruise traffic in t.\r\nWe adjust for calendar calendar indicator and weather confounding factors.\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce exactly the 2_7_daily_checking_balance_improvement.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the 2_7_daily_checking_balance_improvement.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we have to load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(dtplyr) # to speed up to dplyr\r\nlibrary(randChecks) # for randomization check\r\nlibrary(ggridges) # for ridge density plots\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(patchwork) # combining plots\r\nlibrary(kableExtra) # for table formatting\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\",\r\n                  \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nPreparing the Data\r\nWe load the initial and matched data and bind them together:\r\n\r\n\r\n# load matching data\r\ndata_matching <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"1.cruise_experiment\",\r\n      \"matching_data.rds\"\r\n    )\r\n  ) %>%\r\n  mutate(dataset = \"Initial Data\")\r\n\r\n# load matched data\r\ndata_matched <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"1.cruise_experiment\",\r\n      \"matched_data.rds\"\r\n    )\r\n  ) %>%\r\n  mutate(dataset = \"Matched Data\")\r\n\r\n# bind the two datasets\r\ndata <- bind_rows(data_matching, data_matched)\r\n\r\n\r\n\r\nWe change labels of the is_treated variable :\r\n\r\n\r\ndata <- data %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"True\", \"False\"))\r\n\r\n\r\n\r\nLove Plots\r\nContinuous Weather Covariates\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute figures for the love plot\r\ndata_weather_continuous <- data %>%\r\n  dplyr::select(\r\n    dataset,\r\n    is_treated,\r\n    contains(\"temperature\"),\r\n    contains(\"humidity\"),\r\n    contains(\"wind_speed\")\r\n  ) %>%\r\n  pivot_longer(\r\n    cols = -c(is_treated, dataset),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  mutate(\r\n    weather_variable = NA %>%\r\n      ifelse(\r\n        str_detect(variable, \"temperature_average\"),\r\n        \"Average Temperature\",\r\n        .\r\n      ) %>%\r\n      ifelse(\r\n        str_detect(variable, \"humidity_average\"),\r\n        \"Humidity Average\",\r\n        .\r\n      ) %>%\r\n      ifelse(str_detect(variable, \"wind_speed\"), \"Wind Speed\", .)\r\n  ) %>%\r\n  mutate(time = \"0\" %>%\r\n           ifelse(str_detect(variable, \"lag_1\"), \"-1\", .) %>%\r\n           ifelse(str_detect(variable, \"lead_1\"), \"+1\", .)) %>%\r\n  mutate(time = fct_relevel(time, \"-1\", \"0\", \"+1\")) %>%\r\n  dplyr::select(dataset, is_treated, weather_variable, time, values)\r\n\r\ndata_abs_difference_continuous_weather <-\r\n  data_weather_continuous %>%\r\n  group_by(dataset, weather_variable, time, is_treated) %>%\r\n  summarise(mean_values = mean(values, na.rm = TRUE)) %>%\r\n  summarise(abs_difference = abs(mean_values[2] - mean_values[1]))\r\n\r\ndata_sd_weather_continuous <-  data_weather_continuous %>%\r\n  filter(dataset == \"Initial Data\" & is_treated == \"True\") %>%\r\n  group_by(weather_variable, time, is_treated) %>%\r\n  summarise(sd_treatment = sd(values, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  select(weather_variable, time, sd_treatment)\r\n\r\ndata_love_continuous_weather <-\r\n  left_join(\r\n    data_abs_difference_continuous_weather,\r\n    data_sd_weather_continuous,\r\n    by = c(\"weather_variable\", \"time\")\r\n  ) %>%\r\n  mutate(standardized_difference = abs_difference / sd_treatment) %>%\r\n  select(-c(abs_difference, sd_treatment))\r\n\r\n# make the graph\r\ngraph_love_plot_continuous_weather <-\r\n  ggplot(\r\n    data_love_continuous_weather,\r\n    aes(\r\n      y = fct_rev(time),\r\n      x = standardized_difference,\r\n      colour = fct_rev(dataset),\r\n      shape = fct_rev(dataset)\r\n    )\r\n  ) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_vline(xintercept = 0.1,\r\n             color = \"black\",\r\n             linetype = \"dashed\") +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  facet_wrap( ~ weather_variable, scales = \"free_y\") +\r\n  xlab(\"Standardized Mean Differences\") +\r\n  ylab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# plot the graph\r\ngraph_love_plot_continuous_weather\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_continuous_weather,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_continuous_weather.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nCategorical Weather Covariates\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute figures for the love plot\r\ndata_weather_categorical <- data %>%\r\n  select(\r\n    dataset,\r\n    is_treated,\r\n    contains(\"rainfall_height_dummy\"),\r\n    contains(\"wind_direction_east_west\")\r\n  ) %>%\r\n  mutate_at(vars(contains(\"rainfall\")), ~ ifelse(. == 1, \"True\", \"False\")) %>%\r\n  mutate_all( ~ as.character(.)) %>%\r\n  pivot_longer(\r\n    cols = -c(dataset, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  # group by is_treated, variable and values\r\n  group_by(dataset, is_treated, variable, values) %>%\r\n  # compute the number of observations\r\n  summarise(n = n()) %>%\r\n  # compute the proportion\r\n  mutate(freq = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    weather_variable = NA %>%\r\n      ifelse(str_detect(variable, \"wind\"), \"Wind Direction\", .) %>%\r\n      ifelse(str_detect(variable, \"rainfall\"), \"Rainfall Dummy\", .)\r\n  ) %>%\r\n  mutate(time = \"t\" %>%\r\n           ifelse(str_detect(variable, \"lag_1\"), \"t-1\", .) %>%\r\n           ifelse(str_detect(variable, \"lead_1\"), \"t+1\", .)) %>%\r\n  filter(!is.na(time)) %>%\r\n  mutate(variable = paste(weather_variable, time, sep = \" \")) %>%\r\n  select(dataset, is_treated, weather_variable, variable, values, freq) %>%\r\n  pivot_wider(names_from = is_treated, values_from = freq) %>%\r\n  mutate(abs_difference = abs(`True` - `False`)) %>%\r\n  filter(values != \"False\")\r\n\r\n# create the figure for wind direction\r\ngraph_love_plot_wind_direction <- data_weather_categorical %>%\r\n  filter(weather_variable == \"Wind Direction\") %>%\r\n  mutate(variable = fct_relevel(\r\n    variable,\r\n    \"Wind Direction t-1\",\r\n    \"Wind Direction t\",\r\n    \"Wind Direction t+1\"\r\n  )) %>%\r\n  ggplot(.,\r\n         aes(\r\n           y = fct_rev(values),\r\n           x = abs_difference,\r\n           colour = fct_rev(dataset),\r\n           shape = fct_rev(dataset)\r\n         )) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  facet_wrap( ~ variable, scales = \"free_y\", ncol = 3) +\r\n  xlab(\"Absolute Difference in Percentage Points\") +\r\n  ylab(\"\") +\r\n  theme_tufte()\r\n\r\n# print the figure for wind direction\r\ngraph_love_plot_wind_direction\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the figure for wind direction\r\nggsave(\r\n  graph_love_plot_wind_direction,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_wind_direction.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n# create the figure for rainfall dummy\r\ngraph_love_plot_rainfall <- data_weather_categorical %>%\r\n  filter(weather_variable == \"Rainfall Dummy\") %>%\r\n  mutate(variable = fct_relevel(\r\n    variable,\r\n    \"Rainfall Dummy t-1\",\r\n    \"Rainfall Dummy t\",\r\n    \"Rainfall Dummy t+1\"\r\n  )) %>%\r\n  ggplot(.,\r\n         aes(\r\n           y = fct_rev(variable),\r\n           x = abs_difference,\r\n           colour = fct_rev(dataset),\r\n           shape = fct_rev(dataset)\r\n         )) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  xlab(\"Absolute Difference in Percentage Points\") +\r\n  ylab(\"\") +\r\n  theme_tufte()\r\n\r\n# print the figure for rainfall dummy\r\ngraph_love_plot_rainfall\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the figure for rainfall dummy\r\nggsave(\r\n  graph_love_plot_rainfall,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_rainfall.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPollutants\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute figures for the love plot\r\ndata_pollutants <- data %>%\r\n  select(\r\n    dataset,\r\n    is_treated,\r\n    contains(\"no2\"),\r\n    contains(\"o3\"),\r\n    contains(\"pm10\"),\r\n    contains(\"pm25\"),\r\n    contains(\"so2\")\r\n  ) %>%\r\n  pivot_longer(\r\n    cols = -c(dataset, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  mutate(\r\n    pollutant = NA %>%\r\n      ifelse(str_detect(variable, \"no2_l\"), \"NO2 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"no2_sl\"), \"NO2 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"o3\"), \"O3 Lonchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_l\"), \"PM10 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_sl\"), \"PM10 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"pm25\"), \"PM2.5 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"so2\"), \"SO2 Longchamp\", .)\r\n  ) %>%\r\n  mutate(time = NA %>%\r\n           ifelse(str_detect(variable, \"lag_1\"), \"-1\", .)) %>%\r\n  filter(!is.na(time)) %>%\r\n  select(dataset, is_treated, pollutant, time, values)\r\n\r\ndata_abs_difference_pollutants <- data_pollutants %>%\r\n  group_by(dataset, pollutant, time, is_treated) %>%\r\n  summarise(mean_values = mean(values, na.rm = TRUE)) %>%\r\n  summarise(abs_difference = abs(mean_values[2] - mean_values[1]))\r\n\r\ndata_sd_pollutants <-  data_pollutants %>%\r\n  filter(dataset == \"Initial Data\" & is_treated == \"True\") %>%\r\n  group_by(pollutant, time, is_treated) %>%\r\n  summarise(sd_treatment = sd(values, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  select(pollutant, time, sd_treatment)\r\n\r\ndata_love_pollutants <-\r\n  left_join(\r\n    data_abs_difference_pollutants,\r\n    data_sd_pollutants,\r\n    by = c(\"pollutant\", \"time\")\r\n  ) %>%\r\n  mutate(standardized_difference = abs_difference / sd_treatment) %>%\r\n  select(-c(abs_difference, sd_treatment))\r\n\r\n# create the graph\r\ngraph_love_plot_pollutants <-\r\n  ggplot(\r\n    data_love_pollutants,\r\n    aes(\r\n      y = fct_rev(time),\r\n      x = standardized_difference,\r\n      colour = fct_rev(dataset),\r\n      shape = fct_rev(dataset)\r\n    )\r\n  ) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_vline(xintercept = 0.1,\r\n             color = \"black\",\r\n             linetype = \"dashed\") +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  facet_wrap( ~ pollutant, ncol = 4) +\r\n  xlab(\"Standardized Mean Differences\") +\r\n  ylab(\"Day\") +\r\n  theme_tufte()\r\n# print the graph\r\ngraph_love_plot_pollutants\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_pollutants,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_pollutants.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nVessels Traffic\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute figures for the love plot\r\ndata_tonnage <-  data %>%\r\n  # select relevant variables\r\n  select(\r\n    dataset,\r\n    is_treated,\r\n    contains(\"total_gross_tonnage_cruise\"),\r\n    contains(\"total_gross_tonnage_ferry\"),\r\n    contains(\"total_gross_tonnage_other_boat\")\r\n  ) %>%\r\n  # transform data in long format\r\n  pivot_longer(\r\n    cols = -c(dataset, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"tonnage\"\r\n  ) %>%\r\n  # create vessel type variable\r\n  mutate(\r\n    vessel_type = NA %>%\r\n      ifelse(str_detect(variable, \"cruise\"), \"Cruise\", .) %>%\r\n      ifelse(str_detect(variable, \"ferry\"), \"Ferry\", .) %>%\r\n      ifelse(str_detect(variable, \"other_boat\"), \"Other Types of Vessels\", .)\r\n  ) %>%\r\n  mutate(time = NA %>%\r\n           ifelse(str_detect(variable, \"lag_1\"), \"-1\", .) %>%\r\n           ifelse(str_detect(variable, \"lead_1\"), \"1\", .)) %>%\r\n  filter(!is.na(time)) %>%\r\n  select(dataset, vessel_type, is_treated, time, tonnage)\r\n\r\ndata_abs_difference_tonnage <- data_tonnage %>%\r\n  group_by(dataset, vessel_type, time, is_treated) %>%\r\n  summarise(mean_tonnage = mean(tonnage, na.rm = TRUE)) %>%\r\n  summarise(abs_difference = abs(mean_tonnage[2] - mean_tonnage[1]))\r\n\r\ndata_sd_tonnage <-  data_tonnage %>%\r\n  filter(dataset == \"Initial Data\" & is_treated == \"True\") %>%\r\n  group_by(vessel_type, time, is_treated) %>%\r\n  summarise(sd_treatment = sd(tonnage, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  select(vessel_type, time, sd_treatment)\r\n\r\ndata_love_tonnage <-\r\n  left_join(data_abs_difference_tonnage,\r\n            data_sd_tonnage,\r\n            by = c(\"vessel_type\", \"time\")) %>%\r\n  mutate(standardized_difference = abs_difference / sd_treatment) %>%\r\n  select(-c(abs_difference, sd_treatment)) %>%\r\n  filter(!(vessel_type == \"Cruise\" & time == 0))\r\n\r\n# create the graph\r\ngraph_love_plot_tonnage <-\r\n  ggplot(\r\n    data_love_tonnage,\r\n    aes(\r\n      x = standardized_difference,\r\n      y = as.factor(time),\r\n      colour = fct_rev(dataset),\r\n      shape = fct_rev(dataset)\r\n    )\r\n  ) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_vline(xintercept = 0.1,\r\n             color = \"black\",\r\n             linetype = \"dashed\") +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +  facet_wrap( ~ vessel_type) +\r\n  xlab(\"Standardized Mean Differences\") +\r\n  ylab(\"Day\") +\r\n  theme_tufte()\r\n# print the graph\r\ngraph_love_plot_tonnage\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_tonnage,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_tonnage.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nRoad Traffic\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute figures for the love plot\r\ndata_road <-  data %>%\r\n  # select relevant variables\r\n  select(dataset, is_treated, contains(\"road_traffic_flow_all\"), contains(\"road_occupancy_rate\")) %>%\r\n  # transform data in long format\r\n  pivot_longer(\r\n    cols = -c(dataset, is_treated),\r\n    names_to = \"road_traffic\",\r\n    values_to = \"value\"\r\n  ) %>%\r\n  mutate(time = 0 %>%\r\n           ifelse(str_detect(road_traffic, \"lag_1\"),-1, .) %>%\r\n           ifelse(str_detect(road_traffic, \"lead_1\"), 1, .)) %>%\r\n  mutate(road_traffic = ifelse(str_detect(road_traffic, \"road_traffic_flow_all\"), \"Flow of Vehicles\", \"Occupany Rate\")) %>%\r\n  select(dataset, road_traffic, is_treated, time, value)\r\n\r\ndata_abs_difference_road <- data_road %>%\r\n  group_by(dataset, road_traffic, time, is_treated) %>%\r\n  summarise(mean_value = mean(value, na.rm = TRUE)) %>%\r\n  summarise(abs_difference = abs(mean_value[2] - mean_value[1]))\r\n\r\ndata_sd_tonnage <-  data_road %>%\r\n  filter(dataset == \"Initial Data\" & is_treated == \"True\") %>%\r\n  group_by(road_traffic, time, is_treated) %>%\r\n  summarise(sd_treatment = sd(value, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  select(road_traffic, time, sd_treatment)\r\n\r\ndata_love_road <-\r\n  left_join(data_abs_difference_road,\r\n            data_sd_tonnage,\r\n            by = c(\"road_traffic\", \"time\")) %>%\r\n  mutate(standardized_difference = abs_difference / sd_treatment) %>%\r\n  select(-c(abs_difference, sd_treatment))\r\n\r\n\r\n# create the graph\r\ngraph_love_plot_road <-\r\n  ggplot(\r\n    data_love_road,\r\n    aes(\r\n      x = standardized_difference,\r\n      y = as.factor(time),\r\n      colour = fct_rev(dataset),\r\n      shape = fct_rev(dataset)\r\n    )\r\n  ) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_vline(xintercept = 0.1,\r\n             color = \"black\",\r\n             linetype = \"dashed\") +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  facet_wrap(~ road_traffic) + \r\n  xlab(\"Standardized Mean Differences\") +\r\n  ylab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# print the graph\r\ngraph_love_plot_road\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_road,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_road.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nCalendar Indicators\r\nCreate the relevant data:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute figures for the love plot\r\ndata_calendar <- data %>%\r\n  mutate(weekday = lubridate::wday(date, abbr = FALSE, label = TRUE)) %>%\r\n  select(dataset,\r\n         is_treated,\r\n         weekday,\r\n         holidays_dummy,\r\n         bank_day_dummy,\r\n         month,\r\n         year) %>%\r\n  mutate_at(vars(holidays_dummy, bank_day_dummy),\r\n            ~ ifelse(. == 1, \"True\", \"False\")) %>%\r\n  mutate_all( ~ as.character(.)) %>%\r\n  pivot_longer(\r\n    cols = -c(dataset, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  # group by is_treated, variable and values\r\n  group_by(dataset, is_treated, variable, values) %>%\r\n  # compute the number of observations\r\n  summarise(n = n()) %>%\r\n  # compute the proportion\r\n  mutate(freq = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    calendar_variable = NA %>%\r\n      ifelse(str_detect(variable, \"weekday\"), \"Day of the Week\", .) %>%\r\n      ifelse(str_detect(variable, \"holidays_dummy\"), \"Holidays\", .) %>%\r\n      ifelse(str_detect(variable, \"bank_day_dummy\"), \"Bank Day\", .) %>%\r\n      ifelse(str_detect(variable, \"month\"), \"Month\", .) %>%\r\n      ifelse(str_detect(variable, \"year\"), \"Year\", .)\r\n  ) %>%\r\n  select(dataset, is_treated, calendar_variable, values, freq) %>%\r\n  pivot_wider(names_from = is_treated, values_from = freq) %>%\r\n  mutate(abs_difference = abs(`True` - `False`)) %>%\r\n  filter(values != \"False\")\r\n\r\n\r\n\r\nPlot for bank days and holidays:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# graph for bank days and holidays\r\ngraph_love_plot_bank_holidays <- data_calendar %>%\r\n  filter(calendar_variable %in% c(\"Bank Day\", \"Holidays\")) %>%\r\n  ggplot(.,\r\n         aes(\r\n           y = values,\r\n           x = abs_difference,\r\n           colour = fct_rev(dataset),\r\n           shape = fct_rev(dataset)\r\n         )) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  facet_wrap(~ calendar_variable) +\r\n  xlab(\"Absolute Difference in Percentage Points\") +\r\n  ylab(\"\") +\r\n  theme_tufte()\r\n# print the plot\r\ngraph_love_plot_bank_holidays\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the plot\r\nggsave(\r\n  graph_love_plot_bank_holidays,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_bank_holidays.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPlot for days of the week:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# graph for weekdays\r\ngraph_love_plot_weekday <- data_calendar %>%\r\n  filter(calendar_variable == \"Day of the Week\") %>%\r\n  mutate(\r\n    values = fct_relevel(\r\n      values,\r\n      \"Monday\",\r\n      \"Tuesday\",\r\n      \"Wednesday\",\r\n      \"Thursday\",\r\n      \"Friday\",\r\n      \"Saturday\",\r\n      \"Sunday\"\r\n    )\r\n  ) %>%\r\n  ggplot(., aes(\r\n    y = fct_rev(values),\r\n    x = abs_difference,\r\n    colour = fct_rev(dataset),\r\n    shape = fct_rev(dataset)\r\n  )) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  xlab(\"Absolute Difference in Percentage Points\") +\r\n  ylab(\"\") +\r\n  theme_tufte() +\r\n  theme(\r\n    legend.position = \"top\",\r\n    legend.justification = \"left\",\r\n    legend.direction = \"horizontal\"\r\n  )\r\n\r\n# print the plot\r\ngraph_love_plot_weekday\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the plot\r\nggsave(\r\n  graph_love_plot_weekday,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_weekday.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPlot for months:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# graph for month\r\ngraph_love_plot_month <- data_calendar %>%\r\n  filter(calendar_variable == \"Month\") %>%\r\n  mutate(\r\n    values = fct_relevel(\r\n      values,\r\n      \"January\",\r\n      \"February\",\r\n      \"March\",\r\n      \"April\",\r\n      \"May\",\r\n      \"June\",\r\n      \"July\",\r\n      \"August\",\r\n      \"September\",\r\n      \"October\",\r\n      \"November\",\r\n      \"December\"\r\n    )\r\n  ) %>%\r\n  ggplot(., aes(\r\n    y = fct_rev(values),\r\n    x = abs_difference,\r\n    colour = fct_rev(dataset),\r\n    shape = fct_rev(dataset)\r\n  )) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  ggtitle(\"Month\") +\r\n  xlab(\"Absolute Difference in Percentage Points\") +\r\n  ylab(\"\") +\r\n  theme_tufte() +\r\n  theme(\r\n    legend.position = \"top\",\r\n    legend.justification = \"left\",\r\n    legend.direction = \"horizontal\"\r\n  )\r\n\r\n# print the plot\r\ngraph_love_plot_month\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the plot\r\nggsave(\r\n  graph_love_plot_month,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_month.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPlot for years:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# graph for year\r\ngraph_love_plot_year <- data_calendar %>%\r\n  filter(calendar_variable == \"Year\") %>%\r\n  ggplot(., aes(\r\n    y = as.factor(as.numeric(values)),\r\n    x = abs_difference,\r\n    colour = fct_rev(dataset),\r\n    shape = fct_rev(dataset)\r\n  )) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_point(size = 2, alpha = 0.8) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  ggtitle(\"Year\") +\r\n  xlab(\"Absolute Difference in Percentage Points\") +\r\n  ylab(\"\") +\r\n  theme_tufte() +\r\n  theme(\r\n    legend.position = \"top\",\r\n    legend.justification = \"left\",\r\n    legend.direction = \"horizontal\"\r\n  )\r\n\r\n# print the graph\r\ngraph_love_plot_year\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the plot\r\nggsave(\r\n  graph_love_plot_year,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_love_plot_year.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOverall Balance Improvement\r\nWe plot the distribution of standardized mean differences for continuous covariates or the absolute percentage points differences for categorical covariates between treated and control units before and after matching.\r\nContinuous Covariates\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we select the dataset indicator and the standardized difference\r\ndata_love_tonnage <- data_love_tonnage %>%\r\n  ungroup() %>%\r\n  filter(time < 0) %>%\r\n  select(dataset, standardized_difference)\r\n\r\ndata_love_pollutants <- data_love_pollutants %>%\r\n  ungroup() %>%\r\n  filter(time < 0) %>%\r\n  select(dataset, standardized_difference)\r\n\r\ndata_love_continuous_weather <- data_love_continuous_weather %>%\r\n  ungroup() %>%\r\n  filter(time <= 0) %>%\r\n  select(dataset, standardized_difference)\r\n\r\ndata_love_road <- data_love_road %>%\r\n  ungroup() %>%\r\n  filter(time <= 0) %>%\r\n  select(dataset, standardized_difference)\r\n\r\ndata_continuous_love <-\r\n  bind_rows(data_love_tonnage, data_love_pollutants) %>%\r\n  bind_rows(., data_love_continuous_weather) %>%\r\n  bind_rows(., data_love_road)\r\n\r\n# create the graph\r\ngraph_boxplot_continuous_balance_improvement <-\r\n  ggplot(data_continuous_love,\r\n         aes(x = dataset, y = standardized_difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +\r\n  xlab(\"Dataset\") +\r\n  ylab(\"Standardized Mean Differences\") +\r\n  theme_tufte()\r\n# print the graph\r\ngraph_boxplot_continuous_balance_improvement\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_continuous_balance_improvement,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_boxplot_continuous_balance_improvement.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nCategorical Covariates\r\n\r\n\r\nPlease show me the code!\r\n\r\n# we select the dataset indicator and the standardized difference\r\ndata_calendar <- data_calendar %>%\r\n  ungroup() %>%\r\n  select(dataset, abs_difference)\r\n\r\ndata_weather_categorical <- data_weather_categorical %>%\r\n  ungroup() %>%\r\n  filter(\r\n    variable %in% c(\r\n      \"Rainfall Dummy t\",\r\n      \"Rainfall Dummy t-1\",\r\n      \"Wind Direction t\",\r\n      \"Wind Direction t-1\"\r\n    )\r\n  ) %>%\r\n  select(dataset, abs_difference)\r\n\r\ndata_categorical_love <-\r\n  bind_rows(data_calendar, data_weather_categorical)\r\n\r\n# create the graph\r\ngraph_boxplot_categorical_balance_improvement <-\r\n  ggplot(data_categorical_love, aes(x = dataset, y = abs_difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +\r\n  xlab(\"Dataset\") +\r\n  ylab(\"Absolute Difference \\nin Percentage Points\") +\r\n  theme_tufte()\r\n\r\n# print the graph\r\ngraph_boxplot_categorical_balance_improvement\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_categorical_balance_improvement,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_boxplot_categorical_balance_improvement.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe combine the two previous plots in a single figure:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# combine plots\r\ngraph_overall_balance <-\r\n  graph_boxplot_continuous_balance_improvement /graph_boxplot_categorical_balance_improvement +\r\n  plot_annotation(tag_levels = 'A') &\r\n  theme(plot.tag = element_text(size = 30, face = \"bold\"))\r\n\r\n# save the plot\r\nggsave(\r\n  graph_overall_balance,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"1.checking_matching_procedure\",\r\n    \"graph_overall_balance.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 20,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nAnd we compute the overall figures for imbalance before and after matching:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute average imbalance before and after matching\r\ndata_categorical_love <- data_categorical_love %>%\r\n  mutate(Type = \"Categorical (Difference in Percentage Points)\") %>%\r\n  rename(standardized_difference = abs_difference)\r\n  \r\ndata_continuous_love %>%\r\n  mutate(Type = \"Continuous (Standardized Difference)\") %>%\r\n  bind_rows(data_categorical_love) %>%\r\n  group_by(Type, dataset) %>%\r\n  summarise(\"Mean Imbalance\" = round(mean(standardized_difference), 2)) %>%\r\n  rename(Dataset = dataset) %>%\r\n  knitr::kable(., align = c(\"l\", \"l\", \"c\")) %>%\r\n  kable_styling(position = \"center\")\r\n\r\n\r\n\r\nType\r\n\r\n\r\nDataset\r\n\r\n\r\nMean Imbalance\r\n\r\n\r\nCategorical (Difference in Percentage Points)\r\n\r\n\r\nInitial Data\r\n\r\n\r\n4.58\r\n\r\n\r\nCategorical (Difference in Percentage Points)\r\n\r\n\r\nMatched Data\r\n\r\n\r\n1.55\r\n\r\n\r\nContinuous (Standardized Difference)\r\n\r\n\r\nInitial Data\r\n\r\n\r\n0.22\r\n\r\n\r\nContinuous (Standardized Difference)\r\n\r\n\r\nMatched Data\r\n\r\n\r\n0.05\r\n\r\n\r\nRandomization Check for Covariate Balance\r\nFinally, we implement the procedure of Zach Branson (2021) to carry out a randomization check to test whether daily observations are as-if randomized according to observed covariates.\r\n\r\n\r\n# we select covariates\r\ndata_covs <- data_matched %>%\r\n  select(\r\n    pair_number,\r\n    is_treated,\r\n    year,\r\n    month,\r\n    holidays_dummy,\r\n    bank_day_dummy,\r\n    total_gross_tonnage_ferry:total_gross_tonnage_other_boat,\r\n    temperature_average:wind_speed,\r\n    wind_direction_east_west,\r\n    holidays_dummy_lag_1,\r\n    bank_day_dummy_lag_1,\r\n    total_gross_tonnage_cruise_lag_1:total_gross_tonnage_other_boat_lag_1,\r\n    temperature_average_lag_1:wind_speed_lag_1,\r\n    wind_direction_east_west_lag_1,\r\n    mean_no2_l_lead_1:mean_o3_l_lead_1\r\n  )\r\n\r\n# recode some variables\r\ndata_covs <- data_covs %>%\r\n  mutate(is_treated = ifelse(is_treated == TRUE, 1, 0)) %>%\r\n  mutate_at(\r\n    vars(wind_direction_east_west , wind_direction_east_west_lag_1),\r\n    ~ ifelse(. == \"West\", 1, 0)\r\n  ) %>%\r\n  fastDummies::dummy_cols(., select_columns = c(\"year\", \"month\")) %>%\r\n  select(-c(\"year\", \"month\"))\r\n\r\n# format data for asIfRandPlot() function\r\npair_number <- as.numeric(data_covs$pair_number)\r\nis_treated <- data_covs$is_treated\r\n\r\ndata_covs <- data_covs %>%\r\n  select(-pair_number,-is_treated) %>%\r\n  select(-year_2018, - month_December, -bank_day_dummy, - bank_day_dummy_lag_1) \r\n\r\n# run balance check\r\nasIfRandPlot(\r\n  X.matched = data_covs,\r\n  indicator.matched = is_treated,\r\n  assignment = c(\"complete\", \"blocked\"),\r\n  subclass = pair_number,\r\n  perms = 100\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-21T13:11:45+02:00"
    },
    {
      "path": "2_8_daily_analysis_results.html",
      "title": "Analyzing Results",
      "description": "Comparing days with cruise traffic to days without. Adjusting for calendar and weather indicators.\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nPreparing the Data\r\nDistribution of the Pair Differences in Concentration between Treated and Control units for each Pollutant\r\nComputing Pairs Differences in Pollutant Concentrations\r\nPairs Differences in NO2 Concentrations\r\nPairs Differences in O3 Concentrations\r\nPairs Differences in PM10 Concentrations\r\nPairs Differences in PM2.5 Concentrations\r\nPairs Differences in SO2 Concentrations\r\n\r\nTesting the Sharp Null Hypothesis\r\nComputing Fisherian intervals\r\nChecking the Sensivity of Results\r\nOutliers\r\nMissing Outcomes\r\nNeyman’s Approach: Computing Confidence Intervals for the Average Treatment Effects\r\n\r\nHETEROGEINITY ANALYSIS\r\nALTERNATIVE MATCHING\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we take great care providing all steps and R codes required to analyze the effects of cruise traffic on air pollutants at the daily level. We compare days where:\r\ntreated units are days with positive cruise traffic in t.\r\ncontrol units are days without cruise traffic in t.\r\nWe adjust for calendar calendar indicator and weather confouding factors.\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce exactly the 2_8_analysis_results.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the 2_8_analysis_results.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we have to load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(retrodesign) # for assessing type m and s errors\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(patchwork) # combining plots\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\",\r\n                  \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nPreparing the Data\r\nWe load the matched data:\r\n\r\n\r\n# load matched data\r\ndata_matched <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"1.cruise_experiment\",\r\n      \"matched_data.rds\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nDistribution of the Pair Differences in Concentration between Treated and Control units for each Pollutant\r\nComputing Pairs Differences in Pollutant Concentrations\r\nWe first compute the differences in a pollutant’s concentration for each pair over time:\r\n\r\n\r\ndata_matched_wide <- data_matched %>%\r\n  mutate(is_treated = ifelse(is_treated == TRUE, \"treated\", \"control\")) %>%\r\n  select(\r\n    is_treated,\r\n    pair_number,\r\n    contains(\"mean_no2_l\"),\r\n    contains(\"mean_no2_sl\"),\r\n    contains(\"mean_o3\"),\r\n    contains(\"mean_pm10_l\"),\r\n    contains(\"mean_pm10_sl\"),\r\n    contains(\"mean_pm25\"),\r\n    contains(\"mean_so2\")\r\n  ) %>%\r\n  pivot_longer(\r\n    cols = -c(pair_number, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"concentration\"\r\n  ) %>%\r\n  mutate(\r\n    pollutant = NA %>%\r\n      ifelse(str_detect(variable, \"no2_l\"), \"NO2 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"no2_sl\"), \"NO2 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"o3\"), \"O3 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_l\"), \"PM10 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_sl\"), \"PM10 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"pm25\"), \"PM2.5 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"so2\"), \"SO2 Lonchamp\", .)\r\n  ) %>%\r\n  mutate(time = 0 %>%\r\n           ifelse(str_detect(variable, \"lag_1\"),-1, .) %>%\r\n           ifelse(str_detect(variable, \"lead_1\"), 1, .)) %>%\r\n  select(-variable) %>%\r\n  select(pair_number, is_treated, pollutant, time, concentration) %>%\r\n  pivot_wider(names_from = is_treated, values_from = concentration)\r\n\r\ndata_pair_difference_pollutant <- data_matched_wide %>%\r\n  mutate(difference = treated - control) %>%\r\n  select(-c(treated, control)) \r\n\r\n\r\n\r\nPairs Differences in NO2 Concentrations\r\nBoxplots for NO2:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# create the graph for no2\r\ngraph_boxplot_difference_pollutant_no2 <-\r\n  data_pair_difference_pollutant %>%\r\n  filter(str_detect(pollutant, \"NO2\")) %>%\r\n  ggplot(., aes(x = as.factor(time), y = difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  facet_wrap( ~ pollutant) +\r\n  ylab(\"Pair Difference in \\nConcentration (µg/m3)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_boxplot_difference_pollutant_no2\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_difference_pollutant_no2,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_boxplot_difference_pollutant_no2.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPairs Differences in O3 Concentrations\r\nBoxplots for O3:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# create the graph for o3\r\ngraph_boxplot_difference_pollutant_o3 <-\r\n  data_pair_difference_pollutant %>%\r\n  filter(str_detect(pollutant, \"O3\")) %>%\r\n  ggplot(., aes(x = as.factor(time), y = difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  ylab(\"Pair Difference in \\nConcentration (µg/m3)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_boxplot_difference_pollutant_o3\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_difference_pollutant_o3,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_boxplot_difference_pollutant_o3.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPairs Differences in PM10 Concentrations\r\nBoxplots for PM10:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# create the graph for pm10\r\ngraph_boxplot_difference_pollutant_pm10 <-\r\n  data_pair_difference_pollutant %>%\r\n  filter(str_detect(pollutant, \"PM10\")) %>%\r\n  ggplot(., aes(x = as.factor(time), y = difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  facet_wrap( ~ pollutant) +\r\n  ylab(\"Pair Difference in \\nConcentration (µg/m3)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_boxplot_difference_pollutant_pm10\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_difference_pollutant_pm10,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_boxplot_difference_pollutant_pm10.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPairs Differences in PM2.5 Concentrations\r\nBoxplots for PM2.5:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# create the graph for pm2.5\r\ngraph_boxplot_difference_pollutant_pm25 <-\r\n  data_pair_difference_pollutant %>%\r\n  filter(str_detect(pollutant, \"PM2.5\")) %>%\r\n  ggplot(., aes(x = as.factor(time), y = difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  ylab(\"Pair Difference in \\nConcentration (µg/m3)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_boxplot_difference_pollutant_pm25\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_difference_pollutant_pm25,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_boxplot_difference_pollutant_pm25.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nPairs Differences in SO2 Concentrations\r\nBoxplots for SO2:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# create the graph for so2\r\ngraph_boxplot_difference_pollutant_so2 <-\r\n  data_pair_difference_pollutant %>%\r\n  filter(str_detect(pollutant, \"SO2\")) %>%\r\n  ggplot(., aes(x = as.factor(time), y = difference)) +\r\n  geom_boxplot(colour = my_blue) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  ylab(\"Pair Difference in \\nConcentration (µg/m3)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_boxplot_difference_pollutant_so2\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_difference_pollutant_so2,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_boxplot_difference_pollutant_so2.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nTesting the Sharp Null Hypothesis\r\nWe test the sharp null hypothesis of no effect for any units. We first create a dataset where we nest the pair differences by pollutant and time. We also compute the observed test statistic which is the observed average of pair differences:\r\n\r\n\r\n# nest the data by pollutant and time\r\nri_data_sharp_null <- data_pair_difference_pollutant   %>%\r\n  select(pollutant, time, difference) %>%\r\n  group_by(pollutant, time) %>%\r\n  mutate(observed_mean_difference = mean(difference)) %>%\r\n  group_by(pollutant, time, observed_mean_difference) %>%\r\n  summarise(data_difference = list(difference))\r\n\r\n\r\n\r\nWe then create a function to compute the randomization distribution of the test statistic:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes the vector of pair differences\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution <- function(data_difference) {\r\n  randomization_distribution = NULL\r\n  n_columns = dim(permutations_matrix)[2]\r\n  for (i in 1:n_columns) {\r\n    randomization_distribution[i] =  sum(data_difference * permutations_matrix[, i]) / number_pairs\r\n  }\r\n  return(randomization_distribution)\r\n}\r\n\r\n\r\n\r\nWe store the number of pairs and the number of simulations we want to run:\r\n\r\n\r\n# define number of pairs in the experiment\r\nnumber_pairs <- nrow(data_matched) / 2\r\n\r\n# define number of simulations\r\nnumber_simulations <- 100000\r\n\r\n\r\n\r\nWe compute the permutations matrix:\r\n\r\n\r\n# set seed\r\nset.seed(42)\r\n\r\n# compute the permutations matrix\r\npermutations_matrix <-\r\n  matrix(\r\n    rbinom(number_pairs * number_simulations, 1, .5) * 2 - 1,\r\n    nrow = number_pairs,\r\n    ncol = number_simulations\r\n  )\r\n\r\n\r\n\r\nFor each pollutant and time, we compute the randomization distribution of the test statistic using 100,000 iterations. It took us 46 seconds to run this code chunck on our basic local computer:\r\n\r\n\r\n# set seed\r\nset.seed(42)\r\n\r\n# compute the test statistic distribution\r\nri_data_sharp_null <- ri_data_sharp_null %>%\r\n  mutate(\r\n    randomization_distribution = map(data_difference, ~ function_randomization_distribution(.))\r\n  )\r\n\r\n\r\n\r\nUsing the observed value of the test statistic and its randomization distribution, we compute the two-sided p-values:\r\n\r\n\r\n# function to compute the upper one-sided p-value\r\nfunction_fisher_upper_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution >= observed_mean_difference) / number_simulations\r\n  }\r\n\r\n# function compute the lower one-sided p-value\r\nfunction_fisher_lower_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution <= observed_mean_difference) / number_simulations\r\n  }\r\n\r\n# compute the lower and upper one-sided p-values\r\nri_data_sharp_null <- ri_data_sharp_null %>%\r\n  mutate(\r\n    p_value_upper = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_upper_p_value(.x, .y)\r\n    ),\r\n    p_value_lower = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_lower_p_value(.x, .y)\r\n    )\r\n  )\r\n\r\n# compute the two-sided p-value using rosenbaum (2010) procedure\r\nri_data_sharp_null <- ri_data_sharp_null %>%\r\n  rowwise() %>%\r\n  mutate(two_sided_p_value = min(c(p_value_upper, p_value_lower)) * 2) %>%\r\n  mutate(two_sided_p_value = min(two_sided_p_value, 1)) %>%\r\n  select(pollutant, time, observed_mean_difference, two_sided_p_value) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nWe plot below the two-sided p-values for the sharp null hypothesis for each pollutant:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make the graph\r\ngraph_p_values <- ri_data_sharp_null %>%\r\n  ggplot(., aes(x = as.factor(time), y = two_sided_p_value)) +\r\n  geom_segment(aes(\r\n    x = as.factor(time),\r\n    xend = as.factor(time),\r\n    y = 0,\r\n    yend = two_sided_p_value\r\n  )) +\r\n  geom_point(\r\n    shape = 21,\r\n    size = 4,\r\n    colour = \"black\",\r\n    fill = my_blue\r\n  ) +\r\n  facet_wrap( ~ pollutant, ncol = 4) +\r\n  xlab(\"Day\") + ylab(\"Two-Sided P-Value\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_p_values\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_p_values,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_p_values.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe display below the table of Fisher p-values:\r\n\r\n\r\nPlease show me the code!\r\n\r\nri_data_sharp_null %>%\r\n  select(pollutant, time, observed_mean_difference, two_sided_p_value) %>%\r\n  rename(\r\n    \"Pollutant\" = pollutant,\r\n    \"Time\" = time,\r\n    \"Observed Value of the Test Statistic\" = observed_mean_difference,\r\n    \"Two-Sided P-Values\" = two_sided_p_value\r\n  ) %>%\r\n  rmarkdown::paged_table(.)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"Pollutant\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Time\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Observed Value of the Test Statistic\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Two-Sided P-Values\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"NO2 Longchamp\",\"2\":\"-1\",\"3\":\"-0.67724868\",\"4\":\"0.44650\"},{\"1\":\"NO2 Longchamp\",\"2\":\"0\",\"3\":\"-0.21164021\",\"4\":\"0.80164\"},{\"1\":\"NO2 Longchamp\",\"2\":\"1\",\"3\":\"0.15343915\",\"4\":\"0.88958\"},{\"1\":\"NO2 Saint-Louis\",\"2\":\"-1\",\"3\":\"-0.27513228\",\"4\":\"0.80704\"},{\"1\":\"NO2 Saint-Louis\",\"2\":\"0\",\"3\":\"0.56613757\",\"4\":\"0.60136\"},{\"1\":\"NO2 Saint-Louis\",\"2\":\"1\",\"3\":\"0.88359788\",\"4\":\"0.57036\"},{\"1\":\"O3 Longchamp\",\"2\":\"-1\",\"3\":\"-0.77777778\",\"4\":\"0.48822\"},{\"1\":\"O3 Longchamp\",\"2\":\"0\",\"3\":\"-1.14285714\",\"4\":\"0.31590\"},{\"1\":\"O3 Longchamp\",\"2\":\"1\",\"3\":\"-0.67724868\",\"4\":\"0.60584\"},{\"1\":\"PM10 Longchamp\",\"2\":\"-1\",\"3\":\"-0.22222222\",\"4\":\"0.79804\"},{\"1\":\"PM10 Longchamp\",\"2\":\"0\",\"3\":\"-0.63492063\",\"4\":\"0.39290\"},{\"1\":\"PM10 Longchamp\",\"2\":\"1\",\"3\":\"0.30158730\",\"4\":\"0.69542\"},{\"1\":\"PM10 Saint-Louis\",\"2\":\"-1\",\"3\":\"0.87301587\",\"4\":\"0.44396\"},{\"1\":\"PM10 Saint-Louis\",\"2\":\"0\",\"3\":\"-0.60846561\",\"4\":\"0.55014\"},{\"1\":\"PM10 Saint-Louis\",\"2\":\"1\",\"3\":\"0.29629630\",\"4\":\"0.79076\"},{\"1\":\"PM2.5 Longchamp\",\"2\":\"-1\",\"3\":\"-0.31746032\",\"4\":\"0.60510\"},{\"1\":\"PM2.5 Longchamp\",\"2\":\"0\",\"3\":\"-0.07407407\",\"4\":\"0.90092\"},{\"1\":\"PM2.5 Longchamp\",\"2\":\"1\",\"3\":\"0.58730159\",\"4\":\"0.32306\"},{\"1\":\"SO2 Lonchamp\",\"2\":\"-1\",\"3\":\"-0.02645503\",\"4\":\"0.93780\"},{\"1\":\"SO2 Lonchamp\",\"2\":\"0\",\"3\":\"0.07407407\",\"4\":\"0.82164\"},{\"1\":\"SO2 Lonchamp\",\"2\":\"1\",\"3\":\"0.08994709\",\"4\":\"0.74070\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nComputing Fisherian intervals\r\nTo compute Fisherian intervals, we first create a nested dataset with the pair differences for each pollutant and day. We also add the set of hypothetical constant effects.\r\n\r\n\r\n# create a nested dataframe with\r\n# the set of constant treatment effect sizes\r\n# and the vector of observed pair differences\r\nri_data_fi <- data_pair_difference_pollutant %>%\r\n  select(pollutant, time, difference) %>%\r\n  group_by(pollutant, time) %>%\r\n  summarise(data_difference = list(difference)) %>%\r\n  group_by(pollutant, time, data_difference) %>%\r\n  expand(effect = seq(from = -10, to = 10, by = 0.1)) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nWe then substract for each pair difference the hypothetical constant effect:\r\n\r\n\r\n# function to get the observed statistic\r\nadjusted_pair_difference_function <-\r\n  function(pair_differences, effect) {\r\n    adjusted_pair_difference <- pair_differences - effect\r\n    return(adjusted_pair_difference)\r\n  }\r\n\r\n# compute the adjusted pair differences\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    data_adjusted_pair_difference = map2(\r\n      data_difference,\r\n      effect,\r\n      ~ adjusted_pair_difference_function(.x, .y)\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe compute the observed mean of adjusted pair differences:\r\n\r\n\r\n# compute the observed mean of adjusted pair differences\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(observed_mean_difference = map(data_adjusted_pair_difference, ~ mean(.))) %>%\r\n  unnest(cols = c(observed_mean_difference)) %>%\r\n  select(-data_difference) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nWe use the same function_randomization_distribution to compute the randomization distribution of the test statistic but only run 10,000 iterations for each pollutant-day observation:\r\n\r\n\r\n# define number of pairs in the experiment\r\nnumber_pairs <- nrow(data_matched) / 2\r\n\r\n# define number of simulations\r\nnumber_simulations <- 10000\r\n\r\n# set seed\r\nset.seed(42)\r\n\r\n# compute the permutations matrix\r\npermutations_matrix <-\r\n  matrix(\r\n    rbinom(number_pairs * number_simulations, 1, .5) * 2 - 1,\r\n    nrow = number_pairs,\r\n    ncol = number_simulations\r\n  )\r\n\r\n# randomization distribution function\r\n# this function takes the vector of pair differences\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution <- function(data_difference) {\r\n  randomization_distribution = NULL\r\n  n_columns = dim(permutations_matrix)[2]\r\n  for (i in 1:n_columns) {\r\n    randomization_distribution[i] =  sum(data_difference * permutations_matrix[, i]) / number_pairs\r\n  }\r\n  return(randomization_distribution)\r\n}\r\n\r\n\r\n\r\nWe ran the function. It took about one and a half minutes to run on our laptop computer. To quickly compile the .Rmd document, we therefore store the results of the simulations. The code we used is displayed below:\r\n\r\n\r\n# set seed\r\nset.seed(42)\r\n\r\ntictoc::tic()\r\n\r\n# compute the test statistic distribution\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    randomization_distribution = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_randomization_distribution(.)\r\n    )\r\n  )\r\n\r\n#----------------------------------------------------\r\n\r\n# Computing the lower and upper *p*-values functions\r\n\r\n#----------------------------------------------------\r\n\r\n# define the p-values functions\r\nfunction_fisher_upper_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution >= observed_mean_difference) / number_simulations\r\n  }\r\n\r\nfunction_fisher_lower_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution <= observed_mean_difference) / number_simulations\r\n  }\r\n\r\n# compute the lower and upper one-sided p-values\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    p_value_upper = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_upper_p_value(.x, .y)\r\n    ),\r\n    p_value_lower = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_lower_p_value(.x, .y)\r\n    )\r\n  )\r\n\r\n#----------------------------------------------------------\r\n\r\n# RETRIEVING LOWER AND UPPER BOUNDS OF FISHERIAN INTERVALS\r\n\r\n#----------------------------------------------------------\r\n\r\n# retrieve the constant effects with the p-values equal or the closest to 0.025\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    p_value_upper = abs(p_value_upper - 0.025),\r\n    p_value_lower = abs(p_value_lower - 0.025)\r\n  ) %>%\r\n  group_by(pollutant, time) %>%\r\n  filter(p_value_upper == min(p_value_upper) |\r\n           p_value_lower == min(p_value_lower)) %>%\r\n  # in case two effect sizes have a p-value equal to 0.025, we take the effect size\r\n  # that make the Fisherian interval wider to be conservative\r\n  summarise(lower_fi = min(effect),\r\n            upper_fi = max(effect))\r\n\r\n#----------------------------------------------------------\r\n\r\n# COMPUTING POINT ESTIMATES\r\n\r\n#----------------------------------------------------------\r\n\r\n# compute observed average of pair differences\r\nri_data_fi_point_estimate <- data_pair_difference_pollutant   %>%\r\n  select(pollutant, time, difference) %>%\r\n  group_by(pollutant, time) %>%\r\n  summarise(observed_mean_difference = mean(difference)) %>%\r\n  ungroup()\r\n\r\n#----------------------------------------------------------\r\n\r\n# MERGING POINT ESTIMATES WITH INTERVALS\r\n\r\n#----------------------------------------------------------\r\n\r\n# merge ri_data_fi_point_estimate with ri_data_fi\r\nri_data_fi_final <-\r\n  left_join(ri_data_fi,\r\n            ri_data_fi_point_estimate,\r\n            by = c(\"pollutant\", \"time\"))\r\n\r\n# create an indicator to alternate shading of confidence intervals\r\nri_data_fi_final <- ri_data_fi_final %>%\r\n  arrange(pollutant, time) %>%\r\n  mutate(stripe = ifelse((time %% 2) == 0, \"Grey\", \"White\")) %>%\r\n  ungroup()\r\n\r\n# save the data\r\nsaveRDS(\r\n  ri_data_fi_final,\r\n  here::here(\r\n    \"inputs\",\r\n    \"1.data\",\r\n    \"2.daily_data\",\r\n    \"2.data_for_analysis\",\r\n    \"1.cruise_experiment\",\r\n    \"ri_data_fisherian_intervals.rds\"\r\n  )\r\n)\r\n\r\ntictoc::toc()\r\n\r\n\r\n\r\nWe plot below the 95% Fisherian intervals:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# read the data on 95% fisherian intervals\r\nri_data_fi_final <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"2.data_for_analysis\",\r\n      \"1.cruise_experiment\",\r\n      \"ri_data_fisherian_intervals.rds\"\r\n    )\r\n  )\r\n\r\n# make the graph\r\ngraph_fisherian_intervals <-\r\n  ggplot(ri_data_fi_final,\r\n         aes(x = as.factor(time), y = observed_mean_difference)) +\r\n  geom_rect(\r\n    aes(fill = stripe),\r\n    xmin = as.numeric(as.factor(ri_data_fi_final$time)) - 0.42,\r\n    xmax = as.numeric(as.factor(ri_data_fi_final$time)) + 0.42,\r\n    ymin = -Inf,\r\n    ymax = Inf,\r\n    color = NA,\r\n    alpha = 0.4\r\n  ) +\r\n  geom_hline(yintercept = 0, color = \"black\") +\r\n  geom_pointrange(\r\n    aes(\r\n      x = as.factor(time),\r\n      y = observed_mean_difference,\r\n      ymin = lower_fi ,\r\n      ymax = upper_fi\r\n    ),\r\n    colour = my_blue,\r\n    lwd = 1.2\r\n  ) +\r\n  facet_wrap( ~ pollutant, ncol = 4) +\r\n  scale_fill_manual(values = c('grey90', \"white\")) +\r\n  guides(fill = FALSE) +\r\n  ylab(\"Constant-Additive Increase \\nin Concentrations (µg/m³)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# print the graph\r\ngraph_fisherian_intervals\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_fisherian_intervals,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_fisherian_intervals.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe display below the table with the 95% fisherian intervals and the Hodges-Lehmann point estimates:\r\n\r\n\r\nPlease show me the code!\r\n\r\nri_data_fi_final %>%\r\n  select(pollutant, time, observed_mean_difference, lower_fi, upper_fi) %>%\r\n  mutate(observed_mean_difference = round(observed_mean_difference, 1)) %>%\r\n  rename(\r\n    \"Pollutant\" = pollutant,\r\n    \"Time\" = time,\r\n    \"Point Estimate\" = observed_mean_difference,\r\n    \"Lower Bound of the 95% Fisherian Interval\" = lower_fi,\r\n    \"Upper Bound of the 95% Fisherian Interval\" = upper_fi\r\n  ) %>%\r\n  rmarkdown::paged_table(.)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"Pollutant\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Time\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Point Estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Lower Bound of the 95% Fisherian Interval\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Upper Bound of the 95% Fisherian Interval\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"NO2 Longchamp\",\"2\":\"-1\",\"3\":\"-0.7\",\"4\":\"-2.4\",\"5\":\"1.1\"},{\"1\":\"NO2 Longchamp\",\"2\":\"0\",\"3\":\"-0.2\",\"4\":\"-1.8\",\"5\":\"1.4\"},{\"1\":\"NO2 Longchamp\",\"2\":\"1\",\"3\":\"0.2\",\"4\":\"-2.0\",\"5\":\"2.3\"},{\"1\":\"NO2 Saint-Louis\",\"2\":\"-1\",\"3\":\"-0.3\",\"4\":\"-2.4\",\"5\":\"1.9\"},{\"1\":\"NO2 Saint-Louis\",\"2\":\"0\",\"3\":\"0.6\",\"4\":\"-1.6\",\"5\":\"2.7\"},{\"1\":\"NO2 Saint-Louis\",\"2\":\"1\",\"3\":\"0.9\",\"4\":\"-2.2\",\"5\":\"4.0\"},{\"1\":\"O3 Longchamp\",\"2\":\"-1\",\"3\":\"-0.8\",\"4\":\"-3.0\",\"5\":\"1.4\"},{\"1\":\"O3 Longchamp\",\"2\":\"0\",\"3\":\"-1.1\",\"4\":\"-3.4\",\"5\":\"1.1\"},{\"1\":\"O3 Longchamp\",\"2\":\"1\",\"3\":\"-0.7\",\"4\":\"-3.2\",\"5\":\"1.9\"},{\"1\":\"PM10 Longchamp\",\"2\":\"-1\",\"3\":\"-0.2\",\"4\":\"-1.9\",\"5\":\"1.5\"},{\"1\":\"PM10 Longchamp\",\"2\":\"0\",\"3\":\"-0.6\",\"4\":\"-2.1\",\"5\":\"0.8\"},{\"1\":\"PM10 Longchamp\",\"2\":\"1\",\"3\":\"0.3\",\"4\":\"-1.2\",\"5\":\"1.8\"},{\"1\":\"PM10 Saint-Louis\",\"2\":\"-1\",\"3\":\"0.9\",\"4\":\"-1.3\",\"5\":\"3.1\"},{\"1\":\"PM10 Saint-Louis\",\"2\":\"0\",\"3\":\"-0.6\",\"4\":\"-2.6\",\"5\":\"1.4\"},{\"1\":\"PM10 Saint-Louis\",\"2\":\"1\",\"3\":\"0.3\",\"4\":\"-1.9\",\"5\":\"2.5\"},{\"1\":\"PM2.5 Longchamp\",\"2\":\"-1\",\"3\":\"-0.3\",\"4\":\"-1.5\",\"5\":\"0.9\"},{\"1\":\"PM2.5 Longchamp\",\"2\":\"0\",\"3\":\"-0.1\",\"4\":\"-1.2\",\"5\":\"1.1\"},{\"1\":\"PM2.5 Longchamp\",\"2\":\"1\",\"3\":\"0.6\",\"4\":\"-0.6\",\"5\":\"1.8\"},{\"1\":\"SO2 Lonchamp\",\"2\":\"-1\",\"3\":\"0.0\",\"4\":\"-0.6\",\"5\":\"0.5\"},{\"1\":\"SO2 Lonchamp\",\"2\":\"0\",\"3\":\"0.1\",\"4\":\"-0.5\",\"5\":\"0.7\"},{\"1\":\"SO2 Lonchamp\",\"2\":\"1\",\"3\":\"0.1\",\"4\":\"-0.4\",\"5\":\"0.6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nChecking the Sensivity of Results\r\nIn this section, we carry out four investigations:\r\nWe check how our results are sensitive to outliers by computing 95% fisherian intervals based on the Wilcoxon’s signed rank test statistic.\r\nAs we imputed the missing pollutant concentrations, we also want to see how our results might for the non-missing outcomes. We compute 95% fisherian intervals based on the Wilcoxon’s signed rank test statistic.\r\nWe compute confidence intervals for the average treatment effect using Neyman’s approach.\r\nOutliers\r\nTo gauge how sensitive our results are to outliers, we use a Wilcoxon signed rank test statistic and compute 95% fisherian intervals using the wilcox.test() function.\r\n\r\n\r\nPlease show me the code!\r\n\r\n# carry out the wilcox.test\r\ndata_rank_ci <- data_pair_difference_pollutant %>%\r\n  select(-pair_number) %>%\r\n  group_by(pollutant, time) %>%\r\n  nest() %>%\r\n  mutate(\r\n    effect = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$estimate),\r\n    lower_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[1]),\r\n    upper_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[2])\r\n  ) %>%\r\n  unnest(cols = c(effect, lower_ci, upper_ci)) %>%\r\n  mutate(data = \"Wilcoxon Rank Test Statistic\")\r\n\r\n# bind ri_data_fi_final with data_rank_ci\r\ndata_ci <- ri_data_fi_final %>%\r\n  rename(effect = observed_mean_difference,\r\n         lower_ci = lower_fi,\r\n         upper_ci = upper_fi) %>%\r\n  mutate(data = \"Average Pair Difference Test Statistic\") %>%\r\n  bind_rows(., data_rank_ci)\r\n\r\n# create an indicator to alternate shading of confidence intervals\r\ndata_ci <- data_ci %>%\r\n  arrange(pollutant, time) %>%\r\n  mutate(stripe = ifelse((time %% 2) == 0, \"Grey\", \"White\")) %>%\r\n  ungroup()\r\n\r\n# make the graph\r\ngraph_ri_ci_wilcoxon <-\r\n  ggplot(\r\n    data_ci,\r\n    aes(\r\n      x = as.factor(time),\r\n      y = effect,\r\n      ymin = lower_ci,\r\n      ymax = upper_ci,\r\n      colour = data,\r\n      shape = data\r\n    )\r\n  ) +\r\n  geom_rect(\r\n    aes(fill = stripe),\r\n    xmin = as.numeric(as.factor(data_ci$time)) - 0.42,\r\n    xmax = as.numeric(as.factor(data_ci$time)) + 0.42,\r\n    ymin = -Inf,\r\n    ymax = Inf,\r\n    color = NA,\r\n    alpha = 0.4\r\n  ) +\r\n  geom_hline(yintercept = 0, color = \"black\") +\r\n  geom_pointrange(position = position_dodge(width = 1), size = 1.2) +\r\n  scale_shape_manual(name = \"Test Statistic:\", values = c(16, 17)) +\r\n  scale_color_manual(name = \"Test Statistic:\", values = c(my_orange, my_blue)) +\r\n  facet_wrap(~ pollutant, ncol = 4) +\r\n  scale_fill_manual(values = c('grey90', \"white\")) +\r\n  guides(fill = FALSE) +\r\n  ylab(\"Constant-Additive Increase \\nin Concentrations (µg/m³)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# print the graph\r\ngraph_ri_ci_wilcoxon\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_ri_ci_wilcoxon,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_ri_ci_wilcoxon.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nMissing Outcomes\r\nWe load non-imputed air pollution data and compute for each pollutant the 0-1 daily lags and leads:\r\n\r\n\r\n# load marseille raw air pollution data\r\ndata_marseille_raw_pollutants <-\r\n  readRDS(\r\n    here::here(\r\n      \"inputs\",\r\n      \"1.data\",\r\n      \"2.daily_data\",\r\n      \"1.raw_data\" ,\r\n      \"3.pollution_data\",\r\n      \"raw_marseille_pollutants_2008_2018_data.rds\"\r\n    )\r\n  ) %>%\r\n  rename_at(vars(-date), function(x)\r\n    paste0(\"raw_\", x))\r\n\r\n# we first define data_marseille_raw_pollutants_leads and data_marseille_raw_pollutants_lags\r\n# to store leads and lags\r\n\r\ndata_marseille_raw_pollutants_leads <- data_marseille_raw_pollutants\r\ndata_marseille_raw_pollutants_lags <- data_marseille_raw_pollutants\r\n\r\n#\r\n# create leads\r\n#\r\n\r\n# create a list to store dataframe of leads\r\nleads_list <- vector(mode = \"list\", length = 1)\r\nnames(leads_list) <- c(1)\r\n\r\n# create the leads\r\nfor (i in 1) {\r\n  leads_list[[i]] <- data_marseille_raw_pollutants_leads %>%\r\n    mutate_at(vars(-date), ~  lead(., n = i, order_by = date)) %>%\r\n    rename_at(vars(-date), function(x)\r\n      paste0(x, \"_lead_\", i))\r\n}\r\n\r\n# merge the dataframes of leads\r\ndata_leads <- leads_list %>%\r\n  reduce(left_join, by = \"date\")\r\n\r\n# merge the leads with the data_marseille_raw_pollutants_leads\r\ndata_marseille_raw_pollutants_leads <-\r\n  left_join(data_marseille_raw_pollutants_leads, data_leads, by = \"date\") %>%\r\n  select(-c(raw_mean_no2_sl:raw_mean_o3_l))\r\n\r\n#\r\n# create lags\r\n#\r\n\r\n# create a list to store dataframe of lags\r\nlags_list <- vector(mode = \"list\", length = 1)\r\nnames(lags_list) <- c(1)\r\n\r\n# create the lags\r\nfor (i in 1) {\r\n  lags_list[[i]] <- data_marseille_raw_pollutants_lags %>%\r\n    mutate_at(vars(-date), ~  lag(., n = i, order_by = date)) %>%\r\n    rename_at(vars(-date), function(x)\r\n      paste0(x, \"_lag_\", i))\r\n}\r\n\r\n# merge the dataframes of lags\r\ndata_lags <- lags_list %>%\r\n  reduce(left_join, by = \"date\")\r\n\r\n# merge the lags with the initial data_marseille_raw_pollutants_lags\r\ndata_marseille_raw_pollutants_lags <-\r\n  left_join(data_marseille_raw_pollutants_lags, data_lags, by = \"date\")\r\n\r\n#\r\n# merge data_marseille_raw_pollutants_leads with data_marseille_raw_pollutants_lags\r\n#\r\n\r\ndata_marseille_raw_pollutants <-\r\n  left_join(data_marseille_raw_pollutants_lags,\r\n            data_marseille_raw_pollutants_leads,\r\n            by = \"date\")\r\n\r\n\r\n\r\nWe merge these data with the matched data and compute pair differences:\r\n\r\n\r\n# merge with the matched_data\r\ndata_matched_with_raw_pollutants <-\r\n  left_join(data_matched, data_marseille_raw_pollutants, by = \"date\")\r\n\r\n# compute pair differences\r\ndata_matched_wide_raw_pollutants <-\r\n  data_matched_with_raw_pollutants %>%\r\n  mutate(is_treated = ifelse(is_treated == TRUE, \"treated\", \"control\")) %>%\r\n  select(\r\n    is_treated,\r\n    pair_number,\r\n    contains(\"raw_mean_no2_l\"),\r\n    contains(\"raw_mean_no2_sl\"),\r\n    contains(\"raw_mean_o3\"),\r\n    contains(\"raw_mean_pm10_l\"),\r\n    contains(\"raw_mean_pm10_sl\"),\r\n    contains(\"raw_mean_pm25\"),\r\n    contains(\"raw_mean_so2\")\r\n  ) %>%\r\n  pivot_longer(\r\n    cols = -c(pair_number, is_treated),\r\n    names_to = \"variable\",\r\n    values_to = \"concentration\"\r\n  ) %>%\r\n  mutate(\r\n    pollutant = NA %>%\r\n      ifelse(str_detect(variable, \"no2_l\"), \"NO2 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"no2_sl\"), \"NO2 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"o3\"), \"O3 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_l\"), \"PM10 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"pm10_sl\"), \"PM10 Saint-Louis\", .) %>%\r\n      ifelse(str_detect(variable, \"pm25\"), \"PM2.5 Longchamp\", .) %>%\r\n      ifelse(str_detect(variable, \"so2\"), \"SO2 Lonchamp\", .)\r\n  ) %>%\r\n  mutate(time = 0 %>%\r\n           ifelse(str_detect(variable, \"lag_1\"),-1, .) %>%\r\n           ifelse(str_detect(variable, \"lead_1\"), 1, .)) %>%\r\n  select(-variable) %>%\r\n  select(pair_number, is_treated, pollutant, time, concentration) %>%\r\n  pivot_wider(names_from = is_treated, values_from = concentration)\r\n\r\ndata_raw_pair_difference_pollutant <-\r\n  data_matched_wide_raw_pollutants %>%\r\n  mutate(difference = treated - control) %>%\r\n  select(-c(treated, control)) \r\n\r\n\r\n\r\nWe display below the number of missing differences by pollutant and day:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make the graph\r\ngraph_missing_pollutants <- data_raw_pair_difference_pollutant %>%\r\n  group_by(pollutant, time) %>%\r\n  summarise(n_missing = sum(is.na(difference))) %>%\r\n  ggplot(., aes(x = as.factor(time), y = n_missing)) +\r\n  geom_segment(aes(\r\n    x = as.factor(time),\r\n    xend = as.factor(time),\r\n    y = 0,\r\n    yend = n_missing\r\n  )) +\r\n  geom_point(\r\n    shape = 21,\r\n    size = 4,\r\n    colour = \"black\",\r\n    fill = my_blue\r\n  ) +\r\n  facet_wrap( ~ pollutant, ncol = 4) +\r\n  xlab(\"Day\") + ylab(\"Number of Pairs with \\nMissing Concentrations\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_missing_pollutants\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_missing_pollutants,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_missing_pollutants.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nAs we have 189 pairs, up to 21% of the pairs can have missing pollutant concentrations. We compute below the 95% fisherian intervals for pairs without missing concentrations and compare the results to those found with the imputed dataset:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# carry out the wilcox.test\r\ndata_raw_rank_ci <- data_raw_pair_difference_pollutant %>%\r\n  drop_na() %>%\r\n  select(-pair_number) %>%\r\n  group_by(pollutant, time) %>%\r\n  nest() %>%\r\n  mutate(\r\n    effect = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$estimate),\r\n    lower_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[1]),\r\n    upper_ci = map(data, ~ wilcox.test(.$difference, conf.int = TRUE)$conf.int[2])\r\n  ) %>%\r\n  unnest(cols = c(effect, lower_ci, upper_ci)) %>%\r\n  mutate(data = \"Pairs without Missing Concentrations\")\r\n\r\n# bind data_rank_ci with data_raw_rank_ci\r\ndata_ci <- data_rank_ci %>%\r\n  mutate(data = \"Pairs with Imputed Pollutant Concentrations\") %>%\r\n  bind_rows(., data_raw_rank_ci)\r\n\r\n# create an indicator to alternate shading of confidence intervals\r\ndata_ci <- data_ci %>%\r\n  arrange(pollutant, time) %>%\r\n  mutate(stripe = ifelse((time %% 2) == 0, \"Grey\", \"White\")) %>%\r\n  ungroup()\r\n\r\n# make the graph\r\ngraph_ri_ci_missing_concentration <-\r\n  ggplot(\r\n    data_ci,\r\n    aes(\r\n      x = as.factor(time),\r\n      y = effect,\r\n      ymin = lower_ci,\r\n      ymax = upper_ci,\r\n      colour = data,\r\n      shape = data\r\n    )\r\n  ) +\r\n  geom_rect(\r\n    aes(fill = stripe),\r\n    xmin = as.numeric(as.factor(data_ci$time)) - 0.42,\r\n    xmax = as.numeric(as.factor(data_ci$time)) + 0.42,\r\n    ymin = -Inf,\r\n    ymax = Inf,\r\n    color = NA,\r\n    alpha = 0.4\r\n  ) +\r\n  geom_hline(yintercept = 0, color = \"black\") +\r\n  geom_pointrange(position = position_dodge(width = 1), size = 1.2) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(16, 17)) +\r\n  scale_color_manual(name = \"Dataset:\", values = c(my_orange, my_blue)) +\r\n  facet_wrap( ~ pollutant, ncol = 4) +\r\n  scale_fill_manual(values = c('grey90', \"white\")) +\r\n  guides(fill = FALSE) +\r\n  ylab(\"Constant-Additive Increase \\nin Concentrations (µg/m³)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# print the graph\r\ngraph_ri_ci_missing_concentration\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_ri_ci_missing_concentration,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_ri_ci_missing_concentration.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nNeyman’s Approach: Computing Confidence Intervals for the Average Treatment Effects\r\nWe compute confidence intervals for the average treatement using Neyman’s approach. We use the formula for the standard error of pair randomized experiment found in Imbens and Rubin (2015).\r\n\r\n\r\n# we first compute the average treatment effects for each pollutant and hour\r\ndata_pair_mean_difference <- data_pair_difference_pollutant %>%\r\n  group_by(pollutant, time) %>%\r\n  summarise(mean_difference = mean(difference)) %>%\r\n  ungroup()\r\n\r\n# we store the number of pairs\r\nn_pair <- nrow(data_matched) / 2\r\n\r\n# compute the standard error\r\ndata_se_neyman_pair <-\r\n  left_join(\r\n    data_pair_difference_pollutant,\r\n    data_pair_mean_difference,\r\n    by = c(\"pollutant\", \"time\")\r\n  ) %>%\r\n  mutate(squared_difference = (difference - mean_difference) ^ 2) %>%\r\n  group_by(pollutant, time) %>%\r\n  summarise(standard_error = sqrt(1 / (n_pair * (n_pair - 1)) * sum(squared_difference))) %>%\r\n  select(pollutant, time, standard_error) %>%\r\n  ungroup()\r\n\r\n# merge the average treatment effect data witht the standard error data\r\ndata_neyman <-\r\n  left_join(data_pair_mean_difference,\r\n            data_se_neyman_pair,\r\n            by = c(\"pollutant\", \"time\")) %>%\r\n  # compute the 95% confidence intervals\r\n  mutate(\r\n    ci_lower_95 = mean_difference - 1.96 * standard_error,\r\n    ci_upper_95 = mean_difference + 1.96 * standard_error\r\n  )\r\n\r\n\r\n\r\nWe plot the the point estimates for the average treatment effects and their associated 95% confidence intervals:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# create an indicator to alternate shading of confidence intervals\r\ndata_neyman <- data_neyman %>%\r\n  arrange(pollutant, time) %>%\r\n  mutate(stripe = ifelse((time %% 2) == 0, \"Grey\", \"White\")) %>%\r\n  ungroup()\r\n\r\n# make the graph\r\ngraph_neyman_ci <-\r\n  ggplot(data_neyman, aes(x = as.factor(time), y = mean_difference)) +\r\n  geom_rect(\r\n    aes(fill = stripe),\r\n    xmin = as.numeric(as.factor(data_neyman$time)) - 0.42,\r\n    xmax = as.numeric(as.factor(data_neyman$time)) + 0.42,\r\n    ymin = -Inf,\r\n    ymax = Inf,\r\n    color = NA,\r\n    alpha = 0.4\r\n  ) +\r\n  geom_hline(yintercept = 0, color = \"black\") +\r\n  geom_pointrange(\r\n    aes(\r\n      x = as.factor(time),\r\n      y = mean_difference,\r\n      ymin = ci_lower_95 ,\r\n      ymax = ci_upper_95\r\n    ),\r\n    colour = my_blue,\r\n    lwd = 1.2\r\n  ) +\r\n  facet_wrap(~ pollutant, ncol = 4) +\r\n  scale_fill_manual(values = c('grey90', \"white\")) +\r\n  guides(fill = FALSE) +\r\n  ylab(\"Average Pair Difference \\nin Concentrations (µg/m³)\") + xlab(\"Day\") +\r\n  theme_tufte()\r\n\r\n# print the graph\r\ngraph_neyman_ci\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_neyman_ci,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.daily_analysis\",\r\n    \"2.analysis_pollution\",\r\n    \"1.cruise_experiment\",\r\n    \"2.matching_results\",\r\n    \"graph_ci_neyman.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nHETEROGEINITY ANALYSIS\r\nALTERNATIVE MATCHING\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-21T11:53:48+02:00"
    },
    {
      "path": "3_toy_example_randomization_inference.html",
      "title": "Toy Example for Understanding Randomization Inference",
      "description": "How to Compute Fisherian Intervals?\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nToy Example\r\nScience Table\r\nObserved Data\r\n\r\nTesting the Sharp Null Hypothesis of No Treatment\r\nStating the Hypothesis\r\nComputional Shortcut\r\nComputing the Null Distribution of the Test Statistic\r\nComputing the Two-Sided P-Value\r\n\r\nComputing a 95% Fisherian Intervals for a Range of Sharp Null Hypotheses\r\nSteps of the Procedure\r\nComputational Shortcut\r\nImplementation in R\r\n\r\nComparison with Neyman’s Approach\r\nComputing a 95% Fisherian Intervals For Weak Null Hypotheses\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we explain with a toy example how to:\r\nTest the sharp null hypothesis of no effect for all units.\r\nCarry out a test inversion procedure to compute a 95% Fisherian interval for constant treatment effects consistent with the data.\r\nCompare the results with Neyman’s approach targeting the average causal effect.\r\nAdapt the randomization inference for testing weak null hypotheses (i.e., average causal effects).\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce the 3_toy_example_randomization_inference.html document, we need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the 3_toy_example_randomization_inference.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(kableExtra) # for building nice tables\r\nlibrary(Cairo) # for printing custom police of graphs\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\r\n  \"inputs\",\r\n  \"2.functions\",\r\n  \"script_theme_tufte.R\"\r\n))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nToy Example\r\nIn this toy example, we aim to estimate the effect of cruise vessels docking at Marseille’s port on NO\\(_{2}\\) concentration:\r\nFor simplicity, imagine that our matching procedure resulted in 10 pairs of hours with similar weather and calendar characteristics (i.e., same average temperature, same day of the week, etc…).\r\nTreated units are hours with cruise vessels docking at the port while control units are hours without cruise vessels.\r\nThe outcome of the experiment is the hourly NO\\(_{2}\\) concentration measured at a station located in the city.\r\nThe exposition of this toy example is inspired by those found in the chapter II of Paul Rosenbaum’s textbook and Tirthankar Dasgupta and Donald B. Rubin’s forthcoming textbook (Experimental Design: A Randomization-Based Perspective).\r\nScience Table\r\nWe display below the Science Table of our imaginary experiment, that is to say the table showing the treatment status and potential outcomes for each observation:\r\nThe first column Pair is the indicator of the pair. We represent the index of a pair by i which takes values from 1 to 5.\r\nThe second column Unit Index is the index j of a unit within the pair (j is equal to 1 for the first unit in the pair and to 2 for the second unit).\r\nThe third column W indicates the treatment allocation. W = 1 for treated units and W = 0 for controls.\r\nThe fourth and fifth columns are the potential outcomes of each unit and represent the NO\\(_{2}\\) concentrations measured in \\(\\mu g/m^{3}\\). Y(W = 0) is the potential outcome when the unit does not receive the treatment and Y(W = 1) is the potential outcome when the unit is treated. As this is an artificial example, we imagine that we know for each unit the values of both potential outcomes.\r\nThe six column \\(\\tau\\) is the unit constant causal effect. Here, the causal effect is equal to +3 \\(\\mu g/m^{3}\\).\r\nThe last column Y\\(^{obs}\\) represents the potential outcome that we would observe according to the treatment allocation, that is to say \\(Y_{i,j} = W_{i,j}\\times Y_{i,j}(1) + (1-W_{i,j})\\times Y_{i,j}(0)\\). Here, in each pair, the first unit does not receive the treatment so that we observe Y(0) while the second unit is treated and we observe Y(1).\r\n\r\n\r\n# load the science table\r\nscience_table <- readRDS(here::here(\"inputs\", \"1.data\", \"3.toy_example\", \"science_table.RDS\"))\r\n\r\n# display the table\r\nscience_table %>%\r\n  rename(\r\n    Pair = pair,\r\n    \"Unit Index\" = unit,\r\n    W = w,\r\n    \"Y(0)\" = y_0,\r\n    \"Y(1)\" = y_1,\r\n    \"$\\\\tau$\" = tau,\r\n    \"Y$^{obs}$\" = y\r\n  ) %>%\r\n  kable(align = c(rep(\"c\", 6))) %>%\r\n  kable_styling(position = \"center\")\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nUnit Index\r\n\r\n\r\nW\r\n\r\n\r\nY(0)\r\n\r\n\r\nY(1)\r\n\r\n\r\n\\(\\tau\\)\r\n\r\n\r\nY\\(^{obs}\\)\r\n\r\n\r\nI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n37\r\n\r\n\r\n40\r\n\r\n\r\n3\r\n\r\n\r\n37\r\n\r\n\r\nI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n21\r\n\r\n\r\n24\r\n\r\n\r\n3\r\n\r\n\r\n24\r\n\r\n\r\nII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n36\r\n\r\n\r\n3\r\n\r\n\r\n33\r\n\r\n\r\nII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n22\r\n\r\n\r\n25\r\n\r\n\r\n3\r\n\r\n\r\n25\r\n\r\n\r\nIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n38\r\n\r\n\r\n41\r\n\r\n\r\n3\r\n\r\n\r\n38\r\n\r\n\r\nIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n50\r\n\r\n\r\n53\r\n\r\n\r\n3\r\n\r\n\r\n53\r\n\r\n\r\nIV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n44\r\n\r\n\r\n3\r\n\r\n\r\n41\r\n\r\n\r\nIV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n47\r\n\r\n\r\n50\r\n\r\n\r\n3\r\n\r\n\r\n50\r\n\r\n\r\nV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n44\r\n\r\n\r\n3\r\n\r\n\r\n41\r\n\r\n\r\nV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n56\r\n\r\n\r\n59\r\n\r\n\r\n3\r\n\r\n\r\n59\r\n\r\n\r\nVI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n36\r\n\r\n\r\n3\r\n\r\n\r\n33\r\n\r\n\r\nVI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n40\r\n\r\n\r\n43\r\n\r\n\r\n3\r\n\r\n\r\n43\r\n\r\n\r\nVII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n23\r\n\r\n\r\n26\r\n\r\n\r\n3\r\n\r\n\r\n23\r\n\r\n\r\nVII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n28\r\n\r\n\r\n31\r\n\r\n\r\n3\r\n\r\n\r\n31\r\n\r\n\r\nVIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n30\r\n\r\n\r\n3\r\n\r\n\r\n27\r\n\r\n\r\nVIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\n34\r\n\r\n\r\n3\r\n\r\n\r\n34\r\n\r\n\r\nIX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n30\r\n\r\n\r\n3\r\n\r\n\r\n27\r\n\r\n\r\nIX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n19\r\n\r\n\r\n22\r\n\r\n\r\n3\r\n\r\n\r\n22\r\n\r\n\r\nX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n51\r\n\r\n\r\n54\r\n\r\n\r\n3\r\n\r\n\r\n51\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\n34\r\n\r\n\r\n3\r\n\r\n\r\n34\r\n\r\n\r\nObserved Data\r\nIn reality, we do not have access to the Science Table but the table below where we only have information on the pair indicator, the unit index, the treatment allocated and the observed NO\\(_{2}\\) concentration. Our randomization inference procedure will be based only on this table.\r\n\r\n\r\n# create observed data\r\ndata <- science_table %>%\r\n  select(pair, unit, w, y)\r\n\r\n# display observed data\r\ndata %>%\r\n  rename(Pair = pair, \"Unit Index\" = unit, W = w, \"Y$^{obs}$\" = y) %>%\r\n  kable(align = c(rep(\"c\", 4))) %>%\r\n  kable_styling(position = \"center\")\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nUnit Index\r\n\r\n\r\nW\r\n\r\n\r\nY\\(^{obs}\\)\r\n\r\n\r\nI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n37\r\n\r\n\r\nI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n24\r\n\r\n\r\nII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\nII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n25\r\n\r\n\r\nIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n38\r\n\r\n\r\nIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n53\r\n\r\n\r\nIV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\nIV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n50\r\n\r\n\r\nV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\nV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n59\r\n\r\n\r\nVI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\nVI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n43\r\n\r\n\r\nVII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n23\r\n\r\n\r\nVII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\nVIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\nVIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\nIX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\nIX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n22\r\n\r\n\r\nX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n51\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\nBefore moving to the inference, we need to:\r\nKnow the number of unique treatment allocations. In a pair experiment, there are \\(2^{I}\\) unique treatment allocations, with I is the number of pairs. In this experiment, there are \\(2^{10} = 1024\\) unique treatment allocations.\r\nDefine a test statistic. We will build its distribution under the sharp null hypothesis. Here, we use the average of pair differences as a test statistic.\r\nTesting the Sharp Null Hypothesis of No Treatment\r\nStating the Hypothesis\r\nThe sharp null hypothesis of no treatment states that $ i,j$, \\(H_{0}: Y_{i,j}(0) = Y_{i,j}(1)\\), that is to say the treatment has no effect for each unit. With this assumption, we could impute the missing Y(1) for control units and the missing Y(0) for treated units as shown in the table below :\r\n\r\n\r\n# display imputed observed data\r\ndata %>%\r\n  mutate(\"Y(0)\" = y,\r\n         \"Y(1)\" = y) %>%\r\n  rename(Pair = pair, \"Unit Index\" = unit, W = w, \"Y$^{obs}$\" = y) %>%\r\n  kable(align = c(rep(\"c\", 6))) %>%\r\n  kable_styling(position = \"center\")\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nUnit Index\r\n\r\n\r\nW\r\n\r\n\r\nY\\(^{obs}\\)\r\n\r\n\r\nY(0)\r\n\r\n\r\nY(1)\r\n\r\n\r\nI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n37\r\n\r\n\r\n37\r\n\r\n\r\n37\r\n\r\n\r\nI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n24\r\n\r\n\r\n24\r\n\r\n\r\n24\r\n\r\n\r\nII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\nII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n25\r\n\r\n\r\n25\r\n\r\n\r\n25\r\n\r\n\r\nIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n38\r\n\r\n\r\n38\r\n\r\n\r\n38\r\n\r\n\r\nIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n53\r\n\r\n\r\n53\r\n\r\n\r\n53\r\n\r\n\r\nIV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\nIV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n50\r\n\r\n\r\n50\r\n\r\n\r\n50\r\n\r\n\r\nV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\nV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n59\r\n\r\n\r\n59\r\n\r\n\r\n59\r\n\r\n\r\nVI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\nVI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n43\r\n\r\n\r\n43\r\n\r\n\r\n43\r\n\r\n\r\nVII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n23\r\n\r\n\r\n23\r\n\r\n\r\n23\r\n\r\n\r\nVII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\n31\r\n\r\n\r\n31\r\n\r\n\r\nVIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\nVIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\nIX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\nIX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n22\r\n\r\n\r\n22\r\n\r\n\r\n22\r\n\r\n\r\nX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n51\r\n\r\n\r\n51\r\n\r\n\r\n51\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\nComputional Shortcut\r\nTo create the the distribution of the test statistic under the sharp null hypothesis, we could permute the treatment vector, express for each unit the outcome observed according to the permuted value of the treatment and then compute the average of pair differences. This is a bit cumbersome in terms of programming. In the chapter II of his textbook, Paul Rosenbaum offers a more efficient procedure:\r\nFor each unit i of each pair j, its observed outcome is equal to \\(Y_{i,j} = W_{i,j}\\times Y_{i,j}(1) + (1-W_{i,j})*Y_{i,j}(0)\\).\r\nThe difference in outcomes for the pair i (i.e., the difference in outcomes between the treated and control units) is equal to \\(D_{i} = (W_{i,1} - W_{i,2})(Y_{i,1} - Y_{i,2})\\)\r\nUnder the sharp null hypothesis of no effect, we have \\(Y_{i,j}(0) = Y_{i,j}(1)\\) so that \\(D_{i} = (W_{i,1} - W_{i,2})(Y_{i,1}(0) - Y_{i,2}(0))\\).\r\nIf the treatment allocation within a pair is \\((W_{i,1}, W_{i,2})\\) = (0,1), \\(D_{i} = - (Y_{i,1}(0) - Y_{i,2}(0))\\). If the treatment allocation is \\((W_{i,1}, W_{i,2})\\) = (1,0), \\(D_{i} = Y_{i,1}(0) - Y_{i,2}(0)\\).\r\nTherefore, under the sharp null hypothesis of no effect, the randomization of the treatment only changes the sign of the pair differences in outcomes.\r\nIn terms of programming, we can proceed as follows:\r\nWe first compute the observed average of pair differences. We are now working with a table with 10 pair differences.\r\nWe then compute the permutations matrix of all possible treatment assignments. This is a matrix of 10 rows with 1024 columns.\r\nFor each vector of treatment assignment, we compute the average of pair differences.\r\nComputing the Null Distribution of the Test Statistic\r\nWe compute the observed average of pair differences:\r\n\r\n\r\n# compute the observed average of pair differences\r\naverage_observed_pair_differences <- data %>%\r\n  group_by(pair) %>%\r\n  summarise(pair_difference = y[2] - y[1]) %>%\r\n  ungroup() %>%\r\n  summarise(average_pair_differences = mean(pair_difference)) %>%\r\n  pull(average_pair_differences)\r\n\r\n# display the observed average of pair differences\r\naverage_observed_pair_differences\r\n\r\n\r\n[1] 2.4\r\n\r\nWe have already computed the permutations matrix of all treatment assignments and we load this matrix:\r\n\r\n\r\n# open the matrix of treatment permutations\r\npermutations_matrix <- readRDS(here::here(\"inputs\", \"1.data\", \"3.toy_example\", \"permutations_matrix.rds\"))\r\n\r\n\r\n\r\nWe store the vector of observed pair differences :\r\n\r\n\r\n# store vector of pair differences\r\nobserved_pair_differences <- data %>%\r\n  group_by(pair) %>%\r\n  summarise(pair_difference = y[2] - y[1]) %>%\r\n  ungroup() %>%\r\n  pull(pair_difference)\r\n\r\n\r\n\r\nWe then create a function to compute the randomization distribution of the test statistic:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes as inputs the vector of pair differences and the number of pairs\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution <-\r\n  function(vector_pair_difference, n_pairs) {\r\n    randomization_distribution = NULL\r\n    n_columns = dim(permutations_matrix)[2]\r\n    for (i in 1:n_columns) {\r\n      randomization_distribution[i] =  sum(vector_pair_difference * permutations_matrix[, i]) / n_pairs\r\n    }\r\n    return(randomization_distribution)\r\n  }\r\n\r\n\r\n\r\nWe run the function:\r\n\r\n\r\n# get the distribution of permuted test statistics\r\ndistribution_test_statistics <-\r\n  function_randomization_distribution(vector_pair_difference = observed_pair_differences, n_pairs = 10)\r\n\r\n\r\n\r\nWe plot below the distribution of the test statistic under the sharp null hypothesis:\r\n\r\n\r\n# make the graph\r\ngraph_distribution_test_statistic <-\r\n  tibble(distribution_test_statistics = distribution_test_statistics) %>%\r\n  ggplot(., aes(x = distribution_test_statistics)) +\r\n  geom_histogram(colour = \"white\", fill = my_blue) +\r\n  geom_vline(xintercept = average_observed_pair_differences,\r\n             size = 1.2,\r\n             colour = my_orange) +\r\n  xlab(\"Permuted Test Statistics\") + ylab(\"Counts\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_distribution_test_statistic\r\n\r\n\r\n\r\n# save the graph\r\nggsave(\r\n  graph_distribution_test_statistic,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"3.toy_example\",\r\n    \"distribution_test_statistic_sharp_null.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nComputing the Two-Sided P-Value\r\nTo compute a two-sided p-value, we again follow the explanations provided by Paul Rosenbaum in the chapter II of his textbook:\r\nWe first compute the proportions of permuted test statistics that are lower and higher than the observed test statistic.\r\nWe then double the smallest proportion.\r\nWe take the minimum of its value and one. This give us the two-sided \\(p\\)-value.\r\nWe implement this procedure as follows:\r\n\r\n\r\n# number of permutations\r\nn_permutations <- 1024\r\n\r\n# compute upper proportion\r\nupper_p_value <-\r\n  sum(distribution_test_statistics >= average_observed_pair_differences) /\r\n  n_permutations\r\n\r\n# compute lower proportion\r\nlower_p_value <-\r\n  sum(distribution_test_statistics <= average_observed_pair_differences) /\r\n  n_permutations\r\n\r\n# double the smallest proportion\r\ndouble_smallest_proprotion <- min(c(upper_p_value, lower_p_value)) * 2\r\n\r\n# take the minimum of this proportion and one\r\np_value <- min(double_smallest_proprotion, 1)\r\n\r\n\r\n\r\nThe two-sided p-value for the sharp null hypothesis of no effect is equal to 0.55. We fail to reject the sharp null hypothesis of no effect, despite having simulated a true constant effect of +3 \\(\\mu g/m^{3}\\).\r\nComputing a 95% Fisherian Intervals for a Range of Sharp Null Hypotheses\r\nIn addition to computing the sharp null hypothesis of no effect, we can make the randomization inference procedure more informative by computing also the range of constant effects consistent with the data (i.e., a Fisherian interval). We follow again the explanations provided by Tirthankar Dasguspta and Donald B. Rubin in their forthcoming textbook on experimental design: Experimental Design: A Randomization-Based Perspective.\r\nSteps of the Procedure\r\nInstead of gauging a null effect for all units, we test a set of  sharp null hypotheses \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) = Y\\(_{i,j}\\)(0) + \\(\\tau_{k}\\) for k =1,\\(\\ldots\\),  and where \\(\\tau_{k}\\) represents a constant unit-level treatment effect size.\r\nWe must therefore choose of set of constant treatment effects that we would like to test. Here, we test a set of 81 sharp null hypotheses of constant treatment effects ranging from -20  to +20  with increments of 0.5.\r\nFor each constant treatment effect , we compute the upper -value associated with the hypothesis \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(>\\) \\(\\tau_{k}\\) and the lower -value \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(<\\) \\(\\tau_{k}\\).\r\nTo test each hypothesis, we compute the distribution of the test statistic. The sequence of  hypotheses \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(>\\) \\(\\tau_{k}\\) forms an upper -value function of \\(\\tau\\), \\(p^{+}(\\tau)\\), while the sequence of alternative hypotheses \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(<\\) \\(\\tau_{k}\\) makes a lower -value function, \\(\\tau\\), \\(p^{-}(\\tau)\\). To compute the bounds of the 100(1-\\(\\alpha\\))% Fisherian interval, we solve \\(p^{+}(\\tau) = \\frac{\\alpha}{2}\\) for \\(\\tau\\) to get the lower limit and \\(p^{-}(\\tau) = \\frac{\\alpha}{2}\\) for the upper limit. We set our \\(\\alpha\\) significance level to 0.05 and thus compute 95% Fisherian intervals. This procedure allows us to get the range of  treatment effects consistent with our data.\r\nAs a point estimate of a Fisherian interval, we take the observed value of our test statistic which is the average of pair differences in a pollutant concentration. For avoiding confusion, it is very important to note that our test statistic is an estimate for the individual-level treatment effect of an hypothetical experiment and not for the average treatment effect.\r\nComputational Shortcut\r\nFor each hypothesis, we could impute the missing potential outcomes. Then, we would randomly allocate the treatment, express the observed outcome and finally compute the average of pair differences. Again, this is a cumbersome way to proceed. Instead, we use the computional shortcut provided by Paul Rosenbaum in his textbook.\r\nWe start by making a sharp null hypothesis of a constant treatment effect \\(\\tau\\) such that \\(Y_{i,j}(1) = Y_{i,j}(0) + \\tau\\).\r\nFor a pair i, recall that the observed pair difference in outcomes is \\(D_{i} = (W_{i,1} - W_{i,2})(Y_{i,1} - Y_{i,2})\\).\r\nUnder the sharp hypothesis, we have \\(D_{i} = (W_{i,1} - W_{i,2})((Y_{i,1} + \\tau W_{i,1}) - (Y_{i,2} + \\tau W_{i,2}))\\).\r\nWe rearrange the right-hand side expression and find that \\(D_{i} = \\tau + (W_{i,1} - W_{i,2})(Y_{i,1}(0) - Y_{i,2}(0))\\)\r\nWe have \\(D_{i} - \\tau = (W_{i,1} - W_{i,2})(Y_{i,1}(0) - Y_{i,2}(0))\\). This equation means that the observed pair difference in outcomes minus the hypothesized treatment effect is equal to \\(\\pm(Y_{i,1}(0) - Y_{i,2}(0))\\). We can therefore carry out the randomization inference procedure seen in the previous section from the vector of observed pair differences adjusted for the hypothesized treatment effect.\r\nImplementation in R\r\nWe start by creating a nested tibble of our vector of observed pair differences with the set of constant treatment effect sizes we want to test:\r\n\r\n\r\n# create a nested dataframe with\r\n# the set of constant treatment effect sizes\r\n# and the vector of observed pair differences\r\nri_data_fi <-\r\n  tibble(observed_pair_differences = observed_pair_differences) %>%\r\n  summarise(data_observed_pair_differences = list(observed_pair_differences)) %>%\r\n  group_by(data_observed_pair_differences) %>%\r\n  expand(effect = seq(from = -20, to = 20, by = 0.5)) %>%\r\n  ungroup()\r\n\r\n# display the nested table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 2\r\n   data_observed_pair_differences effect\r\n   <list>                          <dbl>\r\n 1 <dbl [10]>                      -20  \r\n 2 <dbl [10]>                      -19.5\r\n 3 <dbl [10]>                      -19  \r\n 4 <dbl [10]>                      -18.5\r\n 5 <dbl [10]>                      -18  \r\n 6 <dbl [10]>                      -17.5\r\n 7 <dbl [10]>                      -17  \r\n 8 <dbl [10]>                      -16.5\r\n 9 <dbl [10]>                      -16  \r\n10 <dbl [10]>                      -15.5\r\n# ... with 71 more rows\r\n\r\nWe then subtract for each pair difference the hypothetical constant effect:\r\n\r\n\r\n# function to get the observed statistic\r\nadjusted_pair_difference_function <-\r\n  function(data_observed_pair_differences, effect) {\r\n    adjusted_pair_difference <- data_observed_pair_differences - effect\r\n    return(adjusted_pair_difference)\r\n  }\r\n\r\n# compute the adjusted pair differences\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    data_adjusted_pair_difference = map2(\r\n      data_observed_pair_differences,\r\n      effect,\r\n      ~ adjusted_pair_difference_function(.x, .y)\r\n    )\r\n  )\r\n\r\n# display the table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 3\r\n   data_observed_pair_differences effect data_adjusted_pair_difference\r\n   <list>                          <dbl> <list>                       \r\n 1 <dbl [10]>                      -20   <dbl [10]>                   \r\n 2 <dbl [10]>                      -19.5 <dbl [10]>                   \r\n 3 <dbl [10]>                      -19   <dbl [10]>                   \r\n 4 <dbl [10]>                      -18.5 <dbl [10]>                   \r\n 5 <dbl [10]>                      -18   <dbl [10]>                   \r\n 6 <dbl [10]>                      -17.5 <dbl [10]>                   \r\n 7 <dbl [10]>                      -17   <dbl [10]>                   \r\n 8 <dbl [10]>                      -16.5 <dbl [10]>                   \r\n 9 <dbl [10]>                      -16   <dbl [10]>                   \r\n10 <dbl [10]>                      -15.5 <dbl [10]>                   \r\n# ... with 71 more rows\r\n\r\nWe compute the observed mean of adjusted pair differences:\r\n\r\n\r\n# compute the observed mean of adjusted pair differences\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(observed_mean_difference = map(data_adjusted_pair_difference, ~ mean(.))) %>%\r\n  unnest(cols = c(observed_mean_difference)) %>%\r\n  select(-data_observed_pair_differences) %>%\r\n  ungroup()\r\n\r\n# display the table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 3\r\n   effect data_adjusted_pair_difference observed_mean_difference\r\n    <dbl> <list>                                           <dbl>\r\n 1  -20   <dbl [10]>                                        22.4\r\n 2  -19.5 <dbl [10]>                                        21.9\r\n 3  -19   <dbl [10]>                                        21.4\r\n 4  -18.5 <dbl [10]>                                        20.9\r\n 5  -18   <dbl [10]>                                        20.4\r\n 6  -17.5 <dbl [10]>                                        19.9\r\n 7  -17   <dbl [10]>                                        19.4\r\n 8  -16.5 <dbl [10]>                                        18.9\r\n 9  -16   <dbl [10]>                                        18.4\r\n10  -15.5 <dbl [10]>                                        17.9\r\n# ... with 71 more rows\r\n\r\nWe use the same function_randomization_distribution() to compute the randomization distribution of the test statistic for each hypothesized constant effect:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes as inputs the vector of pair differences and the number of pairs\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution <-\r\n  function(data_adjusted_pair_difference, n_pairs) {\r\n    randomization_distribution = NULL\r\n    n_columns = dim(permutations_matrix)[2]\r\n    for (i in 1:n_columns) {\r\n      randomization_distribution[i] =  sum(data_adjusted_pair_difference * permutations_matrix[, i]) / n_pairs\r\n    }\r\n    return(randomization_distribution)\r\n  }\r\n\r\n\r\n\r\nWe run the function:\r\n\r\n\r\n# compute the test statistic distribution\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    randomization_distribution = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_randomization_distribution(., n_pairs = 10)\r\n    )\r\n  )\r\n\r\n# display the table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 4\r\n   effect data_adjusted_pair_d~ observed_mean_dif~ randomization_dist~\r\n    <dbl> <list>                             <dbl> <list>             \r\n 1  -20   <dbl [10]>                          22.4 <dbl [1,024]>      \r\n 2  -19.5 <dbl [10]>                          21.9 <dbl [1,024]>      \r\n 3  -19   <dbl [10]>                          21.4 <dbl [1,024]>      \r\n 4  -18.5 <dbl [10]>                          20.9 <dbl [1,024]>      \r\n 5  -18   <dbl [10]>                          20.4 <dbl [1,024]>      \r\n 6  -17.5 <dbl [10]>                          19.9 <dbl [1,024]>      \r\n 7  -17   <dbl [10]>                          19.4 <dbl [1,024]>      \r\n 8  -16.5 <dbl [10]>                          18.9 <dbl [1,024]>      \r\n 9  -16   <dbl [10]>                          18.4 <dbl [1,024]>      \r\n10  -15.5 <dbl [10]>                          17.9 <dbl [1,024]>      \r\n# ... with 71 more rows\r\n\r\nWe compute the lower and upper p-values functions:\r\n\r\n\r\n# define the p-values functions\r\nfunction_fisher_upper_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution >= observed_mean_difference) / n_permutations\r\n  }\r\n\r\nfunction_fisher_lower_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution <= observed_mean_difference) / n_permutations\r\n  }\r\n\r\n# compute the lower and upper one-sided p-values\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    p_value_upper = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_upper_p_value(.x, .y)\r\n    ),\r\n    p_value_lower = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_lower_p_value(.x, .y)\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe plot below the lower and upper p-values functions:\r\n\r\n\r\n# make the graph\r\ngraph_p_value_functions_sharp_nulls <- ri_data_fi %>%\r\n  select(effect, p_value_upper, p_value_lower) %>%\r\n  rename(\"Upper p-value Function\" = p_value_upper,\r\n         \"Lower p-value Function\" = p_value_lower) %>%\r\n  pivot_longer(cols = -c(effect),\r\n               names_to = \"lower_upper\",\r\n               values_to = \"p_value\") %>%\r\n  ggplot(., aes(x = effect, y = p_value)) +\r\n  geom_hline(yintercept = 0.025, colour = my_orange) +\r\n  geom_line(colour = my_blue, size = 1.2) +\r\n  facet_wrap( ~ fct_rev(lower_upper)) +\r\n  xlab(\"Hypothetical Constant Treatment Effects\") + ylab(\"p-value\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_p_value_functions_sharp_nulls\r\n\r\n\r\n\r\n# save the graph\r\nggsave(\r\n  graph_p_value_functions_sharp_nulls,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"3.toy_example\",\r\n    \"graph_p_value_functions_sharp_nulls.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nThe orange line represents the alpha significance level, set at 5%, divided by two. We then retrieve the lower and upper bound of the 95% Fisherian interval:\r\n\r\n\r\n# retrieve the constant effects with the p-values equal or the closest to 0.025\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    p_value_upper = abs(p_value_upper - 0.025),\r\n    p_value_lower = abs(p_value_lower - 0.025)\r\n  ) %>%\r\n  filter(p_value_upper == min(p_value_upper) |\r\n           p_value_lower == min(p_value_lower)) %>%\r\n  # in case two effect sizes have a p-value equal to 0.025, we take the effect size\r\n  # that make the Fisherian interval wider to be conservative\r\n  summarise(fi_lower_95 = min(effect),\r\n            fi_upper_95 = max(effect))\r\n\r\n\r\n\r\nAs a point estimate, we take the value of the observed average of pair differences, that is to say 2.4. For this imaginary experiment, our point estimate is close to the true constant effect but the 95% Fisherian interval is wide: [-7, 11.5]. The data are consistent with both large negative and positive constant treatment effects.\r\nComparison with Neyman’s Approach\r\nWe can compare the result of the randomization inference procedure with the one we would obtain with Neyman’s approach. In that case, the inference procedure is built to target the average causal effect and the source of inference is both the randomization of the treatment and the sampling from a population. We can estimate the finite sample average effect, \\(\\tau_{\\text{fs}}\\), with the average of observed pair differences \\(\\hat{\\tau}\\):\r\n\\[\\begin{equation*}\r\n  \\hat{\\tau} = \\frac{1}{I}\\sum_{i=1}^J(Y^{\\text{obs}}_{\\text{t},i}-Y^{\\text{obs}}_{\\text{c},i}) = \\overline{Y}^{\\text{obs}}_{\\text{t}} - \\overline{Y}^{\\text{obs}}_{\\text{c}}\r\n\\end{equation*}\\]\r\nHere, the subscripts \\(t\\) and \\(c\\) respectively indicate if the unit in a given pair is treated or not. \\(I\\) is the number of pairs. Since there are only one treated and one control unit within each pair, the standard estimate for the sampling variance of the average of pair differences is not defined. We can however compute a conservative estimate of the variance, as explained in chapter 10 of Imbens and Rubin (2015):\r\n\\[\\begin{equation*}\r\n  \\hat{\\mathbb{V}}(\\hat{\\tau}) = \\frac{1}{I(I-1)}\\sum_{I=1}^I(Y^{\\text{obs}}_{\\text{t},i}-Y^{\\text{obs}}_{\\text{c},i} - \\hat{\\tau})^{2}\r\n\\end{equation*}\\]\r\nWe finally compute an asymptotic 95% confidence interval using a Gaussian distribution approximation:\r\n\\[\\begin{equation*}\r\n\\text{CI}_{0.95}(\\tau_{\\text{fs}}) =\\Big( \\hat{\\tau} - 1.96\\times \\sqrt{\\hat{\\mathbb{V}}(\\hat{\\tau})},\\; \\hat{\\tau} + 1.96\\times \\sqrt{\\hat{\\mathbb{V}}(\\hat{\\tau})}\\Big)\r\n\\end{equation*}\\]\r\nAs in the example of Imbens and Rubin (2015), we only have here 10 pairs. To build the 95% confidence interval, we use a \\(t\\)-distribution with degrees of freedom equal to I/2-1=9. The 0.975 quantile is equal to 2.262. We compute the 95% confidence interval with the following code:\r\n\r\n\r\n# we store the number of pairs\r\nn_pairs <- 10\r\n\r\n# compute the standard error\r\nsquared_difference <-\r\n  (observed_pair_differences - average_observed_pair_differences) ^ 2\r\n\r\n# compute the standard error\r\nstandard_error <-\r\n  sqrt(1 / (n_pairs * (n_pairs - 1)) * sum(squared_difference))\r\n\r\n# compute the standard error\r\nci_lower_95 <-\r\n  average_observed_pair_differences - qt(0.975, 9) * standard_error\r\nci_upper_95 <-\r\n  average_observed_pair_differences + qt(0.975, 9) * standard_error\r\n\r\n# store results\r\nneyman_data_ci <-\r\n  tibble(ci_lower_95 = ci_lower_95, ci_upper_95 = ci_upper_95)\r\n\r\n\r\n\r\nThe 95% confidence interval is equal to [-6.303999, 11.103999], which is very similar to the one found with randomization inference. However, the interpretation of the two intervals is different: in the randomization inference procedure,\r\nComputing a 95% Fisherian Intervals For Weak Null Hypotheses\r\nFinally, many researchers restrain from using randomization inference as a mode of inference since it assumes that treatment effects are constant across units. In most applications, this is arguably an unrealistic assumption. To overcome this limit, Jason Wu & Peng Ding (2021) propose to adopt a studentized test statistic that is finite-sample exact under sharp null hypotheses but also asymptotically conservative for the weak null hypothesis.\r\nIn the case of our toy example, this studentized test statistic is equal to the observed average of pair differences divided by the standard error of a pairwise experiment. We therefore just follow the same previous procedure but use the studentized statistic proposed by Jason Wu & Peng Ding (2021).\r\nFirst, we create the data for testing a range of weak null hypotheses with the randomization inference:\r\n\r\n\r\n# create a nested dataframe with \r\n# the set of constant treatment effect sizes\r\n# and the vector of observed pair differences\r\nri_data_fi_weak <- tibble(observed_pair_differences = observed_pair_differences) %>%\r\n  summarise(data_observed_pair_differences = list(observed_pair_differences)) %>%\r\n  group_by(data_observed_pair_differences) %>%\r\n  expand(effect = seq(from = -20, to = 20, by = 0.5)) %>%\r\n  ungroup()\r\n\r\n# display the nested table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 2\r\n   data_observed_pair_differences effect\r\n   <list>                          <dbl>\r\n 1 <dbl [10]>                      -20  \r\n 2 <dbl [10]>                      -19.5\r\n 3 <dbl [10]>                      -19  \r\n 4 <dbl [10]>                      -18.5\r\n 5 <dbl [10]>                      -18  \r\n 6 <dbl [10]>                      -17.5\r\n 7 <dbl [10]>                      -17  \r\n 8 <dbl [10]>                      -16.5\r\n 9 <dbl [10]>                      -16  \r\n10 <dbl [10]>                      -15.5\r\n# ... with 71 more rows\r\n\r\nWe then subtract for each pair difference the hypothetical constant effect using the previously used adjusted_pair_difference_function():\r\n\r\n\r\n# compute the adjusted pair differences\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(data_adjusted_pair_difference = map2(data_observed_pair_differences, effect, ~ adjusted_pair_difference_function(.x, .y)))\r\n\r\n# display the table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 3\r\n   data_observed_pair_differences effect data_adjusted_pair_difference\r\n   <list>                          <dbl> <list>                       \r\n 1 <dbl [10]>                      -20   <dbl [10]>                   \r\n 2 <dbl [10]>                      -19.5 <dbl [10]>                   \r\n 3 <dbl [10]>                      -19   <dbl [10]>                   \r\n 4 <dbl [10]>                      -18.5 <dbl [10]>                   \r\n 5 <dbl [10]>                      -18   <dbl [10]>                   \r\n 6 <dbl [10]>                      -17.5 <dbl [10]>                   \r\n 7 <dbl [10]>                      -17   <dbl [10]>                   \r\n 8 <dbl [10]>                      -16.5 <dbl [10]>                   \r\n 9 <dbl [10]>                      -16   <dbl [10]>                   \r\n10 <dbl [10]>                      -15.5 <dbl [10]>                   \r\n# ... with 71 more rows\r\n\r\nWe then compute the observed studentized statistics:\r\n\r\n\r\n# function to compute neyman t-statistic\r\nfunction_neyman_t_stat <- function(pair_differences, n_pairs) {\r\n  # compute the average of pair differences\r\n  average_pair_difference <- mean(pair_differences)\r\n  # compute the standard error\r\n  squared_difference <-\r\n    (pair_differences - average_pair_difference) ^ 2\r\n  # compute the standard error\r\n  standard_error <-\r\n    sqrt(1 / (n_pairs * (n_pairs - 1)) * sum(squared_difference))\r\n  # compute neyman t-statistic\r\n  neyman_t_stat <- average_pair_difference / standard_error\r\n  return(neyman_t_stat)\r\n}\r\n\r\n\r\n# compute the observed mean of adjusted pair differences\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    observed_neyman_t_stat = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_neyman_t_stat(., n_pairs = 10)\r\n    )\r\n  ) %>%\r\n  unnest(cols = c(observed_neyman_t_stat)) %>%\r\n  select(-data_observed_pair_differences) %>%\r\n  ungroup()\r\n\r\n# display the table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 3\r\n   effect data_adjusted_pair_difference observed_neyman_t_stat\r\n    <dbl> <list>                                         <dbl>\r\n 1  -20   <dbl [10]>                                      5.82\r\n 2  -19.5 <dbl [10]>                                      5.69\r\n 3  -19   <dbl [10]>                                      5.56\r\n 4  -18.5 <dbl [10]>                                      5.43\r\n 5  -18   <dbl [10]>                                      5.30\r\n 6  -17.5 <dbl [10]>                                      5.17\r\n 7  -17   <dbl [10]>                                      5.04\r\n 8  -16.5 <dbl [10]>                                      4.91\r\n 9  -16   <dbl [10]>                                      4.78\r\n10  -15.5 <dbl [10]>                                      4.65\r\n# ... with 71 more rows\r\n\r\nWe create a function to carry out the randomization inference with the studentized test statistic:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes the vector of pair differences\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution_t_stat <-\r\n  function(data_adjusted_pair_difference, n_pairs) {\r\n    randomization_distribution = NULL\r\n    n_columns = dim(permutations_matrix)[2]\r\n    for (i in 1:n_columns) {\r\n      # compute the average of pair differences\r\n      average_pair_difference <-\r\n        sum(data_adjusted_pair_difference * permutations_matrix[, i]) / n_pairs\r\n      # compute the standard error\r\n      squared_difference <-\r\n        (data_adjusted_pair_difference - average_pair_difference) ^ 2\r\n      # compute the standard error\r\n      standard_error <-\r\n        sqrt(1 / (n_pairs * (n_pairs - 1)) * sum(squared_difference))\r\n      # compute neyman t-statistic\r\n      randomization_distribution[i] = average_pair_difference / standard_error\r\n    }\r\n    return(randomization_distribution)\r\n  }\r\n\r\n\r\n\r\nWe run the function:\r\n\r\n\r\n# compute the test statistic distribution\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    randomization_distribution = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_randomization_distribution_t_stat(., n_pairs = 10)\r\n    )\r\n  )\r\n\r\n# display the table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 4\r\n   effect data_adjusted_pair_d~ observed_neyman_t~ randomization_dist~\r\n    <dbl> <list>                             <dbl> <list>             \r\n 1  -20   <dbl [10]>                          5.82 <dbl [1,024]>      \r\n 2  -19.5 <dbl [10]>                          5.69 <dbl [1,024]>      \r\n 3  -19   <dbl [10]>                          5.56 <dbl [1,024]>      \r\n 4  -18.5 <dbl [10]>                          5.43 <dbl [1,024]>      \r\n 5  -18   <dbl [10]>                          5.30 <dbl [1,024]>      \r\n 6  -17.5 <dbl [10]>                          5.17 <dbl [1,024]>      \r\n 7  -17   <dbl [10]>                          5.04 <dbl [1,024]>      \r\n 8  -16.5 <dbl [10]>                          4.91 <dbl [1,024]>      \r\n 9  -16   <dbl [10]>                          4.78 <dbl [1,024]>      \r\n10  -15.5 <dbl [10]>                          4.65 <dbl [1,024]>      \r\n# ... with 71 more rows\r\n\r\nWe compute the lower and upper p-values functions:\r\n\r\n\r\n# define the p-values functions\r\nfunction_fisher_upper_p_value <-\r\n  function(observed_neyman_t_stat,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution >= observed_neyman_t_stat) / n_permutations\r\n  }\r\n\r\nfunction_fisher_lower_p_value <-\r\n  function(observed_neyman_t_stat,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution <= observed_neyman_t_stat) / n_permutations\r\n  }\r\n\r\n# compute the lower and upper one-sided p-values\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    p_value_upper = map2_dbl(\r\n      observed_neyman_t_stat,\r\n      randomization_distribution,\r\n      ~ function_fisher_upper_p_value(.x, .y)\r\n    ),\r\n    p_value_lower = map2_dbl(\r\n      observed_neyman_t_stat,\r\n      randomization_distribution,\r\n      ~ function_fisher_lower_p_value(.x, .y)\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe plot below the lower and upper p-values functions:\r\n\r\n\r\n# make the graph\r\ngraph_p_value_functions_weak_nulls <- ri_data_fi_weak %>%\r\n  select(effect, p_value_upper, p_value_lower) %>%\r\n  rename(\"Upper p-value Function\" = p_value_upper, \"Lower p-value Function\" = p_value_lower) %>%\r\n  pivot_longer(cols = -c(effect), names_to = \"lower_upper\", values_to = \"p_value\") %>%\r\n  ggplot(., aes(x = effect, y = p_value)) +\r\n  geom_hline(yintercept = 0.025, colour = my_orange) +\r\n  geom_line(colour = my_blue, size = 1.2) +\r\n  facet_wrap(~ fct_rev(lower_upper)) +\r\n  xlab(\"Hypothetical Constant Treatment Effects\") + ylab(\"p-value\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_p_value_functions_weak_nulls\r\n\r\n\r\n\r\n# save the graph\r\nggsave(graph_p_value_functions_weak_nulls, filename = here::here(\"inputs\", \"3.outputs\", \"3.toy_example\", \"graph_p_value_functions_weak_nulls.pdf\"), \r\n       width = 20, height = 10, units = \"cm\", device = cairo_pdf)\r\n\r\n\r\n\r\nThe orange line represents the alpha significance level, set at 5%, divided by two. We then retrieve the lower and upper bound of the 95% Fisherian interval:\r\n\r\n\r\n# retrieve the constant effects with the p-values equal or the closest to 0.025\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    p_value_upper = abs(p_value_upper - 0.025),\r\n    p_value_lower = abs(p_value_lower - 0.025)\r\n  ) %>%\r\n  filter(p_value_upper == min(p_value_upper) |\r\n           p_value_lower == min(p_value_lower)) %>%\r\n  # in case two effect sizes have a p-value equal to 0.025, we take the effect size\r\n  # that make the Fisherian interval wider to be conservative\r\n  summarise(fi_lower_95 = min(effect),\r\n            fi_upper_95 = max(effect))\r\n\r\n\r\n\r\nThe 95% Fisherian interval is equal to [-7, 11.5]. It is the same interval found with the randomization inference procedure based on average of pair differences test statistic. It is also very similar to the interval found with Neyman’s approach.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-19T16:15:09+02:00"
    },
    {
      "path": "index.html",
      "title": "Estimating the Local Air Pollution Impacts of Cruise Traffic",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Estimating the Local Air Pollution Impacts of Cruise Traffic\r\n          \r\n          \r\n          Home\r\n          Article\r\n          \r\n          \r\n          Data\r\n           \r\n          ▾\r\n          \r\n          \r\n          Daily Data Wrangling\r\n          \r\n          \r\n          \r\n          \r\n          Daily Matching\r\n           \r\n          ▾\r\n          \r\n          \r\n          Daily Data Procedure\r\n          \r\n          \r\n          Tutorial Randomization Inference\r\n          \r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Estimating the Local Air Pollution Impacts of Cruise Traffic\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                        \r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        \r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n\r\n          \r\n            Hello and welcome!\r\n            This website gathers all the materials for the paper Estimating the Local Air Pollution Impacts of Cruise Traffic: A Principled Approach for Observational Data by Léo Zabrocki, Marion Leroutier and Marie-Abèle Bind.\r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Estimating the Local Air Pollution Impacts of Cruise Traffic\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              Hello and welcome!\r\n              This website gathers all the materials for the paper Estimating the Local Air Pollution Impacts of Cruise Traffic: A Principled Approach for Observational Data by Léo Zabrocki, Marion Leroutier and Marie-Abèle Bind.\r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2022-04-19T16:15:09+02:00"
    }
  ],
  "collections": []
}
