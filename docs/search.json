{
  "articles": [
    {
      "path": "3_toy_example_randomization_inference.html",
      "title": "Toy Example for Understanding Randomization Inference",
      "description": "How to Compute Fisherian Intervals?\n",
      "author": [
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Marion Leroutier",
          "url": "https://www.parisschoolofeconomics.eu/en/leroutier-marion/work-in-progress/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nToy Example\r\nScience Table\r\nObserved Data\r\n\r\nTesting the Sharp Null Hypothesis of No Treatment\r\nStating the Hypothesis\r\nComputional Shortcut\r\nComputing the Null Distribution of the Test Statistic\r\nComputing the Two-Sided P-Value\r\n\r\nComputing a 95% Fisherian Intervals for a Range of Sharp Null Hypotheses\r\nSteps of the Procedure\r\nComputational Shortcut\r\nImplementation in R\r\n\r\nComparison with Neyman’s Approach\r\nComputing a 95% Fisherian Intervals For Weak Null Hypotheses\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we explain with a toy example how to:\r\nTest the sharp null hypothesis of no effect for all units.\r\nCarry out a test inversion procedure to compute a 95% Fisherian interval for constant treatment effects consistent with the data.\r\nCompare the results with Neyman’s approach targeting the average causal effect.\r\nAdapt the randomization inference for testing weak null hypotheses (i.e., average causal effects).\r\nShould you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu and marion.leroutier@hhs.se.\r\nRequired Packages\r\nTo reproduce the 3_toy_example_randomization_inference.html document, we need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the 3_toy_example_randomization_inference.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(kableExtra) # for building nice tables\r\nlibrary(Cairo) # for printing custom police of graphs\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\r\n  \"inputs\",\r\n  \"2.functions\",\r\n  \"script_theme_tufte.R\"\r\n))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nToy Example\r\nIn this toy example, we aim to estimate the effect of cruise vessels docking at Marseille’s port on NO\\(_{2}\\) concentration:\r\nFor simplicity, imagine that our matching procedure resulted in 10 pairs of hours with similar weather and calendar characteristics (i.e., same average temperature, same day of the week, etc…).\r\nTreated units are hours with cruise vessels docking at the port while control units are hours without cruise vessels.\r\nThe outcome of the experiment is the hourly NO\\(_{2}\\) concentration measured at a station located in the city.\r\nThe exposition of this toy example is inspired by those found in the chapter II of Paul Rosenbaum’s textbook and Tirthankar Dasgupta and Donald B. Rubin’s forthcoming textbook (Experimental Design: A Randomization-Based Perspective).\r\nScience Table\r\nWe display below the Science Table of our imaginary experiment, that is to say the table showing the treatment status and potential outcomes for each observation:\r\nThe first column Pair is the indicator of the pair. We represent the index of a pair by i which takes values from 1 to 5.\r\nThe second column Unit Index is the index j of a unit within the pair (j is equal to 1 for the first unit in the pair and to 2 for the second unit).\r\nThe third column W indicates the treatment allocation. W = 1 for treated units and W = 0 for controls.\r\nThe fourth and fifth columns are the potential outcomes of each unit and represent the NO\\(_{2}\\) concentrations measured in \\(\\mu g/m^{3}\\). Y(W = 0) is the potential outcome when the unit does not receive the treatment and Y(W = 1) is the potential outcome when the unit is treated. As this is an artificial example, we imagine that we know for each unit the values of both potential outcomes.\r\nThe six column \\(\\tau\\) is the unit constant causal effect. Here, the causal effect is equal to +3 \\(\\mu g/m^{3}\\).\r\nThe last column Y\\(^{obs}\\) represents the potential outcome that we would observe according to the treatment allocation, that is to say \\(Y_{i,j} = W_{i,j}\\times Y_{i,j}(1) + (1-W_{i,j})\\times Y_{i,j}(0)\\). Here, in each pair, the first unit does not receive the treatment so that we observe Y(0) while the second unit is treated and we observe Y(1).\r\n\r\n\r\n# load the science table\r\nscience_table <- readRDS(here::here(\"inputs\", \"1.data\", \"3.toy_example\", \"science_table.RDS\"))\r\n\r\n# display the table\r\nscience_table %>%\r\n  rename(\r\n    Pair = pair,\r\n    \"Unit Index\" = unit,\r\n    W = w,\r\n    \"Y(0)\" = y_0,\r\n    \"Y(1)\" = y_1,\r\n    \"$\\\\tau$\" = tau,\r\n    \"Y$^{obs}$\" = y\r\n  ) %>%\r\n  kable(., align = c(rep(\"c\", 6)))\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nUnit Index\r\n\r\n\r\nW\r\n\r\n\r\nY(0)\r\n\r\n\r\nY(1)\r\n\r\n\r\n\\(\\tau\\)\r\n\r\n\r\nY\\(^{obs}\\)\r\n\r\n\r\nI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n37\r\n\r\n\r\n40\r\n\r\n\r\n3\r\n\r\n\r\n37\r\n\r\n\r\nI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n21\r\n\r\n\r\n24\r\n\r\n\r\n3\r\n\r\n\r\n24\r\n\r\n\r\nII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n36\r\n\r\n\r\n3\r\n\r\n\r\n33\r\n\r\n\r\nII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n22\r\n\r\n\r\n25\r\n\r\n\r\n3\r\n\r\n\r\n25\r\n\r\n\r\nIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n38\r\n\r\n\r\n41\r\n\r\n\r\n3\r\n\r\n\r\n38\r\n\r\n\r\nIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n50\r\n\r\n\r\n53\r\n\r\n\r\n3\r\n\r\n\r\n53\r\n\r\n\r\nIV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n44\r\n\r\n\r\n3\r\n\r\n\r\n41\r\n\r\n\r\nIV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n47\r\n\r\n\r\n50\r\n\r\n\r\n3\r\n\r\n\r\n50\r\n\r\n\r\nV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n44\r\n\r\n\r\n3\r\n\r\n\r\n41\r\n\r\n\r\nV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n56\r\n\r\n\r\n59\r\n\r\n\r\n3\r\n\r\n\r\n59\r\n\r\n\r\nVI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n36\r\n\r\n\r\n3\r\n\r\n\r\n33\r\n\r\n\r\nVI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n40\r\n\r\n\r\n43\r\n\r\n\r\n3\r\n\r\n\r\n43\r\n\r\n\r\nVII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n23\r\n\r\n\r\n26\r\n\r\n\r\n3\r\n\r\n\r\n23\r\n\r\n\r\nVII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n28\r\n\r\n\r\n31\r\n\r\n\r\n3\r\n\r\n\r\n31\r\n\r\n\r\nVIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n30\r\n\r\n\r\n3\r\n\r\n\r\n27\r\n\r\n\r\nVIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\n34\r\n\r\n\r\n3\r\n\r\n\r\n34\r\n\r\n\r\nIX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n30\r\n\r\n\r\n3\r\n\r\n\r\n27\r\n\r\n\r\nIX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n19\r\n\r\n\r\n22\r\n\r\n\r\n3\r\n\r\n\r\n22\r\n\r\n\r\nX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n51\r\n\r\n\r\n54\r\n\r\n\r\n3\r\n\r\n\r\n51\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\n34\r\n\r\n\r\n3\r\n\r\n\r\n34\r\n\r\n\r\nObserved Data\r\nIn reality, we do not have access to the Science Table but the table below where we only have information on the pair indicator, the unit index, the treatment allocated and the observed NO\\(_{2}\\) concentration. Our randomization inference procedure will be based only on this table.\r\n\r\n\r\n# create observed data\r\ndata <- science_table %>%\r\n  select(pair, unit, w, y)\r\n\r\n# display observed data\r\ndata %>%\r\n  rename(Pair = pair, \"Unit Index\" = unit, W = w, \"Y$^{obs}$\" = y) %>%\r\n  kable(align = c(rep(\"c\", 4)))\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nUnit Index\r\n\r\n\r\nW\r\n\r\n\r\nY\\(^{obs}\\)\r\n\r\n\r\nI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n37\r\n\r\n\r\nI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n24\r\n\r\n\r\nII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\nII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n25\r\n\r\n\r\nIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n38\r\n\r\n\r\nIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n53\r\n\r\n\r\nIV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\nIV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n50\r\n\r\n\r\nV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\nV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n59\r\n\r\n\r\nVI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\nVI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n43\r\n\r\n\r\nVII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n23\r\n\r\n\r\nVII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\nVIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\nVIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\nIX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\nIX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n22\r\n\r\n\r\nX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n51\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\nBefore moving to the inference, we need to:\r\nKnow the number of unique treatment allocations. In a pair experiment, there are \\(2^{I}\\) unique treatment allocations, with I is the number of pairs. In this experiment, there are \\(2^{10} = 1024\\) unique treatment allocations.\r\nDefine a test statistic. We will build its distribution under the sharp null hypothesis. Here, we use the average of pair differences as a test statistic.\r\nTesting the Sharp Null Hypothesis of No Treatment\r\nStating the Hypothesis\r\nThe sharp null hypothesis of no treatment states that $ i,j$, \\(H_{0}: Y_{i,j}(0) = Y_{i,j}(1)\\), that is to say the treatment has no effect for each unit. With this assumption, we could impute the missing Y(1) for control units and the missing Y(0) for treated units as shown in the table below :\r\n\r\n\r\n# display imputed observed data\r\ndata %>%\r\n  mutate(\"Y(0)\" = y,\r\n         \"Y(1)\" = y) %>%\r\n  rename(Pair = pair, \"Unit Index\" = unit, W = w, \"Y$^{obs}$\" = y) %>%\r\n  kable(align = c(rep(\"c\", 6)))\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nUnit Index\r\n\r\n\r\nW\r\n\r\n\r\nY\\(^{obs}\\)\r\n\r\n\r\nY(0)\r\n\r\n\r\nY(1)\r\n\r\n\r\nI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n37\r\n\r\n\r\n37\r\n\r\n\r\n37\r\n\r\n\r\nI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n24\r\n\r\n\r\n24\r\n\r\n\r\n24\r\n\r\n\r\nII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\nII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n25\r\n\r\n\r\n25\r\n\r\n\r\n25\r\n\r\n\r\nIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n38\r\n\r\n\r\n38\r\n\r\n\r\n38\r\n\r\n\r\nIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n53\r\n\r\n\r\n53\r\n\r\n\r\n53\r\n\r\n\r\nIV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\nIV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n50\r\n\r\n\r\n50\r\n\r\n\r\n50\r\n\r\n\r\nV\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\n41\r\n\r\n\r\nV\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n59\r\n\r\n\r\n59\r\n\r\n\r\n59\r\n\r\n\r\nVI\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\n33\r\n\r\n\r\nVI\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n43\r\n\r\n\r\n43\r\n\r\n\r\n43\r\n\r\n\r\nVII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n23\r\n\r\n\r\n23\r\n\r\n\r\n23\r\n\r\n\r\nVII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n31\r\n\r\n\r\n31\r\n\r\n\r\n31\r\n\r\n\r\nVIII\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\nVIII\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\nIX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\n27\r\n\r\n\r\nIX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n22\r\n\r\n\r\n22\r\n\r\n\r\n22\r\n\r\n\r\nX\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n51\r\n\r\n\r\n51\r\n\r\n\r\n51\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\n34\r\n\r\n\r\nComputional Shortcut\r\nTo create the the distribution of the test statistic under the sharp null hypothesis, we could permute the treatment vector, express for each unit the outcome observed according to the permuted value of the treatment and then compute the average of pair differences. This is a bit cumbersome in terms of programming. In the chapter II of his textbook, Paul Rosenbaum offers a more efficient procedure:\r\nFor each unit i of each pair j, its observed outcome is equal to \\(Y_{i,j} = W_{i,j}\\times Y_{i,j}(1) + (1-W_{i,j})*Y_{i,j}(0)\\).\r\nThe difference in outcomes for the pair i (i.e., the difference in outcomes between the treated and control units) is equal to \\(D_{i} = (W_{i,1} - W_{i,2})(Y_{i,1} - Y_{i,2})\\)\r\nUnder the sharp null hypothesis of no effect, we have \\(Y_{i,j}(0) = Y_{i,j}(1)\\) so that \\(D_{i} = (W_{i,1} - W_{i,2})(Y_{i,1}(0) - Y_{i,2}(0))\\).\r\nIf the treatment allocation within a pair is \\((W_{i,1}, W_{i,2})\\) = (0,1), \\(D_{i} = - (Y_{i,1}(0) - Y_{i,2}(0))\\). If the treatment allocation is \\((W_{i,1}, W_{i,2})\\) = (1,0), \\(D_{i} = Y_{i,1}(0) - Y_{i,2}(0)\\).\r\nTherefore, under the sharp null hypothesis of no effect, the randomization of the treatment only changes the sign of the pair differences in outcomes.\r\nIn terms of programming, we can proceed as follows:\r\nWe first compute the observed average of pair differences. We are now working with a table with 10 pair differences.\r\nWe then compute the permutations matrix of all possible treatment assignments. This is a matrix of 10 rows with 1024 columns.\r\nFor each vector of treatment assignment, we compute the average of pair differences.\r\nComputing the Null Distribution of the Test Statistic\r\nWe compute the observed average of pair differences:\r\n\r\n\r\n# compute the observed average of pair differences\r\naverage_observed_pair_differences <- data %>%\r\n  group_by(pair) %>%\r\n  summarise(pair_difference = y[2] - y[1]) %>%\r\n  ungroup() %>%\r\n  summarise(average_pair_differences = mean(pair_difference)) %>%\r\n  pull(average_pair_differences)\r\n\r\n# display the observed average of pair differences\r\naverage_observed_pair_differences\r\n\r\n\r\n[1] 2.4\r\n\r\nWe have already computed the permutations matrix of all treatment assignments and we load this matrix:\r\n\r\n\r\n# open the matrix of treatment permutations\r\npermutations_matrix <- readRDS(here::here(\"inputs\", \"1.data\", \"3.toy_example\", \"permutations_matrix.rds\"))\r\n\r\n\r\n\r\nWe store the vector of observed pair differences :\r\n\r\n\r\n# store vector of pair differences\r\nobserved_pair_differences <- data %>%\r\n  group_by(pair) %>%\r\n  summarise(pair_difference = y[2] - y[1]) %>%\r\n  ungroup() %>%\r\n  pull(pair_difference)\r\n\r\n\r\n\r\nWe then create a function to compute the randomization distribution of the test statistic:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes as inputs the vector of pair differences and the number of pairs\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution <-\r\n  function(vector_pair_difference, n_pairs) {\r\n    randomization_distribution = NULL\r\n    n_columns = dim(permutations_matrix)[2]\r\n    for (i in 1:n_columns) {\r\n      randomization_distribution[i] =  sum(vector_pair_difference * permutations_matrix[, i]) / n_pairs\r\n    }\r\n    return(randomization_distribution)\r\n  }\r\n\r\n\r\n\r\nWe run the function:\r\n\r\n\r\n# get the distribution of permuted test statistics\r\ndistribution_test_statistics <-\r\n  function_randomization_distribution(vector_pair_difference = observed_pair_differences, n_pairs = 10)\r\n\r\n\r\n\r\nWe plot below the distribution of the test statistic under the sharp null hypothesis:\r\n\r\n\r\n# make the graph\r\ngraph_distribution_test_statistic <-\r\n  tibble(distribution_test_statistics = distribution_test_statistics) %>%\r\n  ggplot(., aes(x = distribution_test_statistics)) +\r\n  geom_histogram(colour = \"white\", fill = my_blue) +\r\n  geom_vline(xintercept = average_observed_pair_differences,\r\n             size = 1.2,\r\n             colour = my_orange) +\r\n  xlab(\"Permuted Test Statistics\") + ylab(\"Counts\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_distribution_test_statistic\r\n\r\n\r\n\r\n# save the graph\r\nggsave(\r\n  graph_distribution_test_statistic,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"3.toy_example\",\r\n    \"distribution_test_statistic_sharp_null.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nComputing the Two-Sided P-Value\r\nTo compute a two-sided p-value, we again follow the explanations provided by Paul Rosenbaum in the chapter II of his textbook:\r\nWe first compute the proportions of permuted test statistics that are lower and higher than the observed test statistic.\r\nWe then double the smallest proportion.\r\nWe take the minimum of its value and one. This give us the two-sided \\(p\\)-value.\r\nWe implement this procedure as follows:\r\n\r\n\r\n# number of permutations\r\nn_permutations <- 1024\r\n\r\n# compute upper proportion\r\nupper_p_value <-\r\n  sum(distribution_test_statistics >= average_observed_pair_differences) /\r\n  n_permutations\r\n\r\n# compute lower proportion\r\nlower_p_value <-\r\n  sum(distribution_test_statistics <= average_observed_pair_differences) /\r\n  n_permutations\r\n\r\n# double the smallest proportion\r\ndouble_smallest_proprotion <- min(c(upper_p_value, lower_p_value)) * 2\r\n\r\n# take the minimum of this proportion and one\r\np_value <- min(double_smallest_proprotion, 1)\r\n\r\n\r\n\r\nThe two-sided p-value for the sharp null hypothesis of no effect is equal to 0.55. We fail to reject the sharp null hypothesis of no effect, despite having simulated a true constant effect of +3 \\(\\mu g/m^{3}\\).\r\nComputing a 95% Fisherian Intervals for a Range of Sharp Null Hypotheses\r\nIn addition to computing the sharp null hypothesis of no effect, we can make the randomization inference procedure more informative by computing also the range of constant effects consistent with the data (i.e., a Fisherian interval). We follow again the explanations provided by Tirthankar Dasguspta and Donald B. Rubin in their forthcoming textbook on experimental design: Experimental Design: A Randomization-Based Perspective.\r\nSteps of the Procedure\r\nInstead of gauging a null effect for all units, we test a set of  sharp null hypotheses \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) = Y\\(_{i,j}\\)(0) + \\(\\tau_{k}\\) for k =1,\\(\\ldots\\),  and where \\(\\tau_{k}\\) represents a constant unit-level treatment effect size.\r\nWe must therefore choose of set of constant treatment effects that we would like to test. Here, we test a set of 81 sharp null hypotheses of constant treatment effects ranging from -20  to +20  with increments of 0.5.\r\nFor each constant treatment effect , we compute the upper -value associated with the hypothesis \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(>\\) \\(\\tau_{k}\\) and the lower -value \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(<\\) \\(\\tau_{k}\\).\r\nTo test each hypothesis, we compute the distribution of the test statistic. The sequence of  hypotheses \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(>\\) \\(\\tau_{k}\\) forms an upper -value function of \\(\\tau\\), \\(p^{+}(\\tau)\\), while the sequence of alternative hypotheses \\(H_{0}^{k}\\): Y\\(_{i,j}\\)(1) - Y\\(_{i,j}\\)(0) \\(<\\) \\(\\tau_{k}\\) makes a lower -value function, \\(\\tau\\), \\(p^{-}(\\tau)\\). To compute the bounds of the 100(1-\\(\\alpha\\))% Fisherian interval, we solve \\(p^{+}(\\tau) = \\frac{\\alpha}{2}\\) for \\(\\tau\\) to get the lower limit and \\(p^{-}(\\tau) = \\frac{\\alpha}{2}\\) for the upper limit. We set our \\(\\alpha\\) significance level to 0.05 and thus compute 95% Fisherian intervals. This procedure allows us to get the range of  treatment effects consistent with our data.\r\nAs a point estimate of a Fisherian interval, we take the observed value of our test statistic which is the average of pair differences in a pollutant concentration. For avoiding confusion, it is very important to note that our test statistic is an estimate for the individual-level treatment effect of an hypothetical experiment and not for the average treatment effect.\r\nComputational Shortcut\r\nFor each hypothesis, we could impute the missing potential outcomes. Then, we would randomly allocate the treatment, express the observed outcome and finally compute the average of pair differences. Again, this is a cumbersome way to proceed. Instead, we use the computional shortcut provided by Paul Rosenbaum in his textbook.\r\nWe start by making a sharp null hypothesis of a constant treatment effect \\(\\tau\\) such that \\(Y_{i,j}(1) = Y_{i,j}(0) + \\tau\\).\r\nFor a pair i, recall that the observed pair difference in outcomes is \\(D_{i} = (W_{i,1} - W_{i,2})(Y_{i,1} - Y_{i,2})\\).\r\nUnder the sharp hypothesis, we have \\(D_{i} = (W_{i,1} - W_{i,2})((Y_{i,1} + \\tau W_{i,1}) - (Y_{i,2} + \\tau W_{i,2}))\\).\r\nWe rearrange the right-hand side expression and find that \\(D_{i} = \\tau + (W_{i,1} - W_{i,2})(Y_{i,1}(0) - Y_{i,2}(0))\\)\r\nWe have \\(D_{i} - \\tau = (W_{i,1} - W_{i,2})(Y_{i,1}(0) - Y_{i,2}(0))\\). This equation means that the observed pair difference in outcomes minus the hypothesized treatment effect is equal to \\(\\pm(Y_{i,1}(0) - Y_{i,2}(0))\\). We can therefore carry out the randomization inference procedure seen in the previous section from the vector of observed pair differences adjusted for the hypothesized treatment effect.\r\nImplementation in R\r\nWe start by creating a nested tibble of our vector of observed pair differences with the set of constant treatment effect sizes we want to test:\r\n\r\n\r\n# create a nested dataframe with\r\n# the set of constant treatment effect sizes\r\n# and the vector of observed pair differences\r\nri_data_fi <-\r\n  tibble(observed_pair_differences = observed_pair_differences) %>%\r\n  summarise(data_observed_pair_differences = list(observed_pair_differences)) %>%\r\n  group_by(data_observed_pair_differences) %>%\r\n  expand(effect = seq(from = -20, to = 20, by = 0.5)) %>%\r\n  ungroup()\r\n\r\n# display the nested table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 2\r\n   data_observed_pair_differences effect\r\n   <list>                          <dbl>\r\n 1 <dbl [10]>                      -20  \r\n 2 <dbl [10]>                      -19.5\r\n 3 <dbl [10]>                      -19  \r\n 4 <dbl [10]>                      -18.5\r\n 5 <dbl [10]>                      -18  \r\n 6 <dbl [10]>                      -17.5\r\n 7 <dbl [10]>                      -17  \r\n 8 <dbl [10]>                      -16.5\r\n 9 <dbl [10]>                      -16  \r\n10 <dbl [10]>                      -15.5\r\n# ... with 71 more rows\r\n\r\nWe then subtract for each pair difference the hypothetical constant effect:\r\n\r\n\r\n# function to get the observed statistic\r\nadjusted_pair_difference_function <-\r\n  function(data_observed_pair_differences, effect) {\r\n    adjusted_pair_difference <- data_observed_pair_differences - effect\r\n    return(adjusted_pair_difference)\r\n  }\r\n\r\n# compute the adjusted pair differences\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    data_adjusted_pair_difference = map2(\r\n      data_observed_pair_differences,\r\n      effect,\r\n      ~ adjusted_pair_difference_function(.x, .y)\r\n    )\r\n  )\r\n\r\n# display the table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 3\r\n   data_observed_pair_differences effect data_adjusted_pair_difference\r\n   <list>                          <dbl> <list>                       \r\n 1 <dbl [10]>                      -20   <dbl [10]>                   \r\n 2 <dbl [10]>                      -19.5 <dbl [10]>                   \r\n 3 <dbl [10]>                      -19   <dbl [10]>                   \r\n 4 <dbl [10]>                      -18.5 <dbl [10]>                   \r\n 5 <dbl [10]>                      -18   <dbl [10]>                   \r\n 6 <dbl [10]>                      -17.5 <dbl [10]>                   \r\n 7 <dbl [10]>                      -17   <dbl [10]>                   \r\n 8 <dbl [10]>                      -16.5 <dbl [10]>                   \r\n 9 <dbl [10]>                      -16   <dbl [10]>                   \r\n10 <dbl [10]>                      -15.5 <dbl [10]>                   \r\n# ... with 71 more rows\r\n\r\nWe compute the observed mean of adjusted pair differences:\r\n\r\n\r\n# compute the observed mean of adjusted pair differences\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(observed_mean_difference = map(data_adjusted_pair_difference, ~ mean(.))) %>%\r\n  unnest(cols = c(observed_mean_difference)) %>%\r\n  select(-data_observed_pair_differences) %>%\r\n  ungroup()\r\n\r\n# display the table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 3\r\n   effect data_adjusted_pair_difference observed_mean_difference\r\n    <dbl> <list>                                           <dbl>\r\n 1  -20   <dbl [10]>                                        22.4\r\n 2  -19.5 <dbl [10]>                                        21.9\r\n 3  -19   <dbl [10]>                                        21.4\r\n 4  -18.5 <dbl [10]>                                        20.9\r\n 5  -18   <dbl [10]>                                        20.4\r\n 6  -17.5 <dbl [10]>                                        19.9\r\n 7  -17   <dbl [10]>                                        19.4\r\n 8  -16.5 <dbl [10]>                                        18.9\r\n 9  -16   <dbl [10]>                                        18.4\r\n10  -15.5 <dbl [10]>                                        17.9\r\n# ... with 71 more rows\r\n\r\nWe use the same function_randomization_distribution() to compute the randomization distribution of the test statistic for each hypothesized constant effect:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes as inputs the vector of pair differences and the number of pairs\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution <-\r\n  function(data_adjusted_pair_difference, n_pairs) {\r\n    randomization_distribution = NULL\r\n    n_columns = dim(permutations_matrix)[2]\r\n    for (i in 1:n_columns) {\r\n      randomization_distribution[i] =  sum(data_adjusted_pair_difference * permutations_matrix[, i]) / n_pairs\r\n    }\r\n    return(randomization_distribution)\r\n  }\r\n\r\n\r\n\r\nWe run the function:\r\n\r\n\r\n# compute the test statistic distribution\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    randomization_distribution = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_randomization_distribution(., n_pairs = 10)\r\n    )\r\n  )\r\n\r\n# display the table\r\nri_data_fi\r\n\r\n\r\n# A tibble: 81 x 4\r\n   effect data_adjusted_pair_d~ observed_mean_dif~ randomization_dist~\r\n    <dbl> <list>                             <dbl> <list>             \r\n 1  -20   <dbl [10]>                          22.4 <dbl [1,024]>      \r\n 2  -19.5 <dbl [10]>                          21.9 <dbl [1,024]>      \r\n 3  -19   <dbl [10]>                          21.4 <dbl [1,024]>      \r\n 4  -18.5 <dbl [10]>                          20.9 <dbl [1,024]>      \r\n 5  -18   <dbl [10]>                          20.4 <dbl [1,024]>      \r\n 6  -17.5 <dbl [10]>                          19.9 <dbl [1,024]>      \r\n 7  -17   <dbl [10]>                          19.4 <dbl [1,024]>      \r\n 8  -16.5 <dbl [10]>                          18.9 <dbl [1,024]>      \r\n 9  -16   <dbl [10]>                          18.4 <dbl [1,024]>      \r\n10  -15.5 <dbl [10]>                          17.9 <dbl [1,024]>      \r\n# ... with 71 more rows\r\n\r\nWe compute the lower and upper p-values functions:\r\n\r\n\r\n# define the p-values functions\r\nfunction_fisher_upper_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution >= observed_mean_difference) / n_permutations\r\n  }\r\n\r\nfunction_fisher_lower_p_value <-\r\n  function(observed_mean_difference,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution <= observed_mean_difference) / n_permutations\r\n  }\r\n\r\n# compute the lower and upper one-sided p-values\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    p_value_upper = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_upper_p_value(.x, .y)\r\n    ),\r\n    p_value_lower = map2_dbl(\r\n      observed_mean_difference,\r\n      randomization_distribution,\r\n      ~ function_fisher_lower_p_value(.x, .y)\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe plot below the lower and upper p-values functions:\r\n\r\n\r\n# make the graph\r\ngraph_p_value_functions_sharp_nulls <- ri_data_fi %>%\r\n  select(effect, p_value_upper, p_value_lower) %>%\r\n  rename(\"Upper p-value Function\" = p_value_upper,\r\n         \"Lower p-value Function\" = p_value_lower) %>%\r\n  pivot_longer(cols = -c(effect),\r\n               names_to = \"lower_upper\",\r\n               values_to = \"p_value\") %>%\r\n  ggplot(., aes(x = effect, y = p_value)) +\r\n  geom_hline(yintercept = 0.025, colour = my_orange) +\r\n  geom_line(colour = my_blue, size = 1.2) +\r\n  facet_wrap( ~ fct_rev(lower_upper)) +\r\n  xlab(\"Hypothetical Constant Treatment Effects\") + ylab(\"p-value\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_p_value_functions_sharp_nulls\r\n\r\n\r\n\r\n# save the graph\r\nggsave(\r\n  graph_p_value_functions_sharp_nulls,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"3.toy_example\",\r\n    \"graph_p_value_functions_sharp_nulls.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nThe orange line represents the alpha significance level, set at 5%, divided by two. We then retrieve the lower and upper bound of the 95% Fisherian interval:\r\n\r\n\r\n# retrieve the constant effects with the p-values equal or the closest to 0.025\r\nri_data_fi <- ri_data_fi %>%\r\n  mutate(\r\n    p_value_upper = abs(p_value_upper - 0.025),\r\n    p_value_lower = abs(p_value_lower - 0.025)\r\n  ) %>%\r\n  filter(p_value_upper == min(p_value_upper) |\r\n           p_value_lower == min(p_value_lower)) %>%\r\n  # in case two effect sizes have a p-value equal to 0.025, we take the effect size\r\n  # that make the Fisherian interval wider to be conservative\r\n  summarise(fi_lower_95 = min(effect),\r\n            fi_upper_95 = max(effect))\r\n\r\n\r\n\r\nAs a point estimate, we take the value of the observed average of pair differences, that is to say 2.4. For this imaginary experiment, our point estimate is close to the true constant effect but the 95% Fisherian interval is wide: [-7, 11.5]. The data are consistent with both large negative and positive constant treatment effects.\r\nComparison with Neyman’s Approach\r\nWe can compare the result of the randomization inference procedure with the one we would obtain with Neyman’s approach. In that case, the inference procedure is built to target the average causal effect and the source of inference is both the randomization of the treatment and the sampling from a population. We can estimate the finite sample average effect, \\(\\tau_{\\text{fs}}\\), with the average of observed pair differences \\(\\hat{\\tau}\\):\r\n\\[\\begin{equation*}\r\n  \\hat{\\tau} = \\frac{1}{I}\\sum_{i=1}^J(Y^{\\text{obs}}_{\\text{t},i}-Y^{\\text{obs}}_{\\text{c},i}) = \\overline{Y}^{\\text{obs}}_{\\text{t}} - \\overline{Y}^{\\text{obs}}_{\\text{c}}\r\n\\end{equation*}\\]\r\nHere, the subscripts \\(t\\) and \\(c\\) respectively indicate if the unit in a given pair is treated or not. \\(I\\) is the number of pairs. Since there are only one treated and one control unit within each pair, the standard estimate for the sampling variance of the average of pair differences is not defined. We can however compute a conservative estimate of the variance, as explained in chapter 10 of Imbens and Rubin (2015):\r\n\\[\\begin{equation*}\r\n  \\hat{\\mathbb{V}}(\\hat{\\tau}) = \\frac{1}{I(I-1)}\\sum_{I=1}^I(Y^{\\text{obs}}_{\\text{t},i}-Y^{\\text{obs}}_{\\text{c},i} - \\hat{\\tau})^{2}\r\n\\end{equation*}\\]\r\nWe finally compute an asymptotic 95% confidence interval using a Gaussian distribution approximation:\r\n\\[\\begin{equation*}\r\n\\text{CI}_{0.95}(\\tau_{\\text{fs}}) =\\Big( \\hat{\\tau} - 1.96\\times \\sqrt{\\hat{\\mathbb{V}}(\\hat{\\tau})},\\; \\hat{\\tau} + 1.96\\times \\sqrt{\\hat{\\mathbb{V}}(\\hat{\\tau})}\\Big)\r\n\\end{equation*}\\]\r\nAs in the example of Imbens and Rubin (2015), we only have here 10 pairs. To build the 95% confidence interval, we use a \\(t\\)-distribution with degrees of freedom equal to I/2-1=9. The 0.975 quantile is equal to 2.262. We compute the 95% confidence interval with the following code:\r\n\r\n\r\n# we store the number of pairs\r\nn_pairs <- 10\r\n\r\n# compute the standard error\r\nsquared_difference <-\r\n  (observed_pair_differences - average_observed_pair_differences) ^ 2\r\n\r\n# compute the standard error\r\nstandard_error <-\r\n  sqrt(1 / (n_pairs * (n_pairs - 1)) * sum(squared_difference))\r\n\r\n# compute the standard error\r\nci_lower_95 <-\r\n  average_observed_pair_differences - qt(0.975, 9) * standard_error\r\nci_upper_95 <-\r\n  average_observed_pair_differences + qt(0.975, 9) * standard_error\r\n\r\n# store results\r\nneyman_data_ci <-\r\n  tibble(ci_lower_95 = ci_lower_95, ci_upper_95 = ci_upper_95)\r\n\r\n\r\n\r\nThe 95% confidence interval is equal to [-6.303999, 11.103999], which is very similar to the one found with randomization inference. However, the interpretation of the two intervals is different: in the randomization inference procedure,\r\nComputing a 95% Fisherian Intervals For Weak Null Hypotheses\r\nFinally, many researchers restrain from using randomization inference as a mode of inference since it assumes that treatment effects are constant across units. In most applications, this is arguably an unrealistic assumption. To overcome this limit, Jason Wu & Peng Ding (2021) propose to adopt a studentized test statistic that is finite-sample exact under sharp null hypotheses but also asymptotically conservative for the weak null hypothesis.\r\nIn the case of our toy example, this studentized test statistic is equal to the observed average of pair differences divided by the standard error of a pairwise experiment. We therefore just follow the same previous procedure but use the studentized statistic proposed by Jason Wu & Peng Ding (2021).\r\nFirst, we create the data for testing a range of weak null hypotheses with the randomization inference:\r\n\r\n\r\n# create a nested dataframe with \r\n# the set of constant treatment effect sizes\r\n# and the vector of observed pair differences\r\nri_data_fi_weak <- tibble(observed_pair_differences = observed_pair_differences) %>%\r\n  summarise(data_observed_pair_differences = list(observed_pair_differences)) %>%\r\n  group_by(data_observed_pair_differences) %>%\r\n  expand(effect = seq(from = -20, to = 20, by = 0.5)) %>%\r\n  ungroup()\r\n\r\n# display the nested table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 2\r\n   data_observed_pair_differences effect\r\n   <list>                          <dbl>\r\n 1 <dbl [10]>                      -20  \r\n 2 <dbl [10]>                      -19.5\r\n 3 <dbl [10]>                      -19  \r\n 4 <dbl [10]>                      -18.5\r\n 5 <dbl [10]>                      -18  \r\n 6 <dbl [10]>                      -17.5\r\n 7 <dbl [10]>                      -17  \r\n 8 <dbl [10]>                      -16.5\r\n 9 <dbl [10]>                      -16  \r\n10 <dbl [10]>                      -15.5\r\n# ... with 71 more rows\r\n\r\nWe then subtract for each pair difference the hypothetical constant effect using the previously used adjusted_pair_difference_function():\r\n\r\n\r\n# compute the adjusted pair differences\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(data_adjusted_pair_difference = map2(data_observed_pair_differences, effect, ~ adjusted_pair_difference_function(.x, .y)))\r\n\r\n# display the table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 3\r\n   data_observed_pair_differences effect data_adjusted_pair_difference\r\n   <list>                          <dbl> <list>                       \r\n 1 <dbl [10]>                      -20   <dbl [10]>                   \r\n 2 <dbl [10]>                      -19.5 <dbl [10]>                   \r\n 3 <dbl [10]>                      -19   <dbl [10]>                   \r\n 4 <dbl [10]>                      -18.5 <dbl [10]>                   \r\n 5 <dbl [10]>                      -18   <dbl [10]>                   \r\n 6 <dbl [10]>                      -17.5 <dbl [10]>                   \r\n 7 <dbl [10]>                      -17   <dbl [10]>                   \r\n 8 <dbl [10]>                      -16.5 <dbl [10]>                   \r\n 9 <dbl [10]>                      -16   <dbl [10]>                   \r\n10 <dbl [10]>                      -15.5 <dbl [10]>                   \r\n# ... with 71 more rows\r\n\r\nWe then compute the observed studentized statistics:\r\n\r\n\r\n# function to compute neyman t-statistic\r\nfunction_neyman_t_stat <- function(pair_differences, n_pairs) {\r\n  # compute the average of pair differences\r\n  average_pair_difference <- mean(pair_differences)\r\n  # compute the standard error\r\n  squared_difference <-\r\n    (pair_differences - average_pair_difference) ^ 2\r\n  # compute the standard error\r\n  standard_error <-\r\n    sqrt(1 / (n_pairs * (n_pairs - 1)) * sum(squared_difference))\r\n  # compute neyman t-statistic\r\n  neyman_t_stat <- average_pair_difference / standard_error\r\n  return(neyman_t_stat)\r\n}\r\n\r\n\r\n# compute the observed mean of adjusted pair differences\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    observed_neyman_t_stat = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_neyman_t_stat(., n_pairs = 10)\r\n    )\r\n  ) %>%\r\n  unnest(cols = c(observed_neyman_t_stat)) %>%\r\n  select(-data_observed_pair_differences) %>%\r\n  ungroup()\r\n\r\n# display the table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 3\r\n   effect data_adjusted_pair_difference observed_neyman_t_stat\r\n    <dbl> <list>                                         <dbl>\r\n 1  -20   <dbl [10]>                                      5.82\r\n 2  -19.5 <dbl [10]>                                      5.69\r\n 3  -19   <dbl [10]>                                      5.56\r\n 4  -18.5 <dbl [10]>                                      5.43\r\n 5  -18   <dbl [10]>                                      5.30\r\n 6  -17.5 <dbl [10]>                                      5.17\r\n 7  -17   <dbl [10]>                                      5.04\r\n 8  -16.5 <dbl [10]>                                      4.91\r\n 9  -16   <dbl [10]>                                      4.78\r\n10  -15.5 <dbl [10]>                                      4.65\r\n# ... with 71 more rows\r\n\r\nWe create a function to carry out the randomization inference with the studentized test statistic:\r\n\r\n\r\n# randomization distribution function\r\n# this function takes the vector of pair differences\r\n# and then compute the average pair difference according\r\n# to the permuted treatment assignment\r\nfunction_randomization_distribution_t_stat <-\r\n  function(data_adjusted_pair_difference, n_pairs) {\r\n    randomization_distribution = NULL\r\n    n_columns = dim(permutations_matrix)[2]\r\n    for (i in 1:n_columns) {\r\n      # compute the average of pair differences\r\n      average_pair_difference <-\r\n        sum(data_adjusted_pair_difference * permutations_matrix[, i]) / n_pairs\r\n      # compute the standard error\r\n      squared_difference <-\r\n        (data_adjusted_pair_difference - average_pair_difference) ^ 2\r\n      # compute the standard error\r\n      standard_error <-\r\n        sqrt(1 / (n_pairs * (n_pairs - 1)) * sum(squared_difference))\r\n      # compute neyman t-statistic\r\n      randomization_distribution[i] = average_pair_difference / standard_error\r\n    }\r\n    return(randomization_distribution)\r\n  }\r\n\r\n\r\n\r\nWe run the function:\r\n\r\n\r\n# compute the test statistic distribution\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    randomization_distribution = map(\r\n      data_adjusted_pair_difference,\r\n      ~ function_randomization_distribution_t_stat(., n_pairs = 10)\r\n    )\r\n  )\r\n\r\n# display the table\r\nri_data_fi_weak\r\n\r\n\r\n# A tibble: 81 x 4\r\n   effect data_adjusted_pair_d~ observed_neyman_t~ randomization_dist~\r\n    <dbl> <list>                             <dbl> <list>             \r\n 1  -20   <dbl [10]>                          5.82 <dbl [1,024]>      \r\n 2  -19.5 <dbl [10]>                          5.69 <dbl [1,024]>      \r\n 3  -19   <dbl [10]>                          5.56 <dbl [1,024]>      \r\n 4  -18.5 <dbl [10]>                          5.43 <dbl [1,024]>      \r\n 5  -18   <dbl [10]>                          5.30 <dbl [1,024]>      \r\n 6  -17.5 <dbl [10]>                          5.17 <dbl [1,024]>      \r\n 7  -17   <dbl [10]>                          5.04 <dbl [1,024]>      \r\n 8  -16.5 <dbl [10]>                          4.91 <dbl [1,024]>      \r\n 9  -16   <dbl [10]>                          4.78 <dbl [1,024]>      \r\n10  -15.5 <dbl [10]>                          4.65 <dbl [1,024]>      \r\n# ... with 71 more rows\r\n\r\nWe compute the lower and upper p-values functions:\r\n\r\n\r\n# define the p-values functions\r\nfunction_fisher_upper_p_value <-\r\n  function(observed_neyman_t_stat,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution >= observed_neyman_t_stat) / n_permutations\r\n  }\r\n\r\nfunction_fisher_lower_p_value <-\r\n  function(observed_neyman_t_stat,\r\n           randomization_distribution) {\r\n    sum(randomization_distribution <= observed_neyman_t_stat) / n_permutations\r\n  }\r\n\r\n# compute the lower and upper one-sided p-values\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    p_value_upper = map2_dbl(\r\n      observed_neyman_t_stat,\r\n      randomization_distribution,\r\n      ~ function_fisher_upper_p_value(.x, .y)\r\n    ),\r\n    p_value_lower = map2_dbl(\r\n      observed_neyman_t_stat,\r\n      randomization_distribution,\r\n      ~ function_fisher_lower_p_value(.x, .y)\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe plot below the lower and upper p-values functions:\r\n\r\n\r\n# make the graph\r\ngraph_p_value_functions_weak_nulls <- ri_data_fi_weak %>%\r\n  select(effect, p_value_upper, p_value_lower) %>%\r\n  rename(\"Upper p-value Function\" = p_value_upper, \"Lower p-value Function\" = p_value_lower) %>%\r\n  pivot_longer(cols = -c(effect), names_to = \"lower_upper\", values_to = \"p_value\") %>%\r\n  ggplot(., aes(x = effect, y = p_value)) +\r\n  geom_hline(yintercept = 0.025, colour = my_orange) +\r\n  geom_line(colour = my_blue, size = 1.2) +\r\n  facet_wrap(~ fct_rev(lower_upper)) +\r\n  xlab(\"Hypothetical Constant Treatment Effects\") + ylab(\"p-value\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_p_value_functions_weak_nulls\r\n\r\n\r\n\r\n# save the graph\r\nggsave(graph_p_value_functions_weak_nulls, filename = here::here(\"inputs\", \"3.outputs\", \"3.toy_example\", \"graph_p_value_functions_weak_nulls.pdf\"), \r\n       width = 20, height = 10, units = \"cm\", device = cairo_pdf)\r\n\r\n\r\n\r\nThe orange line represents the alpha significance level, set at 5%, divided by two. We then retrieve the lower and upper bound of the 95% Fisherian interval:\r\n\r\n\r\n# retrieve the constant effects with the p-values equal or the closest to 0.025\r\nri_data_fi_weak <- ri_data_fi_weak %>%\r\n  mutate(\r\n    p_value_upper = abs(p_value_upper - 0.025),\r\n    p_value_lower = abs(p_value_lower - 0.025)\r\n  ) %>%\r\n  filter(p_value_upper == min(p_value_upper) |\r\n           p_value_lower == min(p_value_lower)) %>%\r\n  # in case two effect sizes have a p-value equal to 0.025, we take the effect size\r\n  # that make the Fisherian interval wider to be conservative\r\n  summarise(fi_lower_95 = min(effect),\r\n            fi_upper_95 = max(effect))\r\n\r\n\r\n\r\nThe 95% Fisherian interval is equal to [-7, 11.5]. It is the same interval found with the randomization inference procedure based on average of pair differences test statistic. It is also very similar to the interval found with Neyman’s approach.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-04-18T23:06:22+02:00"
    },
    {
      "path": "index.html",
      "title": "Estimating the Local Air Pollution Impacts of Cruise Traffic",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Estimating the Local Air Pollution Impacts of Cruise Traffic\r\n          \r\n          \r\n          Home\r\n          Article\r\n          \r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Estimating the Local Air Pollution Impacts of Cruise Traffic\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                        \r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        \r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n\r\n          \r\n            Hello and welcome!\r\n            This website gathers all the materials for the paper Estimating the Local Air Pollution Impacts of Cruise Traffic: A Principled Approach for Observational Data by Léo Zabrocki, Marion Leroutier and Marie-Abèle Bind.\r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Estimating the Local Air Pollution Impacts of Cruise Traffic\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              Hello and welcome!\r\n              This website gathers all the materials for the paper Estimating the Local Air Pollution Impacts of Cruise Traffic: A Principled Approach for Observational Data by Léo Zabrocki, Marion Leroutier and Marie-Abèle Bind.\r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2022-04-18T09:58:42+02:00"
    }
  ],
  "collections": []
}
